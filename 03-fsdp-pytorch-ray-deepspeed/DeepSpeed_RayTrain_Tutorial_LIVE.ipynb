{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Started with PyTorch DeepSpeed and Ray Train\n",
        "\n",
        "This notebook demonstrates how to train large models using **DeepSpeed ZeRO** with Ray Train. DeepSpeed enables memory-efficient distributed training by partitioning optimizer states, gradients, and (optionally) model parameters across multiple GPUs—similar in goal to PyTorch FSDP2.\n",
        "\n",
        "**Learning Objectives:**\n",
        "1. Configure DeepSpeed ZeRO for distributed training\n",
        "2. Use DeepSpeed's built-in checkpointing with Ray Train\n",
        "3. Load trained models for inference\n",
        "\n",
        "**Key differences from FSDP2 (LIVE):** (1) **Config-driven** setup (JSON/dict) vs Python `fully_shard()`, (2) **Training step**: `model_engine.backward(loss)` + `model_engine.step()`, (3) **Checkpoints**: `model_engine.save_checkpoint()` / `load_checkpoint()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 0`: What is `DeepSpeed`?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "[DeepSpeed](https://www.deepspeed.ai/) is Microsoft's deep learning optimization library. Its main feature is **ZeRO (Zero Redundancy Optimizer)**, which partitions training state across GPUs to reduce memory—similar in goal to PyTorch FSDP:\n",
        "\n",
        "- **ZeRO Stage 1:** Partitions optimizer states across workers  \n",
        "- **ZeRO Stage 2:** Partitions optimizer states + gradients (we use this here)  \n",
        "- **ZeRO Stage 3:** Partitions optimizer + gradients + **model parameters** (closest to FSDP)\n",
        "\n",
        "**Why use DeepSpeed with Ray Train?** Same reason as FSDP: train larger models or use larger batch sizes by spreading memory across GPUs. DeepSpeed uses a **config file or dict** to control behavior instead of Python API calls like `fully_shard()`, and it integrates with Ray Train the same way—each worker runs your training function with one GPU, and Ray handles orchestration and checkpoints.\n",
        "\n",
        "### DeepSpeed vs FSDP2 (quick comparison)\n",
        "\n",
        "| | **FSDP2 (LIVE)** | **DeepSpeed (this)** |\n",
        "|---|------------------|----------------------|\n",
        "| **Setup** | `fully_shard(block, mesh=...)` + device mesh | Config dict + `deepspeed.initialize(model, optimizer, config)` |\n",
        "| **Training step** | `optimizer.zero_grad()` → `loss.backward()` → `optimizer.step()` | `model_engine.backward(loss)` → `model_engine.step()` |\n",
        "| **Checkpointing** | DCP: `AppState`, `dcp.save`/`dcp.load` | `model_engine.save_checkpoint(dir)` / `load_checkpoint(dir)` |\n",
        "| **Ray Train** | Same: `TorchTrainer`, `ScalingConfig`, `prepare_data_loader` | Same |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 1`: Environment Setup\n",
        "\n",
        "Check Ray cluster status and install dependencies (if needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======== Autoscaler status: 2026-02-18 21:22:36.126067 ========\n",
            "Node status\n",
            "---------------------------------------------------------------\n",
            "Active:\n",
            " 1 head\n",
            "Idle:\n",
            " 1 1xL4:16CPU-64GB-2\n",
            " 1 1xL4:16CPU-64GB-1\n",
            "Pending:\n",
            " (no pending nodes)\n",
            "Recent failures:\n",
            " (no failures)\n",
            "\n",
            "Resources\n",
            "---------------------------------------------------------------\n",
            "Total Usage:\n",
            " 0.0/32.0 CPU\n",
            " 0.0/2.0 GPU\n",
            " 0.0/2.0 anyscale/accelerator_shape:1xL4\n",
            " 0.0/1.0 anyscale/cpu_only:true\n",
            " 0.0/1.0 anyscale/node-group:1xL4:16CPU-64GB-1\n",
            " 0.0/1.0 anyscale/node-group:1xL4:16CPU-64GB-2\n",
            " 0.0/1.0 anyscale/node-group:head\n",
            " 0.0/3.0 anyscale/provider:aws\n",
            " 0.0/3.0 anyscale/region:us-west-2\n",
            " 0B/160.00GiB memory\n",
            " 12.08KiB/44.64GiB object_store_memory\n",
            "\n",
            "From request_resources:\n",
            " (none)\n",
            "Pending Demands:\n",
            " (no resource demands)\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Check Ray cluster status\n",
        "!ray status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stdlib imports\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "# --- DIFFERENT from FSDP: DeepSpeed needs these env vars on worker nodes that may not have\n",
        "# the full CUDA toolkit (e.g. no nvcc). FSDP has no such requirement.\n",
        "os.environ[\"DS_BUILD_OPS\"] = \"0\"\n",
        "os.environ[\"DS_SKIP_CUDA_CHECK\"] = \"1\"\n",
        "\n",
        "# Ray Train imports\n",
        "import ray\n",
        "import ray.train\n",
        "import ray.train.torch\n",
        "\n",
        "# PyTorch core imports\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Computer vision components\n",
        "from torchvision.models import VisionTransformer\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "\n",
        "# --- DIFFERENT: We do NOT import FSDP (fully_shard, FSDPModule, etc.) or DCP (dcp.save/load).\n",
        "# DeepSpeed is imported inside prepare_model() so it only runs on GPU workers, not the driver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 2`: Model Definition\n",
        "\n",
        "We use the same Vision Transformer (ViT) as the FSDP2 LIVE notebook—ideal for demonstrating DeepSpeed ZeRO. No code change to the model for DeepSpeed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model (same as FSDP LIVE — ViT for 28x28 FashionMNIST)\n",
        "def init_model(hidden_dim):\n",
        "    model = VisionTransformer(\n",
        "        image_size=28,\n",
        "        patch_size=7,\n",
        "        num_layers=12,\n",
        "        num_heads=8,\n",
        "        hidden_dim=hidden_dim,\n",
        "        mlp_dim=768,\n",
        "        num_classes=10,\n",
        "    )\n",
        "    model.conv_proj = torch.nn.Conv2d(1, hidden_dim, kernel_size=7, stride=7)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 3`: DeepSpeed Config and Model Preparation\n",
        "\n",
        "DeepSpeed uses a **configuration dictionary** (or JSON file) to specify ZeRO stage, precision, and batch size. No device mesh or `fully_shard()`—one config dict and `deepspeed.initialize()` handle device placement, ZeRO partitioning, and optimizer wrapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Required on clusters where worker nodes have CUDA runtime but NOT the full toolkit (no nvcc).\n",
        "# DeepSpeed's import triggers nvcc checks; this creates a dummy nvcc so the import succeeds.\n",
        "# FSDP has no equivalent — it does not invoke nvcc.\n",
        "def _setup_deepspeed_env():\n",
        "    import os\n",
        "    import tempfile as _tmpfile\n",
        "    os.environ[\"DS_BUILD_OPS\"] = \"0\"\n",
        "    os.environ[\"DS_SKIP_CUDA_CHECK\"] = \"1\"\n",
        "    import torch.utils.cpp_extension\n",
        "    real_cuda_home = torch.utils.cpp_extension.CUDA_HOME or \"/usr/local/cuda\"\n",
        "    fake_cuda = _tmpfile.mkdtemp(prefix=\"ds_cuda_\")\n",
        "    nvcc_dir = os.path.join(fake_cuda, \"bin\")\n",
        "    os.makedirs(nvcc_dir, exist_ok=True)\n",
        "    cuda_ver = torch.version.cuda or \"12.8\"\n",
        "    with open(os.path.join(nvcc_dir, \"nvcc\"), \"w\") as f:\n",
        "        f.write(f'#!/bin/bash\\necho \"Cuda compilation tools, release {cuda_ver}, V{cuda_ver}.89\"\\n')\n",
        "    os.chmod(os.path.join(nvcc_dir, \"nvcc\"), 0o755)\n",
        "    torch.utils.cpp_extension.CUDA_HOME = fake_cuda\n",
        "    import deepspeed\n",
        "    torch.utils.cpp_extension.CUDA_HOME = real_cuda_home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- DIFFERENT from FSDP: FSDP uses Python API (device mesh + fully_shard(block), fully_shard(model)).\n",
        "# DeepSpeed uses a single config dict and one call to deepspeed.initialize().\n",
        "def get_deepspeed_config(config):\n",
        "    return {\n",
        "        \"fp16\": {\n",
        "            \"enabled\": config.get(\"use_float16\", False)\n",
        "        },\n",
        "        \"zero_optimization\": {\n",
        "            \"stage\": 2,  # Stage 2 = partition optimizer + gradients (Stage 3 = also params, like FSDP)\n",
        "            \"allgather_bucket_size\": 2e8,\n",
        "            \"reduce_bucket_size\": 2e8,\n",
        "            \"overlap_comm\": True,\n",
        "            \"contiguous_gradients\": True,\n",
        "        },\n",
        "        \"train_micro_batch_size_per_gpu\": config.get(\"batch_size\", 128),\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"gradient_clipping\": 1.0,\n",
        "        \"steps_per_print\": 1000,\n",
        "    }\n",
        "\n",
        "\n",
        "def prepare_model(model, config):\n",
        "    import deepspeed\n",
        "\n",
        "    ds_config = get_deepspeed_config(config)\n",
        "\n",
        "    # Standard PyTorch optimizer; DeepSpeed wraps with ZeRO\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config.get(\"learning_rate\", 0.001)\n",
        "    )\n",
        "\n",
        "    # DeepSpeed handles device placement & ZeRO optimizer wrapping\n",
        "    model_engine, optimizer, _, _ = deepspeed.initialize(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        config=ds_config\n",
        "    )\n",
        "\n",
        "    return model_engine, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 4`: Checkpointing with DeepSpeed\n",
        "\n",
        "DeepSpeed provides **built-in checkpointing**—no PyTorch DCP or `AppState` wrapper. Use `model_engine.save_checkpoint(dir)` and `model_engine.load_checkpoint(dir)`; each worker saves its own partition shard (parallel I/O), similar in spirit to FSDP2's DCP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Checkpoint Helpers for DeepSpeed + Ray Train\n",
        "# These handle checkpointing and reporting in a multi-node distributed setting.\n",
        "# NOTE: DeepSpeed's save_checkpoint/load_checkpoint are different from FSDP's DCP.\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "def report_metrics_and_save_deepspeed_checkpoint(model_engine, metrics, epoch):\n",
        "    \"\"\"\n",
        "    Saves a DeepSpeed checkpoint for the current epoch, then reports metrics to Ray Train.\n",
        "\n",
        "    Args:\n",
        "        model_engine: The DeepSpeed engine/wrapped model.\n",
        "        metrics (dict): Metrics to report (e.g., {\"loss\": avg_loss}).\n",
        "        epoch (int): The current epoch (for checkpoint tag).\n",
        "    \"\"\"\n",
        "    with tempfile.TemporaryDirectory() as tmp:\n",
        "        # Save DeepSpeed checkpoint with current epoch in tag and client_state\n",
        "        model_engine.save_checkpoint(tmp, tag=f\"epoch_{epoch}\", client_state={\"epoch\": epoch})\n",
        "        # Synchronize all distributed processes before reporting\n",
        "        dist.barrier()\n",
        "        # Ray Train expects a checkpoint directory, so create one from the temp path\n",
        "        checkpoint = ray.train.Checkpoint.from_directory(tmp)\n",
        "        # Report metrics and checkpoint to Ray Train dashboard/driver\n",
        "        ray.train.report(metrics, checkpoint=checkpoint)\n",
        "\n",
        "\n",
        "def save_model_for_inference(model_engine, world_rank):\n",
        "    \"\"\"\n",
        "    Saves a full PyTorch model state_dict for inference. Only rank 0 saves,\n",
        "    since with ZeRO Stage 2 all params are already replicated per GPU.\n",
        "\n",
        "    Args:\n",
        "        model_engine: The DeepSpeed engine/wrapped model.\n",
        "        world_rank (int): Process rank (only 0 saves).\n",
        "    \"\"\"\n",
        "    with tempfile.TemporaryDirectory() as tmp:\n",
        "        model_path = os.path.join(tmp, \"full-model.pt\")\n",
        "        checkpoint = None\n",
        "\n",
        "        # Only rank 0 writes the full model state_dict out.\n",
        "        if world_rank == 0:\n",
        "            torch.save(model_engine.module.state_dict(), model_path)\n",
        "            # Create Ray checkpoint from directory (to allow Ray model recovery)\n",
        "            checkpoint = ray.train.Checkpoint.from_directory(tmp)\n",
        "\n",
        "        # Ensure all processes synchronize before reporting\n",
        "        dist.barrier()\n",
        "        # Empty dict for metrics, specify checkpoint_dir_name for clarity\n",
        "        ray.train.report({}, checkpoint=checkpoint, checkpoint_dir_name=\"full_model\")\n",
        "\n",
        "\n",
        "def load_deepspeed_checkpoint(model_engine, ckpt):\n",
        "    \"\"\"\n",
        "    Loads the latest DeepSpeed checkpoint from a Ray Checkpoint.\n",
        "\n",
        "    Args:\n",
        "        model_engine: The DeepSpeed engine/wrapped model (already initialized).\n",
        "        ckpt: A Ray Checkpoint object.\n",
        "\n",
        "    Returns:\n",
        "        epoch (int): The epoch number restored from the checkpoint, or 0 if no checkpoint found.\n",
        "    \"\"\"\n",
        "    with ckpt.as_directory() as d:\n",
        "        # Find all checkpoint tags that match \"epoch_*\"\n",
        "        tags = [x for x in os.listdir(d) if x.startswith(\"epoch_\")]\n",
        "        # Get the latest epoch tag (by string sort, which should work for integer epochs)\n",
        "        tag = sorted(tags)[-1] if tags else None\n",
        "\n",
        "        if not tag:\n",
        "            # No checkpoint found, return 0 (train from scratch)\n",
        "            return 0\n",
        "\n",
        "        # Load the checkpoint; client_state can carry arbitrary user state (e.g., epoch)\n",
        "        _, client_state = model_engine.load_checkpoint(d, tag=tag)\n",
        "        # Return saved epoch (defaults to 0 if not found)\n",
        "        return (client_state or {}).get(\"epoch\", 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 5`: Training Function\n",
        "\n",
        "Below is the training function that runs on each worker. Same structure as FSDP LIVE; only the **model wrapper** (DeepSpeed engine) and **training step** (`model_engine.backward` / `model_engine.step`) differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_func(config):\n",
        "    # --- DIFFERENT from FSDP: workers may not have nvcc; patch CUDA_HOME and import DeepSpeed first.\n",
        "    _setup_deepspeed_env()\n",
        "    # --- SAME as FSDP: create model with init_model()\n",
        "    model = init_model(config[\"hidden_dim\"])\n",
        "    # --- DIFFERENT: FSDP does model.to(device), then prepare_model() which calls fully_shard().\n",
        "    # DeepSpeed: one prepare_model() call wraps model + optimizer and handles device.\n",
        "    model_engine, _ = prepare_model(model, config)\n",
        "    criterion = CrossEntropyLoss()\n",
        "\n",
        "    # --- SAME as FSDP: resume from checkpoint via ray.train.get_checkpoint()\n",
        "    start_epoch = 0\n",
        "    ckpt = ray.train.get_checkpoint()\n",
        "    if ckpt:\n",
        "        start_epoch = load_deepspeed_checkpoint(model_engine, ckpt)\n",
        "\n",
        "    # --- SAME as FSDP: data loading with prepare_data_loader (distributed sampler)\n",
        "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
        "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
        "    train_data = FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_data, batch_size=config.get(\"batch_size\", 128), shuffle=True, num_workers=2)\n",
        "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
        "\n",
        "    world_rank = ray.train.get_context().get_world_rank()\n",
        "    world_size = ray.train.get_context().get_world_size()\n",
        "    epochs = config[\"epochs\"]\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        if world_size > 1:\n",
        "            train_loader.sampler.set_epoch(epoch)\n",
        "        running_loss, num_batches = 0.0, 0\n",
        "        for images, labels in train_loader:\n",
        "            outputs = model_engine(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # --- DIFFERENT: FSDP uses optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            # DeepSpeed: model_engine.backward(loss) then model_engine.step() (engine handles zero_grad)\n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "            running_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        avg_loss = running_loss / num_batches\n",
        "        metrics = {\"loss\": avg_loss, \"epoch\": epoch + 1}\n",
        "        report_metrics_and_save_deepspeed_checkpoint(model_engine, metrics, epoch + 1)\n",
        "        if world_rank == 0:\n",
        "            print(metrics)\n",
        "\n",
        "    save_model_for_inference(model_engine, world_rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "**Note:** We call `_setup_deepspeed_env()` at the start of `train_func` so DeepSpeed can be imported on worker nodes that have CUDA runtime but not the full toolkit (no `nvcc`). Without this, you would see `FileNotFoundError: .../nvcc`.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 6`: Final Training Configuration and Launch\n",
        "\n",
        "Configure scaling and resource requirements (same as FSDP LIVE). Add **DeepSpeed env vars** in `worker_runtime_env` so workers can import DeepSpeed without the full CUDA toolkit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure scaling and resource requirements (same as FSDP LIVE)\n",
        "scaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True)\n",
        "\n",
        "train_loop_config = {\n",
        "    \"epochs\": 1,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"batch_size\": 128,\n",
        "    \"hidden_dim\": 3840,\n",
        "    \"use_float16\": False,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-18 21:22:41,536\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.91.12:6379...\n",
            "2026-02-18 21:22:41,548\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-ffbqdd398vb4g8i97u3tsubr23.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
            "2026-02-18 21:22:41,753\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_ae31baf8ffd7c346159f8e63a26c57a688dcdc7b.zip' (83.26MiB) to Ray cluster...\n",
            "2026-02-18 21:22:42,090\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_ae31baf8ffd7c346159f8e63a26c57a688dcdc7b.zip'.\n",
            "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
            "  warnings.warn(\n",
            "\u001b[36m(TrainController pid=48334)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
            "\u001b[36m(TrainController pid=48334)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
            "\u001b[36m(TrainController pid=48334)\u001b[0m Started training worker group of size 2: \n",
            "\u001b[36m(TrainController pid=48334)\u001b[0m - (ip=10.0.122.26, pid=11194) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(TrainController pid=48334)\u001b[0m - (ip=10.0.95.162, pid=12800) world_rank=1, local_rank=0, node_rank=1\n",
            "\u001b[36m(TrainController pid=48334)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m Before initializing optimizer states\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m MA 4.35 GB         Max_MA 4.35 GB         CA 4.36 GB         Max_CA 4 GB \n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m CPU Virtual Memory:  used = 3.82 GB, percent = 6.3%\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m After initializing optimizer states\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m MA 4.35 GB         Max_MA 5.81 GB         CA 5.81 GB         Max_CA 6 GB \n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m CPU Virtual Memory:  used = 3.67 GB, percent = 6.1%\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m After initializing ZeRO optimizer\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m MA 4.35 GB         Max_MA 4.35 GB         CA 5.81 GB         Max_CA 6 GB \n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m CPU Virtual Memory:  used = 3.67 GB, percent = 6.1%\n",
            "\u001b[36m(RayTrainWorker pid=12800, ip=10.0.95.162)\u001b[0m [rank1]:[W218 21:24:47.921532984 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m /tmp/ray/session_2026-02-18_20-02-04_912386_2347/runtime_resources/pip/d58b107088d1e5ed25fd0158fb090a141d6e04a9/virtualenv/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m   return func(*args, **kwargs)\n",
            "\u001b[36m(RayTrainWorker pid=12800, ip=10.0.95.162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_mnist_zfnL9/checkpoint_2026-02-18_21-25-00.927613)\n",
            "\u001b[36m(RayTrainWorker pid=12800, ip=10.0.95.162)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_mnist_zfnL9/checkpoint_2026-02-18_21-25-00.927613), metrics={'loss': 2.4052480459213257, 'epoch': 1}, validation_spec=None)\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m [rank0]:[W218 21:24:47.809815188 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_mnist_zfnL9/checkpoint_2026-02-18_21-25-00.927613)\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_mnist_zfnL9/checkpoint_2026-02-18_21-25-00.927613), metrics={'loss': 2.3740878224372866, 'epoch': 1}, validation_spec=None)\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m {'loss': 2.3740878224372866, 'epoch': 1}\n",
            "\u001b[36m(RayTrainWorker pid=12800, ip=10.0.95.162)\u001b[0m Reporting training result 2: TrainingReport(checkpoint=None, metrics={}, validation_spec=None)\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_mnist_zfnL9/full_model)\n",
            "\u001b[36m(RayTrainWorker pid=11194, ip=10.0.122.26)\u001b[0m Reporting training result 2: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_mnist_zfnL9/full_model), metrics={}, validation_spec=None)\n",
            "\u001b[36m(TrainController pid=48334)\u001b[0m [State Transition] RUNNING -> SHUTTING_DOWN.\n"
          ]
        }
      ],
      "source": [
        "# Generate a unique run name (same pattern as FSDP LIVE)\n",
        "import random\n",
        "import string\n",
        "name = \"deepspeed_mnist_\" + ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n",
        "\n",
        "# TorchTrainer is the main entry point for Ray Train (same as FSDP).\n",
        "# worker_runtime_env must set DS_BUILD_OPS and DS_SKIP_CUDA_CHECK for DeepSpeed on worker nodes.\n",
        "trainer = ray.train.torch.TorchTrainer(\n",
        "    train_func,\n",
        "    scaling_config=scaling_config,\n",
        "    train_loop_config=train_loop_config,\n",
        "    run_config=ray.train.RunConfig(\n",
        "        storage_path=\"/mnt/cluster_storage/\",\n",
        "        name=name,\n",
        "        failure_config=ray.train.FailureConfig(max_failures=2),\n",
        "        worker_runtime_env={\n",
        "            \"env_vars\": {\"DS_BUILD_OPS\": \"0\", \"DS_SKIP_CUDA_CHECK\": \"1\"}\n",
        "        },\n",
        "    ),\n",
        ")\n",
        "result = trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 7`: Inspect Training Artifacts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Training artifacts include:\n",
        "- `checkpoint_*/` — Epoch checkpoints with DeepSpeed shards\n",
        "- `full_model/` — Consolidated model for inference (same as FSDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 24\n",
            "drwxr-xr-x  4 ray users 6144 Feb 18 21:26 .\n",
            "drwxr-xr-x 22 ray  1000 6144 Feb 18 21:22 ..\n",
            "-rw-r--r--  1 ray users    0 Feb 18 21:22 .validate_storage_marker\n",
            "drwxr-xr-x  3 ray users 6144 Feb 18 21:25 checkpoint_2026-02-18_21-25-00.927613\n",
            "-rw-r--r--  1 ray users  335 Feb 18 21:27 checkpoint_manager_snapshot.json\n",
            "drwxr-xr-x  2 ray users 6144 Feb 18 21:26 full_model\n"
          ]
        }
      ],
      "source": [
        "# List artifacts\n",
        "!ls -la /mnt/cluster_storage/{name}/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Step 8`: Load Model for Inference\n",
        "\n",
        "The consolidated model (`full-model.pt`) is a standard PyTorch state dict—same as FSDP2. It works without DeepSpeed or any distributed setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load consolidated model (standard PyTorch state_dict, same as FSDP)\n",
        "model = init_model(train_loop_config[\"hidden_dim\"])\n",
        "state = torch.load(f\"/mnt/cluster_storage/{name}/full_model/full-model.pt\", map_location=\"cpu\")\n",
        "model.load_state_dict(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load some test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: .\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5,), std=(0.5,))\n",
              "           )"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
        "test_data = FashionMNIST(\n",
        "    root=\".\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted_label=4 test_label=9\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(test_data.data[0].reshape(1, 1, 28, 28).float())\n",
        "    predicted_label = out.argmax().item()\n",
        "    test_label = test_data.targets[0].item()\n",
        "    print(f\"{predicted_label=} {test_label=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
