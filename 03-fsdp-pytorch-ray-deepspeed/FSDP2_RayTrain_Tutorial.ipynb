{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Get Started with PyTorch FSDP2 and Ray Train: A Complete Guide\n",
    "\n",
    "This notebook will show you how to train large models that don't fit in a single GPU's memory using PyTorch's Fully Sharded Data Parallel (FSDP2) with Ray Train. FSDP2 enables model sharding across multiple GPUs and nodes, significantly reducing memory footprint compared to standard Distributed Data Parallel (DDP).\n",
    "\n",
    "In this tutorial, you:\n",
    "1. Learn how FSDP2 shards model parameters, gradients, and optimizer states across workers\n",
    "2. Configure memory optimization techniques: CPU offloading, mixed precision, and resharding strategies\n",
    "3. Use PyTorch Distributed Checkpoint (DCP) for efficient checkpointing of sharded models\n",
    "4. Profile GPU memory usage with PyTorch's memory snapshot API\n",
    "5. Load the trained model for inference\n",
    "\n",
    "## What is FSDP2?\n",
    "\n",
    "[Fully Sharded Data Parallel (FSDP2)](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html) is PyTorch's native solution for training large models that exceed single GPU memory:\n",
    "\n",
    "> FSDP2 shards model parameters, gradients, and optimizer states across data-parallel workers. During the forward pass, parameters are all-gathered for computation, then re-sharded. This enables training models that are much larger than what fits on a single GPU.\n",
    "\n",
    "FSDP2 is a significant improvement over FSDP1:\n",
    "- **Per-parameter sharding**: Chunks each parameter on dim-0 across workers (vs. flattening/concatenating in FSDP1)\n",
    "- **DTensor integration**: Better support for tensor parallelism and multi-dimensional parallelism\n",
    "- **Relaxed constraints**: Handles frozen parameters more naturally\n",
    "- **Communication-free state dicts**: Sharded state dicts without collective communication\n",
    "\n",
    "## When to Use FSDP2?\n",
    "\n",
    "Use FSDP2 when:\n",
    "- Your model doesn't fit in a single GPU's memory even with gradient checkpointing\n",
    "- You want native PyTorch integration (no external dependencies)\n",
    "- You need fine-grained control over sharding strategies\n",
    "- You're building custom training loops with PyTorch's ecosystem\n",
    "\n",
    "For comparison with DeepSpeed (another memory optimization solution), see the [DeepSpeed tutorial](./DeepSpeed_RayTrain_Tutorial.ipynb) in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial requires:\n",
    "- A Ray cluster with GPU workers (this example uses 2x T4 GPUs)\n",
    "- PyTorch 2.0+ with CUDA support\n",
    "- Shared storage accessible from all workers (e.g., `/mnt/cluster_storage/`)\n",
    "\n",
    "When running on open-source Ray (without Anyscale), you'll need to:\n",
    "- Configure your Ray cluster manually\n",
    "- Set up NFS or cloud storage for checkpointing\n",
    "- Ensure PyTorch is installed on all worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## `Step 1`: Environment Setup\n",
    "\n",
    "First, let's check the Ray cluster status and install dependencies. This tutorial requires:\n",
    "- A Ray cluster with GPU workers (this example uses 2 GPUs)\n",
    "- PyTorch 2.0+ with CUDA support\n",
    "- Shared storage accessible from all workers (e.g., `/mnt/cluster_storage/`)\n",
    "\n",
    "When running on open-source Ray (without Anyscale), you'll need to:\n",
    "- Configure your Ray cluster manually\n",
    "- Set up NFS or cloud storage for checkpointing\n",
    "- Ensure PyTorch is installed on all worker nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Autoscaler status: 2026-02-01 09:03:18.953253 ========\n",
      "Node status\n",
      "---------------------------------------------------------------\n",
      "Active:\n",
      " 1 head_node\n",
      "Idle:\n",
      " 1 1xA10G:8CPU-32GB-2\n",
      " 1 1xA10G:8CPU-32GB-1\n",
      "Pending:\n",
      " (no pending nodes)\n",
      "Recent failures:\n",
      " (no failures)\n",
      "\n",
      "Resources\n",
      "---------------------------------------------------------------\n",
      "Total Usage:\n",
      " 0.0/16.0 CPU\n",
      " 0.0/2.0 GPU\n",
      " 0.0/2.0 anyscale/accelerator_shape:1xA10G\n",
      " 0.0/1.0 anyscale/cpu_only:true\n",
      " 0.0/1.0 anyscale/node-group:1xA10G:8CPU-32GB-1\n",
      " 0.0/1.0 anyscale/node-group:1xA10G:8CPU-32GB-2\n",
      " 0.0/1.0 anyscale/node-group:head_node\n",
      " 0.0/3.0 anyscale/provider:aws\n",
      " 0.0/3.0 anyscale/region:us-west-2\n",
      " 0B/96.00GiB memory\n",
      " 153.03KiB/26.63GiB object_store_memory\n",
      "\n",
      "Total Constraints:\n",
      " (no request_resources() constraints)\n",
      "Total Demands:\n",
      " (no resource demands)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Check Ray cluster status\n",
    "!ray status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully registered `torch, torchvision` and 1 other packages to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_g54aiirwj1s8t9ktgzikqur41k/prj_f1j47h9srml4cyg962id75ms2e/workspaces/expwrk_k1d9n8z4panjhy1yh7bqe1ywiz?workspace-tab=dependencies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install -q torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "Ray version: 2.49.1\n"
     ]
    }
   ],
   "source": [
    "# Verify installation and check versions\n",
    "import torch\n",
    "import ray\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Ray version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train V2 API (recommended for latest features)\n",
    "import os\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Standard library imports\n",
    "import tempfile\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## `Step 2`: Model Definition\n",
    "\n",
    "We'll use a Vision Transformer (ViT) for this tutorial. ViT has clear, repeatable block structures (transformer encoder blocks) that are ideal for demonstrating FSDP2's sharding capabilities.\n",
    "\n",
    "### Key Architecture Decisions:\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|----------|\n",
    "| `image_size` | 28 | FashionMNIST native resolution |\n",
    "| `patch_size` | 7 | Creates 4x4 = 16 patches per image |\n",
    "| `num_layers` | 10 | Sufficient depth for FSDP2 demonstration |\n",
    "| `hidden_dim` | 128 | Moderate model size for T4 GPUs |\n",
    "| `num_classes` | 10 | FashionMNIST categories |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Vision Transformer model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,006,090 (1.01M)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import VisionTransformer\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "def init_model() -> torch.nn.Module:\n",
    "    \"\"\"Initialize a Vision Transformer model for FashionMNIST classification.\n",
    "    \n",
    "    The model is configured for 28x28 grayscale images with 10 output classes.\n",
    "    We modify the patch embedding layer to accept single-channel input.\n",
    "    \n",
    "    Returns:\n",
    "        torch.nn.Module: Configured ViT model\n",
    "    \"\"\"\n",
    "    logger.info(\"Initializing Vision Transformer model...\")\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        image_size=28,        # FashionMNIST image size\n",
    "        patch_size=7,         # Creates 4x4 = 16 patches\n",
    "        num_layers=10,        # Number of transformer encoder layers\n",
    "        num_heads=2,          # Attention heads per layer\n",
    "        hidden_dim=128,       # Embedding dimension\n",
    "        mlp_dim=128,          # Feed-forward network dimension\n",
    "        num_classes=10,       # FashionMNIST has 10 classes\n",
    "    )\n",
    "\n",
    "    # Modify patch embedding for grayscale images (1 channel instead of 3)\n",
    "    model.conv_proj = torch.nn.Conv2d(\n",
    "        in_channels=1,        # Grayscale input\n",
    "        out_channels=128,     # Match hidden_dim\n",
    "        kernel_size=7,        # Match patch_size\n",
    "        stride=7,             # Non-overlapping patches\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Quick test: verify model initialization\n",
    "test_model = init_model()\n",
    "param_count = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"Model parameters: {param_count:,} ({param_count / 1e6:.2f}M)\")\n",
    "del test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## `Step 3`: FSDP2 Sharding Configuration\n",
    "\n",
    "FSDP2's `fully_shard` API provides several knobs for memory-performance tradeoffs. Every configuration option is highlighted and explained with a numbered comment; for example, \"[1].\"\n",
    "\n",
    "### Key Configuration Options:\n",
    "\n",
    "**Device Mesh [1]**: The `DeviceMesh` describes the topology of your training cluster. For data parallelism, we use a simple 1D mesh where each dimension represents a data-parallel rank. For more advanced setups (tensor parallelism, pipeline parallelism), you can use multi-dimensional meshes.\n",
    "\n",
    "**CPU Offloading [2]**: Stores sharded parameters, gradients, and optimizer states on CPU, copying to GPU only during computation. Use when GPU memory is constrained, but be aware of increased CPU-GPU transfer overhead.\n",
    "\n",
    "**Mixed Precision [3]**: Uses FP16 for parameters and gradient reductions while maintaining FP32 for critical operations. Provides ~2x memory reduction and faster computation on tensor cores.\n",
    "\n",
    "**Resharding After Forward [4]**: The `reshard_after_forward=True` flag frees all-gathered weights after the forward pass, reducing peak memory at the cost of more communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP2 imports\n",
    "from torch.distributed.fsdp import (\n",
    "    fully_shard,\n",
    "    FSDPModule,\n",
    "    CPUOffloadPolicy,\n",
    "    MixedPrecisionPolicy,\n",
    ")\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "\n",
    "import ray.train\n",
    "\n",
    "def shard_model(model: torch.nn.Module):\n",
    "    \"\"\"Apply FSDP2 sharding to the model.\n",
    "    \n",
    "    Sharding strategy:\n",
    "    1. Shard each transformer encoder block individually\n",
    "    2. Wrap the entire model with FSDP2\n",
    "    \n",
    "    This granularity provides good memory savings while limiting\n",
    "    communication overhead between workers.\n",
    "    \"\"\"\n",
    "    logger.info(\"Applying FSDP2 sharding to model...\")\n",
    "\n",
    "    # [1] Create device mesh for data parallelism.\n",
    "    # The DeviceMesh describes the topology of your training cluster.\n",
    "    # For data parallelism, we use a simple 1D mesh.\n",
    "    # =================================================================\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    mesh = init_device_mesh(\n",
    "        device_type=\"cuda\",\n",
    "        mesh_shape=(world_size,),\n",
    "        mesh_dim_names=(\"data_parallel\",)\n",
    "    )\n",
    "\n",
    "    # [2] Configure CPU offloading (reduces GPU memory usage).\n",
    "    # This stores sharded parameters, gradients, and optimizer states on CPU,\n",
    "    # copying to GPU only during forward/backward computation.\n",
    "    # ================================================================\n",
    "    offload_policy = CPUOffloadPolicy()\n",
    "\n",
    "    # [3] Configure mixed precision (FP16 for efficiency).\n",
    "    # Uses FP16 for parameters and gradient reductions while maintaining\n",
    "    # FP32 for critical operations. Provides ~2x memory reduction.\n",
    "    # ================================================================\n",
    "    mp_policy = MixedPrecisionPolicy(\n",
    "        param_dtype=torch.float16,\n",
    "        reduce_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # [4] Shard each encoder block (per-layer sharding).\n",
    "    # This provides good memory savings while limiting communication overhead.\n",
    "    # ========================================================================\n",
    "    for encoder_block in model.encoder.layers.children():\n",
    "        fully_shard(\n",
    "            encoder_block,\n",
    "            mesh=mesh,\n",
    "            reshard_after_forward=True,  # Free all-gathered weights after forward\n",
    "            offload_policy=offload_policy,\n",
    "            mp_policy=mp_policy\n",
    "        )\n",
    "\n",
    "    # [5] Shard the root model.\n",
    "    # =========================\n",
    "    fully_shard(\n",
    "        model,\n",
    "        mesh=mesh,\n",
    "        reshard_after_forward=True,\n",
    "        offload_policy=offload_policy,\n",
    "        mp_policy=mp_policy\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Model sharded across {world_size} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## `Step 4`: Distributed Checkpointing\n",
    "\n",
    "PyTorch Distributed Checkpoint (DCP) provides efficient checkpointing for sharded models. Key features:\n",
    "\n",
    "- **Sharded saving**: Each worker saves only its shard (parallel I/O)\n",
    "- **Automatic resharding**: Load checkpoints even if worker count changes\n",
    "- **Optimizer state support**: Save full training state for resumption\n",
    "\n",
    "### Architecture\n",
    "\n",
    "We use PyTorch's `Stateful` protocol to wrap model and optimizer state:\n",
    "\n",
    "```\n",
    "AppState (Stateful)\n",
    "‚îú‚îÄ‚îÄ model (FSDPModule)\n",
    "‚îú‚îÄ‚îÄ optimizer (Adam)\n",
    "‚îî‚îÄ‚îÄ epoch (int)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCP imports\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    get_state_dict,\n",
    "    set_state_dict,\n",
    "    get_model_state_dict,\n",
    "    StateDictOptions\n",
    ")\n",
    "from torch.distributed.checkpoint.stateful import Stateful\n",
    "import torch.distributed.checkpoint as dcp\n",
    "\n",
    "class AppState(Stateful):\n",
    "    \"\"\"Wrapper for checkpointing application state with DCP.\n",
    "    \n",
    "    Implements PyTorch's Stateful protocol for automatic state\n",
    "    serialization/deserialization during dcp.save/load calls.\n",
    "    \n",
    "    The key insight is that get_state_dict/set_state_dict handle\n",
    "    FSDP2's fully qualified names (FQNs) automatically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer=None, epoch=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Extract sharded state dict for saving.\"\"\"\n",
    "        model_state_dict, optimizer_state_dict = get_state_dict(\n",
    "            self.model, self.optimizer\n",
    "        )\n",
    "        return {\n",
    "            \"model\": model_state_dict,\n",
    "            \"optim\": optimizer_state_dict,\n",
    "            \"epoch\": self.epoch\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Load sharded state dict (handles resharding automatically).\"\"\"\n",
    "        set_state_dict(\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "            model_state_dict=state_dict[\"model\"],\n",
    "            optim_state_dict=state_dict[\"optim\"],\n",
    "        )\n",
    "        if \"epoch\" in state_dict:\n",
    "            self.epoch = state_dict[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fsdp_checkpoint(\n",
    "    model: FSDPModule, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    ckpt: ray.train.Checkpoint\n",
    ") -> int | None:\n",
    "    \"\"\"Load an FSDP checkpoint for resuming training.\n",
    "    \n",
    "    DCP automatically handles resharding if the number of workers\n",
    "    differs from when the checkpoint was saved.\n",
    "    \n",
    "    Args:\n",
    "        model: FSDP-wrapped model\n",
    "        optimizer: Optimizer instance\n",
    "        ckpt: Ray Train checkpoint object\n",
    "        \n",
    "    Returns:\n",
    "        Epoch number from checkpoint, or None if not available\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading distributed checkpoint...\")\n",
    "    \n",
    "    try:\n",
    "        with ckpt.as_directory() as checkpoint_dir:\n",
    "            app_state = AppState(model, optimizer)\n",
    "            dcp.load(\n",
    "                state_dict={\"app\": app_state},\n",
    "                checkpoint_id=checkpoint_dir\n",
    "            )\n",
    "        logger.info(f\"Loaded checkpoint from epoch {app_state.epoch}\")\n",
    "        return app_state.epoch\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Checkpoint loading failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics_and_save_fsdp_checkpoint(\n",
    "    model: FSDPModule, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    metrics: dict, \n",
    "    epoch: int = 0\n",
    ") -> None:\n",
    "    \"\"\"Save checkpoint and report metrics to Ray Train.\n",
    "    \n",
    "    Each worker saves its shard to a temporary directory, then\n",
    "    Ray Train consolidates these to shared storage.\n",
    "    \n",
    "    Args:\n",
    "        model: FSDP-wrapped model\n",
    "        optimizer: Optimizer instance\n",
    "        metrics: Dict of metrics (loss, accuracy, etc.)\n",
    "        epoch: Current epoch number\n",
    "    \"\"\"\n",
    "    logger.info(\"Saving checkpoint and reporting metrics...\")\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Save distributed checkpoint\n",
    "        dcp.save(\n",
    "            state_dict={\"app\": AppState(model, optimizer, epoch)},\n",
    "            checkpoint_id=temp_dir\n",
    "        )\n",
    "        \n",
    "        # Report to Ray Train (uploads to shared storage)\n",
    "        checkpoint = ray.train.Checkpoint.from_directory(temp_dir)\n",
    "        ray.train.report(metrics, checkpoint=checkpoint)\n",
    "        \n",
    "    logger.info(f\"Checkpoint saved. Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_inference(model: FSDPModule, world_rank: int) -> None:\n",
    "    \"\"\"Consolidate sharded model into a single file for inference.\n",
    "    \n",
    "    This all-gathers parameters to rank 0 and saves a standard\n",
    "    PyTorch checkpoint compatible with torch.load().\n",
    "    \n",
    "    Warning: For very large models, this may exceed CPU memory on rank 0.\n",
    "    In such cases, use distributed loading for inference instead.\n",
    "    \n",
    "    Args:\n",
    "        model: FSDP-wrapped model\n",
    "        world_rank: Current worker's rank\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing model for inference...\")\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        save_file = os.path.join(temp_dir, \"full-model.pt\")\n",
    "\n",
    "        # All-gather model state to rank 0\n",
    "        model_state_dict = get_model_state_dict(\n",
    "            model=model,\n",
    "            options=StateDictOptions(\n",
    "                full_state_dict=True,    # Reconstruct full model\n",
    "                cpu_offload=True,        # Save GPU memory\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logger.info(\"Retrieved complete model state dict\")\n",
    "        checkpoint = None\n",
    "\n",
    "        # Only rank 0 saves the consolidated checkpoint\n",
    "        if world_rank == 0:\n",
    "            torch.save(model_state_dict, save_file)\n",
    "            logger.info(f\"Saved model to {save_file}\")\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_dir)\n",
    "\n",
    "        # Report checkpoint (only rank 0's is non-None)\n",
    "        ray.train.report({}, checkpoint=checkpoint, checkpoint_dir_name=\"full_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## `Step 5`: Training Function\n",
    "\n",
    "The training function runs on each worker. Key responsibilities:\n",
    "\n",
    "1. **Initialize model** on GPU\n",
    "2. **Apply FSDP2 sharding** using the `shard_model()` function\n",
    "3. **Resume from checkpoint** if available\n",
    "4. **Run training loop** with metric reporting\n",
    "5. **Save final model** for inference\n",
    "\n",
    "### Memory Profiling\n",
    "\n",
    "We use PyTorch's CUDA memory snapshot API (PyTorch 2.10+) to profile GPU memory usage:\n",
    "\n",
    "```python\n",
    "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
    "# ... training ...\n",
    "torch.cuda.memory._dump_snapshot(path)\n",
    "```\n",
    "\n",
    "The snapshot can be visualized using PyTorch's memory visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train.torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(config):\n",
    "    \"\"\"Main training function for FSDP2 + Ray Train.\n",
    "    \n",
    "    This function runs on each distributed worker. Ray Train handles:\n",
    "    - Process group initialization\n",
    "    - Device assignment\n",
    "    - Checkpoint coordination\n",
    "    \n",
    "    Args:\n",
    "        config: Dict with training hyperparameters\n",
    "    \"\"\"\n",
    "    # === Model Setup ===\n",
    "    model = init_model()\n",
    "    \n",
    "    # Get assigned device and move model\n",
    "    device = ray.train.torch.get_device()\n",
    "    torch.cuda.set_device(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Apply FSDP2 sharding\n",
    "    shard_model(model)\n",
    "    \n",
    "    # === Optimizer Setup ===\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.get('learning_rate', 0.001))\n",
    "    \n",
    "    # === Checkpoint Loading ===\n",
    "    start_epoch = 0\n",
    "    loaded_checkpoint = ray.train.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        latest_epoch = load_fsdp_checkpoint(model, optimizer, loaded_checkpoint)\n",
    "        start_epoch = latest_epoch + 1 if latest_epoch is not None else 0\n",
    "        logger.info(f\"Resuming from epoch {start_epoch}\")\n",
    "    \n",
    "    # === Data Loading ===\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = FashionMNIST(\n",
    "        root=data_dir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=config.get('batch_size', 64),\n",
    "        shuffle=True\n",
    "    )\n",
    "    # Wrap with DistributedSampler and auto device placement\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    \n",
    "    # === Training Context ===\n",
    "    world_rank = ray.train.get_context().get_world_rank()\n",
    "    run_name = ray.train.get_context().get_experiment_name()\n",
    "    \n",
    "    # === Memory Profiling Setup ===\n",
    "    torch.cuda.memory._record_memory_history(max_entries=100000)\n",
    "    \n",
    "    # === Training Loop ===\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epochs = config.get('epochs', 5)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # Ensure proper shuffling for distributed sampler\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Report metrics and save checkpoint\n",
    "        avg_loss = running_loss / num_batches\n",
    "        metrics = {\"loss\": avg_loss, \"epoch\": epoch}\n",
    "        report_metrics_and_save_fsdp_checkpoint(model, optimizer, metrics, epoch)\n",
    "        \n",
    "        if world_rank == 0:\n",
    "            logger.info(f\"Epoch {epoch}: loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # === Save Memory Snapshot ===\n",
    "    try:\n",
    "        snapshot_path = f\"/mnt/cluster_storage/{run_name}/rank{world_rank}_memory_snapshot.pickle\"\n",
    "        torch.cuda.memory._dump_snapshot(snapshot_path)\n",
    "        logger.info(f\"Saved memory snapshot to {snapshot_path}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not save memory snapshot: {e}\")\n",
    "    finally:\n",
    "        torch.cuda.memory._record_memory_history(enabled=None)\n",
    "    \n",
    "    # === Save Final Model ===\n",
    "    save_model_for_inference(model, world_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## `Step 6`: Launch Distributed Training\n",
    "\n",
    "Ray Train's `TorchTrainer` handles:\n",
    "- Spawning workers across the cluster\n",
    "- Initializing PyTorch distributed process groups\n",
    "- Coordinating checkpoints to shared storage\n",
    "- Automatic fault tolerance and restarts\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `num_workers` | Number of distributed workers (typically = number of GPUs) |\n",
    "| `use_gpu` | Enable GPU training |\n",
    "| `storage_path` | Shared storage for checkpoints (NFS, S3, etc.) |\n",
    "| `max_failures` | Number of retries on worker failure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: fsdp_mnist_881fdef1\n",
      "Workers: 2\n",
      "Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "import ray.train\n",
    "import ray.train.torch\n",
    "\n",
    "# Scaling configuration\n",
    "scaling_config = ray.train.ScalingConfig(\n",
    "    num_workers=2,      # Use 2 GPU workers\n",
    "    use_gpu=True        # Enable GPU training\n",
    ")\n",
    "\n",
    "# Training hyperparameters\n",
    "train_loop_config = {\n",
    "    \"epochs\": 1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 64,\n",
    "}\n",
    "\n",
    "# Unique experiment name\n",
    "experiment_name = f\"fsdp_mnist_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Run configuration\n",
    "run_config = ray.train.RunConfig(\n",
    "    storage_path=\"/mnt/cluster_storage/\",\n",
    "    name=experiment_name,\n",
    "    failure_config=ray.train.FailureConfig(max_failures=1),\n",
    ")\n",
    "\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"Workers: {scaling_config.num_workers}\")\n",
    "print(f\"Epochs: {train_loop_config['epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 09:10:50,001\tINFO worker.py:1771 -- Connecting to existing Ray cluster at address: 10.0.42.67:6379...\n",
      "2026-02-01 09:10:50,012\tINFO worker.py:1942 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-lehzcwqlg8w8ui213vjr5igw7j.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FSDP2 training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 09:10:50,015\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_292539a4a240db81dceafbcd8b58bcffaa79e9ec.zip' (0.83MiB) to Ray cluster...\n",
      "2026-02-01 09:10:50,020\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_292539a4a240db81dceafbcd8b58bcffaa79e9ec.zip'.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [FailurePolicy] Decision: FailureDecision.RETRY, Error source: controller, Error count / maximum errors allowed: 1/inf, Error: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +7m41s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=44010)\u001b[0m [FailurePolicy] Decision: FailureDecision.RETRY, Error source: controller, Error count / maximum errors allowed: 2/inf, Error: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [FailurePolicy] Decision: FailureDecision.RETRY, Error source: controller, Error count / maximum errors allowed: 3/inf, Error: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m WARNING: 4 PYTHON worker processes have been started on node: 45948528ba3a9a20719aa1c27d141f383ef42c4b67a14c209bd645c0 with address: 10.0.42.67. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m Started training worker group of size 2: \n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m - (ip=10.0.53.63, pid=11655) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m - (ip=10.0.15.209, pid=11779) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Initializing Vision Transformer model...\n",
      "\u001b[36m(RayTrainWorker pid=11779, ip=10.0.15.209)\u001b[0m Applying FSDP2 sharding to model...\n",
      "\u001b[36m(RayTrainWorker pid=11779, ip=10.0.15.209)\u001b[0m Model sharded across 2 workers\n",
      "  0%|          | 0.00/26.4M [00:00<?, ?B/s]63)\u001b[0m \n",
      "  0%|          | 32.8k/26.4M [00:00<01:52, 234kB/s] \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:02<00:00, 11.8MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 206kB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 211kB/s] \n",
      "\u001b[36m(RayTrainWorker pid=11779, ip=10.0.15.209)\u001b[0m Initializing Vision Transformer model...\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Applying FSDP2 sharding to model...\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Model sharded across 2 workers\n",
      "  0%|          | 0.00/4.42M [00:00<?, ?B/s]\u001b[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:01<00:00, 3.91MB/s]\u001b[32m [repeated 45x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Saving checkpoint and reporting metrics...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 56.8MB/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_881fdef1/checkpoint_2026-02-01_09-13-50.319574)\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Checkpoint saved. Metrics: {'loss': 0.7452967792177505, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Epoch 0: loss=0.7453\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Saved memory snapshot to /mnt/cluster_storage/fsdp_mnist_881fdef1/rank0_memory_snapshot.pickle\n",
      "\u001b[36m(RayTrainWorker pid=11779, ip=10.0.15.209)\u001b[0m Saving checkpoint and reporting metrics...\n",
      "\u001b[36m(RayTrainWorker pid=11779, ip=10.0.15.209)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_881fdef1/checkpoint_2026-02-01_09-13-50.319574)\n",
      "\u001b[36m(RayTrainWorker pid=11779, ip=10.0.15.209)\u001b[0m Checkpoint saved. Metrics: {'loss': 0.7377029126132729, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Preparing model for inference...\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Retrieved complete model state dict\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Saved model to /tmp/tmp07p1hei5/full-model.pt\n",
      "\u001b[36m(RayTrainWorker pid=11655, ip=10.0.53.63)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_881fdef1/full_model)\n",
      "\u001b[36m(TrainController pid=44010)\u001b[0m [State Transition] RUNNING -> FINISHED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Final checkpoint: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_881fdef1/full_model)\n"
     ]
    }
   ],
   "source": [
    "# Create and run trainer\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_loop_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "print(\"Starting FSDP2 training...\")\n",
    "result = trainer.fit()\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final checkpoint: {result.checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## `Step 7`: Inspect Training Artifacts\n",
    "\n",
    "After training, the following artifacts are saved to cluster storage:\n",
    "\n",
    "```\n",
    "/mnt/cluster_storage/{experiment_name}/\n",
    "‚îú‚îÄ‚îÄ checkpoint_*/              # Epoch checkpoints (distributed shards)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __0_0.distcp          # Rank 0 shard\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ __1_0.distcp          # Rank 1 shard\n",
    "‚îú‚îÄ‚îÄ full_model/               # Consolidated model for inference\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ full-model.pt         # Standard PyTorch checkpoint\n",
    "‚îú‚îÄ‚îÄ rank*_memory_snapshot.pickle  # Memory profiling data\n",
    "‚îî‚îÄ‚îÄ checkpoint_manager_snapshot.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts in /mnt/cluster_storage/fsdp_mnist_881fdef1/:\n",
      "  üìÅ checkpoint_2026-02-01_09-13-50.319574/\n",
      "  üìÑ rank1_memory_snapshot.pickle (26.64 MB)\n",
      "  üìÑ rank0_memory_snapshot.pickle (26.64 MB)\n",
      "  üìÑ checkpoint_manager_snapshot.json (0.00 MB)\n",
      "  üìÑ .validate_storage_marker (0.00 MB)\n",
      "  üìÅ full_model/\n"
     ]
    }
   ],
   "source": [
    "# List training artifacts\n",
    "import os\n",
    "storage_path = f\"/mnt/cluster_storage/{experiment_name}/\"\n",
    "print(f\"Artifacts in {storage_path}:\")\n",
    "for item in os.listdir(storage_path):\n",
    "    full_path = os.path.join(storage_path, item)\n",
    "    if os.path.isdir(full_path):\n",
    "        print(f\"  üìÅ {item}/\")\n",
    "    else:\n",
    "        size_mb = os.path.getsize(full_path) / (1024 * 1024)\n",
    "        print(f\"  üìÑ {item} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## `Step 8`: Load Model for Inference\n",
    "\n",
    "The consolidated model (`full-model.pt`) can be loaded without FSDP2 for inference. This is a standard PyTorch checkpoint that works on any device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /mnt/cluster_storage/fsdp_mnist_881fdef1/full_model/full-model.pt\n"
     ]
    }
   ],
   "source": [
    "# Path to the saved model\n",
    "PATH_TO_FULL_MODEL = f\"/mnt/cluster_storage/{experiment_name}/full_model/full-model.pt\"\n",
    "print(f\"Loading model from: {PATH_TO_FULL_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Vision Transformer model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "inference_model = init_model()\n",
    "state_dict = torch.load(PATH_TO_FULL_MODEL, map_location='cpu', weights_only=True)\n",
    "inference_model.load_state_dict(state_dict)\n",
    "inference_model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:02<00:00, 11.8MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 207kB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:01<00:00, 3.86MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 21.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "test_data = FashionMNIST(\n",
    "    root=\"/tmp\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Class labels for FashionMNIST\n",
    "CLASSES = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions:\n",
      "--------------------------------------------------\n",
      "‚úó Sample 0: predicted=T-shirt/top  actual=Ankle boot\n",
      "‚úó Sample 1: predicted=Ankle boot   actual=Pullover\n",
      "‚úó Sample 2: predicted=Bag          actual=Trouser\n",
      "‚úó Sample 3: predicted=Bag          actual=Trouser\n",
      "‚úó Sample 4: predicted=Ankle boot   actual=Shirt\n",
      "‚úó Sample 5: predicted=Bag          actual=Trouser\n",
      "‚úó Sample 6: predicted=Dress        actual=Coat\n",
      "‚úì Sample 7: predicted=Shirt        actual=Shirt\n",
      "‚úó Sample 8: predicted=Bag          actual=Sandal\n",
      "‚úó Sample 9: predicted=Bag          actual=Sneaker\n",
      "--------------------------------------------------\n",
      "Accuracy on samples: 1/10 (10%)\n"
     ]
    }
   ],
   "source": [
    "# Run inference on sample images\n",
    "print(\"\\nSample predictions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for i in range(10):\n",
    "        image = test_data.data[i].reshape(1, 1, 28, 28).float()\n",
    "        output = inference_model(image)\n",
    "        predicted = output.argmax().item()\n",
    "        actual = test_data.targets[i].item()\n",
    "        correct += (predicted == actual)\n",
    "        \n",
    "        status = \"‚úì\" if predicted == actual else \"‚úó\"\n",
    "        print(f\"{status} Sample {i}: predicted={CLASSES[predicted]:12s} actual={CLASSES[actual]}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Accuracy on samples: {correct}/10 ({correct*10}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. **Configure FSDP2** with memory optimization strategies:\n",
    "   - CPU offloading for reduced GPU memory\n",
    "   - Mixed precision (FP16) for faster training\n",
    "   - Resharding after forward pass\n",
    "\n",
    "2. **Integrate with Ray Train** for distributed training:\n",
    "   - `TorchTrainer` for multi-GPU/multi-node training\n",
    "   - Automatic process group initialization\n",
    "   - Fault tolerance with checkpoint recovery\n",
    "\n",
    "3. **Use PyTorch Distributed Checkpoint (DCP)**:\n",
    "   - Sharded saving for parallel I/O\n",
    "   - Automatic resharding on load\n",
    "   - Model consolidation for inference\n",
    "\n",
    "4. **Profile GPU memory** using PyTorch's memory snapshot API\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Scale up**: Try more workers or larger models\n",
    "- **Hybrid parallelism**: Combine FSDP2 with tensor/pipeline parallelism\n",
    "- **Production deployment**: Use cloud storage (S3, GCS) for checkpoints\n",
    "- **Hyperparameter tuning**: Integrate with Ray Tune\n",
    "- **Try DeepSpeed**: See the [DeepSpeed tutorial](./DeepSpeed_RayTrain_Tutorial.ipynb) for an alternative approach\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [PyTorch FSDP2 Tutorial](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n",
    "- [Ray Train Documentation](https://docs.ray.io/en/latest/train/getting-started-pytorch.html)\n",
    "- [PyTorch DCP Guide](https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
