{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e495450c",
   "metadata": {},
   "source": [
    "# Get Started with PyTorch FSDP2 and Ray Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab037eed",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates how to train large models using PyTorch's Fully Sharded Data Parallel (FSDP2) with Ray Train. FSDP2 enables model sharding across multiple GPUs, reducing memory footprint compared to standard DDP.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Configure FSDP2 sharding for distributed training\n",
    "2. Use PyTorch Distributed Checkpoint (DCP) for sharded model checkpointing\n",
    "3. Load trained models for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf614847",
   "metadata": {},
   "source": [
    "This notebook will walk you through a high level overview of using FSDP with Ray Train.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Here is the roadmap for this notebook:\n",
    "\n",
    "<ol>\n",
    "  <li>What is FSDP?</li>\n",
    "  <li>FSDP vs DDP simplified</li>\n",
    "  <li>How to use FSDP (v2) with Ray Train?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc9f9c",
   "metadata": {},
   "source": [
    "\n",
    "## What is FSDP2?\n",
    "\n",
    "[FSDP2](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) is PyTorch's native solution for training large models:\n",
    "\n",
    "- Shards model parameters, gradients, and optimizer states across workers\n",
    "- All-gathers parameters during forward pass, then re-shards\n",
    "- Enables training models larger than single GPU memory\n",
    "\n",
    "**When to use FSDP2:**\n",
    "- Model exceeds single GPU memory\n",
    "- You want native PyTorch integration\n",
    "- Building custom training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fc785",
   "metadata": {},
   "source": [
    "### FSDP Workflow\n",
    "\n",
    "Below is a diagram (taken and adapted from PyTorch) that shows the FSDP workflow\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/FSDP.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b402013",
   "metadata": {},
   "source": [
    "\n",
    "Here is a table explaining the different phases, their steps and what happens:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Phase</th>\n",
    "      <th>Step</th>\n",
    "      <th>What Happens</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Initialization</td>\n",
    "      <td>Parameter sharding</td>\n",
    "      <td>Each rank stores only its own shard of every parameter it owns.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Forward pass</td>\n",
    "      <td>\n",
    "        <ol>\n",
    "          <li>all_gather</li>\n",
    "          <li>Compute</li>\n",
    "          <li>Free full weights</li>\n",
    "        </ol>\n",
    "      </td>\n",
    "      <td>Ranks gather one another’s shards to reconstruct full weights(parameters), execute the forward computation, then immediately free the temporary shards.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Backward pass</td>\n",
    "      <td>\n",
    "        <ol>\n",
    "          <li>all_gather</li>\n",
    "          <li>Back-propagate</li>\n",
    "          <li>reduce_scatter</li>\n",
    "          <li>Free full weights</li>\n",
    "        </ol>\n",
    "      </td>\n",
    "      <td>Shards are re-gathered, gradients are computed, then reduced-and-scattered so each rank keeps only its own gradient shard; full replicas are discarded again.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180b274",
   "metadata": {},
   "source": [
    "## FSDP vs DDP simplified\n",
    "\n",
    "Let's go over a toy example  (inspired by [this guide on parallelism from huggingface](https://huggingface.co/docs/transformers/v4.13.0/en/parallelism)) comparing how FSDP and DDP operate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41726255",
   "metadata": {},
   "source": [
    "### 1  Toy model\n",
    "\n",
    "| **La** | **Lb** | **Lc** |\n",
    "|:------:|:------:|:------:|\n",
    " a₀ | b₀ | c₀ |\n",
    " a₁ | b₁ | c₁ |\n",
    " a₂ | b₂ | c₂ |\n",
    "\n",
    "Layer `La` contains the weights `[a₀, a₁, a₂]`\n",
    "\n",
    "Total parameters = **9 scalars** (3 per layer).\n",
    "\n",
    "---\n",
    "\n",
    "### 2  Parameter layout at rest  \n",
    "\n",
    "#### 2.1  DDP  (full replication)\n",
    "\n",
    "| **GPU** | **La**           | **Lb**           | **Lc**           |\n",
    "|---------|------------------|------------------|------------------|\n",
    "| 0       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "| 1       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "| 2       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "\n",
    "*Each worker stores **100 %** of the model (parameters + optimizer states + gradients).*\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2  FSDP  (parameter sharding)\n",
    "\n",
    "| **GPU** | **La** | **Lb** | **Lc** |\n",
    "|---------|:------:|:------:|:------:|\n",
    "| 0       | a₀     | b₀     | c₀     |\n",
    "| 1       | a₁     | b₁     | c₁     |\n",
    "| 2       | a₂     | b₂     | c₂     |\n",
    "\n",
    "*At rest each worker keeps only **1∕3** of every layer—so memory ≈ 33 % of DDP.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3  Execution flow (GPU 0’s perspective)\n",
    "\n",
    "#### 3.1  FSDP\n",
    "\n",
    "<details>\n",
    "<summary><strong>Forward pass (Layer La)</strong></summary>\n",
    "\n",
    "1. **Need:** a₀ a₁ a₂  \n",
    "2. **Has locally:** a₀  \n",
    "3. **Gather:** a₁ from GPU 1, a₂ from GPU 2  \n",
    "4. **Compute:** *y = La(x)* with full weights  \n",
    "5. **Free:** a₁, a₂ (optional halfway-release)\n",
    "\n",
    "Repeat for **Lb** then **Lc**.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Backward pass (Layer Lc)</strong></summary>\n",
    "\n",
    "1. **Need:** c₀ c₁ c₂  \n",
    "2. **Has locally:** c₀  \n",
    "3. **Gather:** c₁ from GPU 1, c₂ from GPU 2  \n",
    "4. **Compute:** ∂L/∂c₀, ∂L/∂c₁, ∂L/∂c₂  \n",
    "5. **Reduce-scatter:** average gradients; GPU k keeps only its own shard (cₖ)  \n",
    "6. **Free:** c₁, c₂ (optional halfway-release)\n",
    "\n",
    "Work upstream through **Lb → La** in reverse order.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Optimizer step</strong></summary>\n",
    "\n",
    "*Purely local.*  \n",
    "GPU 0 updates a₀, b₀, c₀ using the averaged gradient shards already resident in memory. No extra communication.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2  DDP  (for comparison)\n",
    "\n",
    "| Phase             | What GPU 0 already owns         | Communication |\n",
    "|-------------------|---------------------------------|---------------|\n",
    "| **Forward**       | Full La, Lb, Lc                 | None          |\n",
    "| **Backward**      | Full La, Lb, Lc + grads         | **All-reduce**|\n",
    "| **Optimizer step**| Full params & full grads        | None          |\n",
    "\n",
    "---\n",
    "\n",
    "### 4  Key take-aways\n",
    "\n",
    "|                         | **DDP**                           | **FSDP**                               |\n",
    "|-------------------------|-----------------------------------|----------------------------------------|\n",
    "| **Memory footprint**    | Replicated (× #GPUs)              | 1∕#GPUs at rest; slightly higher during gather |\n",
    "| **Communication**       | All-reduce once per layer (backward) | Gather + reduce-scatter per layer      |\n",
    "| **Optimizer states**    | Fully replicated                  | Sharded—1∕#GPUs memory|\n",
    "| **Implementation ease** | Very simple                       | More knobs (wrapping policy, offloading,prefetch) |\n",
    "| **When to prefer**      | Fits in memory; small models      | Large models that would otherwise OOM  |\n",
    "\n",
    "> **Rule of thumb:**  \n",
    "> *If you can replicate the model comfortably, DDP wins on simplicity and sometimes speed.  \n",
    "> If you’re out of memory or pushing model scale, FSDP (or ZeRO-style sharding) is the way forward.*\n",
    "\n",
    "---\n",
    "\n",
    "#### 5  Why we wrapped each layer separately\n",
    "\n",
    "For illustration we treated **“one layer = one FSDP unit.”**  \n",
    "In practice you can:\n",
    "\n",
    "* **Cluster layers** into larger FSDP units to reduce the number of gather calls.\n",
    "* **Wrap only the largest sub-modules** (e.g., big embeddings, attention blocks) and leave tiny layers unsharded.\n",
    "\n",
    "Experiment with *auto-wrap policies* to find the sweet spot between memory savings and communication overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a4689",
   "metadata": {},
   "source": [
    "## When to Consider FSDP\n",
    "\n",
    "- **Model no longer fits** on a single GPU even with mixed precision.  \n",
    "- **Batch size is GPU memory-bound** under classic DDP.  \n",
    "- **You have multiple GPUs** with sufficient interconnect bandwidth.  \n",
    "- **You already use DDP** but need to push to larger architectures (e.g., multi-billion-parameter transformers).  \n",
    "- **You want minimal code changes**—wrap layers with `torch.distributed.fsdp.FullyShardedDataParallel`.  \n",
    "\n",
    "FSDP lets you step beyond the memory limits of traditional data parallelism while keeping your training loop largely unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d868ce",
   "metadata": {},
   "source": [
    "### What is FSDP?\n",
    "\n",
    "Fully Sharded Data Parallel (FSDP) is a parallelism method that combines the advantages of data and model parallelism for distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ee6cd",
   "metadata": {},
   "source": [
    "### FSDP Workflow\n",
    "\n",
    "Below is a diagram (taken and adapted from PyTorch) that shows the FSDP workflow\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/FSDP.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16da40",
   "metadata": {},
   "source": [
    "\n",
    "Here is a table explaining the different phases, their steps and what happens:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Phase</th>\n",
    "      <th>Step</th>\n",
    "      <th>What Happens</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Initialization</td>\n",
    "      <td>Parameter sharding</td>\n",
    "      <td>Each rank stores only its own shard of every parameter it owns.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Forward pass</td>\n",
    "      <td>\n",
    "        <ol>\n",
    "          <li>all_gather</li>\n",
    "          <li>Compute</li>\n",
    "          <li>Free full weights</li>\n",
    "        </ol>\n",
    "      </td>\n",
    "      <td>Ranks gather one another’s shards to reconstruct full weights(parameters), execute the forward computation, then immediately free the temporary shards.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Backward pass</td>\n",
    "      <td>\n",
    "        <ol>\n",
    "          <li>all_gather</li>\n",
    "          <li>Back-propagate</li>\n",
    "          <li>reduce_scatter</li>\n",
    "          <li>Free full weights</li>\n",
    "        </ol>\n",
    "      </td>\n",
    "      <td>Shards are re-gathered, gradients are computed, then reduced-and-scattered so each rank keeps only its own gradient shard; full replicas are discarded again.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa7eea",
   "metadata": {},
   "source": [
    "### FSDP vs DDP simplified\n",
    "\n",
    "Let's go over a toy example  (inspired by [this guide on parallelism from huggingface](https://huggingface.co/docs/transformers/v4.13.0/en/parallelism)) comparing how FSDP and DDP operate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c20dce",
   "metadata": {},
   "source": [
    "### 1  Toy model\n",
    "\n",
    "| **La** | **Lb** | **Lc** |\n",
    "|:------:|:------:|:------:|\n",
    " a₀ | b₀ | c₀ |\n",
    " a₁ | b₁ | c₁ |\n",
    " a₂ | b₂ | c₂ |\n",
    "\n",
    "Layer `La` contains the weights `[a₀, a₁, a₂]`\n",
    "\n",
    "Total parameters = **9 scalars** (3 per layer).\n",
    "\n",
    "---\n",
    "\n",
    "### 2  Parameter layout at rest  \n",
    "\n",
    "#### 2.1  DDP  (full replication)\n",
    "\n",
    "| **GPU** | **La**           | **Lb**           | **Lc**           |\n",
    "|---------|------------------|------------------|------------------|\n",
    "| 0       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "| 1       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "| 2       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "\n",
    "*Each worker stores **100 %** of the model (parameters + optimizer states + gradients).*\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2  FSDP  (parameter sharding)\n",
    "\n",
    "| **GPU** | **La** | **Lb** | **Lc** |\n",
    "|---------|:------:|:------:|:------:|\n",
    "| 0       | a₀     | b₀     | c₀     |\n",
    "| 1       | a₁     | b₁     | c₁     |\n",
    "| 2       | a₂     | b₂     | c₂     |\n",
    "\n",
    "*At rest each worker keeps only **1∕3** of every layer—so memory ≈ 33 % of DDP.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3  Execution flow (GPU 0’s perspective)\n",
    "\n",
    "#### 3.1  FSDP\n",
    "\n",
    "<details>\n",
    "<summary><strong>Forward pass (Layer La)</strong></summary>\n",
    "\n",
    "1. **Need:** a₀ a₁ a₂  \n",
    "2. **Has locally:** a₀  \n",
    "3. **Gather:** a₁ from GPU 1, a₂ from GPU 2  \n",
    "4. **Compute:** *y = La(x)* with full weights  \n",
    "5. **Free:** a₁, a₂ (optional halfway-release)\n",
    "\n",
    "Repeat for **Lb** then **Lc**.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Backward pass (Layer Lc)</strong></summary>\n",
    "\n",
    "1. **Need:** c₀ c₁ c₂  \n",
    "2. **Has locally:** c₀  \n",
    "3. **Gather:** c₁ from GPU 1, c₂ from GPU 2  \n",
    "4. **Compute:** ∂L/∂c₀, ∂L/∂c₁, ∂L/∂c₂  \n",
    "5. **Reduce-scatter:** average gradients; GPU k keeps only its own shard (cₖ)  \n",
    "6. **Free:** c₁, c₂ (optional halfway-release)\n",
    "\n",
    "Work upstream through **Lb → La** in reverse order.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Optimizer step</strong></summary>\n",
    "\n",
    "*Purely local.*  \n",
    "GPU 0 updates a₀, b₀, c₀ using the averaged gradient shards already resident in memory. No extra communication.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2  DDP  (for comparison)\n",
    "\n",
    "| Phase             | What GPU 0 already owns         | Communication |\n",
    "|-------------------|---------------------------------|---------------|\n",
    "| **Forward**       | Full La, Lb, Lc                 | None          |\n",
    "| **Backward**      | Full La, Lb, Lc + grads         | **All-reduce**|\n",
    "| **Optimizer step**| Full params & full grads        | None          |\n",
    "\n",
    "---\n",
    "\n",
    "### 4  Key take-aways\n",
    "\n",
    "|                         | **DDP**                           | **FSDP**                               |\n",
    "|-------------------------|-----------------------------------|----------------------------------------|\n",
    "| **Memory footprint**    | Replicated (× #GPUs)              | 1∕#GPUs at rest; slightly higher during gather |\n",
    "| **Communication**       | All-reduce once per layer (backward) | Gather + reduce-scatter per layer      |\n",
    "| **Optimizer states**    | Fully replicated                  | Sharded—1∕#GPUs memory|\n",
    "| **Implementation ease** | Very simple                       | More knobs (wrapping policy, offloading,prefetch) |\n",
    "| **When to prefer**      | Fits in memory; small models      | Large models that would otherwise OOM  |\n",
    "\n",
    "> **Rule of thumb:**  \n",
    "> *If you can replicate the model comfortably, DDP wins on simplicity and sometimes speed.  \n",
    "> If you’re out of memory or pushing model scale, FSDP (or ZeRO-style sharding) is the way forward.*\n",
    "\n",
    "---\n",
    "\n",
    "#### 5  Why we wrapped each layer separately\n",
    "\n",
    "For illustration we treated **“one layer = one FSDP unit.”**  \n",
    "In practice you can:\n",
    "\n",
    "* **Cluster layers** into larger FSDP units to reduce the number of gather calls.\n",
    "* **Wrap only the largest sub-modules** (e.g., big embeddings, attention blocks) and leave tiny layers unsharded.\n",
    "\n",
    "Experiment with *auto-wrap policies* to find the sweet spot between memory savings and communication overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83f43a",
   "metadata": {},
   "source": [
    "### When to Consider FSDP\n",
    "\n",
    "- **Model no longer fits** on a single GPU even with mixed precision.  \n",
    "- **Batch size is GPU memory-bound** under classic DDP.  \n",
    "- **You have multiple GPUs** with sufficient interconnect bandwidth.  \n",
    "- **You already use DDP** but need to push to larger architectures (e.g., multi-billion-parameter transformers).  \n",
    "- **You want minimal code changes**—wrap layers with `torch.distributed.fsdp.FullyShardedDataParallel`.  \n",
    "\n",
    "FSDP lets you step beyond the memory limits of traditional data parallelism while keeping your training loop largely unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a4b3b",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Check Ray cluster status and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea41fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Autoscaler status: 2026-02-18 01:48:49.477942 ========\n",
      "Node status\n",
      "---------------------------------------------------------------\n",
      "Active:\n",
      " 1 head\n",
      " 1 1xL4:16CPU-64GB-2\n",
      "Idle:\n",
      " 1 1xL4:16CPU-64GB-1\n",
      "Pending:\n",
      " (no pending nodes)\n",
      "Recent failures:\n",
      " (no failures)\n",
      "\n",
      "Resources\n",
      "---------------------------------------------------------------\n",
      "Total Usage:\n",
      " 0.0/32.0 CPU\n",
      " 0.0/2.0 GPU\n",
      " 0.0/2.0 anyscale/accelerator_shape:1xL4\n",
      " 0.0/1.0 anyscale/cpu_only:true\n",
      " 0.0/1.0 anyscale/node-group:1xL4:16CPU-64GB-1\n",
      " 0.0/1.0 anyscale/node-group:1xL4:16CPU-64GB-2\n",
      " 0.0/1.0 anyscale/node-group:head\n",
      " 0.0/3.0 anyscale/provider:aws\n",
      " 0.0/3.0 anyscale/region:us-west-2\n",
      " 0B/160.00GiB memory\n",
      " 10.41KiB/44.64GiB object_store_memory\n",
      "\n",
      "From request_resources:\n",
      " (none)\n",
      "Pending Demands:\n",
      " (no resource demands)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Check Ray cluster status\n",
    "!ray status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fccaeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stdlib imports\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Ray Train imports\n",
    "import ray\n",
    "import ray.train\n",
    "import ray.train.torch\n",
    "\n",
    "# PyTorch core and FSDP2 imports\n",
    "import torch\n",
    "from torch.distributed.fsdp import (\n",
    "    fully_shard,\n",
    "    FSDPModule,\n",
    "    CPUOffloadPolicy,\n",
    "    MixedPrecisionPolicy,\n",
    ")\n",
    "\n",
    "# PyTorch Distributed Checkpoint (DCP) imports\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    get_state_dict,\n",
    "    set_state_dict,\n",
    "    get_model_state_dict,\n",
    "    StateDictOptions\n",
    ")\n",
    "from torch.distributed.device_mesh import init_device_mesh \n",
    "from torch.distributed.checkpoint.stateful import Stateful\n",
    "import torch.distributed.checkpoint as dcp\n",
    "\n",
    "# PyTorch training components\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Computer vision components\n",
    "from torchvision.models import VisionTransformer\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077a1b4",
   "metadata": {},
   "source": [
    "## Step 2: Model Definition\n",
    "\n",
    "We use a Vision Transformer (ViT) with repeatable encoder blocks - ideal for demonstrating FSDP2's per-layer sharding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459e9e2",
   "metadata": {},
   "source": [
    "Let's go over a simple example of how to use FSDP with Ray Train and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a6fc7",
   "metadata": {},
   "source": [
    "Below is a sample training function that we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57bd20bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    # Step 1: Initialize the model\n",
    "    model = init_model(config[\"hidden_dim\"])\n",
    "\n",
    "    # Configure device and move model to GPU\n",
    "    device = ray.train.torch.get_device()\n",
    "    torch.cuda.set_device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Step 2: Apply FSDP2 sharding to the model\n",
    "    prepare_model(\n",
    "        model,\n",
    "        skip_model_shard=config[\"skip_model_shard\"],\n",
    "        skip_cpu_offload=config[\"skip_cpu_offload\"],\n",
    "        use_float16=config[\"use_float16\"],\n",
    "    )\n",
    "\n",
    "    # Step 3: Initialize loss function and optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.get(\"learning_rate\", 0.001))\n",
    "\n",
    "    # Step 4: Load from checkpoint if available (for resuming training)\n",
    "    start_epoch = 0\n",
    "    loaded_checkpoint = ray.train.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        start_epoch = load_fsdp_checkpoint(model, optimizer, loaded_checkpoint)\n",
    "\n",
    "    # Step 5: Prepare training data\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = FashionMNIST(\n",
    "        root=data_dir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=config.get(\"batch_size\", 128), shuffle=True, num_workers=2\n",
    "    )\n",
    "    # Prepare data loader for distributed training\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "\n",
    "    world_rank = ray.train.get_context().get_world_rank()\n",
    "\n",
    "    # Step 6: Main training loop\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # Set epoch for distributed sampler to ensure proper shuffling\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            # Note: Data is automatically moved to the correct device by prepare_data_loader\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Standard training step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track metrics\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # Step 7: Report metrics and save checkpoint after each epoch\n",
    "        avg_loss = running_loss / num_batches\n",
    "        metrics = {\"loss\": avg_loss, \"epoch\": epoch + 1}\n",
    "        report_metrics_and_save_fsdp_checkpoint(model, optimizer, metrics, epoch + 1)\n",
    "\n",
    "        # Log metrics from rank 0 only to avoid duplicate outputs\n",
    "        if world_rank == 0:\n",
    "            print(metrics)\n",
    "\n",
    "    # Step 8: Save the final model for inference\n",
    "    save_model_for_inference(model, world_rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3df2a2",
   "metadata": {},
   "source": [
    "### Initialize the model\n",
    "Initialize a Vision Transformer model for FashionMNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e494626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(hidden_dim) -> torch.nn.Module:\n",
    "    # Create a ViT model with architecture suitable for 28x28 images\n",
    "    model = VisionTransformer(\n",
    "        image_size=28,         # FashionMNIST image size\n",
    "        patch_size=7,          # Divide 28x28 into 4x4 patches of 7x7 pixels each\n",
    "        num_layers=12,         # Number of transformer encoder layers\n",
    "        num_heads=8,           # Number of attention heads per layer\n",
    "        hidden_dim=hidden_dim, # Hidden dimension size\n",
    "        mlp_dim=768,           # MLP dimension in transformer blocks\n",
    "        num_classes=10,        # FashionMNIST has 10 classes\n",
    "    )\n",
    "\n",
    "    # Modify the patch embedding layer for grayscale images (1 channel instead of 3)\n",
    "    model.conv_proj = torch.nn.Conv2d(\n",
    "        in_channels=1,            # FashionMNIST is grayscale (1 channel)\n",
    "        out_channels=hidden_dim,  # Must match the hidden_dim\n",
    "        kernel_size=7,            # Match patch_size\n",
    "        stride=7,                 # Non-overlapping patches\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd466bb1",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the model and FSDP2 Sharding Configuration\n",
    "\n",
    "To prepare the model, we use the `fully_shard` (FSDP2) function from pytorch but we have to first:\n",
    "1. Create a device mesh\n",
    "2. Apply `fully_shard` to each block we want to shard\n",
    "3. Apply `fully_shard` to the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138fa9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(\n",
    "    model: torch.nn.Module,\n",
    "    skip_model_shard: bool,\n",
    "    skip_cpu_offload: bool,\n",
    "    use_float16: bool,\n",
    "):\n",
    "    # Step 1: Create 1D device mesh for data parallel sharding\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    mesh = init_device_mesh(\n",
    "        device_type=\"cuda\", mesh_shape=(world_size,), mesh_dim_names=(\"data_parallel\",)\n",
    "    )\n",
    "\n",
    "    # Step 2: Configure CPU offloading policy (optional)\n",
    "    offload_policy = CPUOffloadPolicy() if not skip_cpu_offload else None\n",
    "\n",
    "    # Step 3: Configure mixed precision policy (optional)\n",
    "    mp_policy_float16 = MixedPrecisionPolicy(\n",
    "        param_dtype=torch.float16,  # Store parameters in half precision\n",
    "        reduce_dtype=torch.float16,  # Use half precision for gradient reduction\n",
    "    )\n",
    "    default_policy = MixedPrecisionPolicy(\n",
    "        param_dtype=None,\n",
    "        reduce_dtype=None,\n",
    "        output_dtype=None,\n",
    "        cast_forward_inputs=True\n",
    "    )\n",
    "    mp_policy = mp_policy_float16 if use_float16 else default_policy\n",
    "\n",
    "    # Step 4: Apply sharding to each transformer encoder block\n",
    "    for encoder_block in model.encoder.layers.children():\n",
    "        fully_shard(\n",
    "            encoder_block,\n",
    "            mesh=mesh,\n",
    "            reshard_after_forward=not skip_model_shard,\n",
    "            offload_policy=offload_policy,\n",
    "            mp_policy=mp_policy,\n",
    "        )\n",
    "\n",
    "    # Step 5: Apply sharding to the root model\n",
    "    # This wraps the entire model and enables top-level FSDP2 functionality\n",
    "    fully_shard(\n",
    "        model,\n",
    "        mesh=mesh,\n",
    "        reshard_after_forward=not skip_model_shard,\n",
    "        offload_policy=offload_policy,\n",
    "        mp_policy=mp_policy,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce5091",
   "metadata": {},
   "source": [
    "Here is a table of the main keyword arguments for FSDP:\n",
    "\n",
    "| Parameter | What it controls | Typical values & when to use |\n",
    "|-----------|-----------------|------------------------------|\n",
    "| `reshard_after_forward` | Whether to free (reshard) parameters immediately after forward pass to save memory | **`True`** – free parameters after forward pass for maximum memory savings; increases communication overhead.<br>**`False`** (default) – keep parameters in memory through backward pass; faster but uses more memory. |\n",
    "| `offload_policy` | Whether inactive parameter shards are moved to CPU RAM | **`None`** (default) – fastest, keeps all data on GPU.<br>**`CPUOffloadPolicy()`** – offloads parameters to CPU RAM; frees GPU memory at the cost of extra PCIe traffic; enables larger models on small-RAM GPUs. |\n",
    "| `mp_policy` | Controls mixed precision settings for parameters and gradients | **`None`** (default) – use model's native precision (typically float32).<br>**`MixedPrecisionPolicy(param_dtype=torch.float16)`** – store parameters in half precision for memory savings and to leverage tensor cores.<br>**`MixedPrecisionPolicy(param_dtype=torch.bfloat16)`** – use bfloat16 for newer architectures (A100, H100) with better numerical stability and tensor core acceleration.<br>**`MixedPrecisionPolicy(param_dtype=torch.bfloat16, reduce_dtype=torch.float32)`** – commonly used configuration; bfloat16 parameters with float32 gradient reduction for better numerical stability during reduce-scatter operations.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df808d",
   "metadata": {},
   "source": [
    "## Step 4: Distributed Checkpointing with PyTorch\n",
    "\n",
    "`torch.distributed.checkpoint()` enables saving and loading models from multiple ranks in parallel. You can use this module to save on any number of ranks in parallel, and then re-shard across differing cluster topologies at load time.\n",
    "\n",
    "PyTorch Distributed Checkpoint (DCP) provides efficient checkpointing for sharded models:\n",
    "- Each worker saves only its shard (parallel I/O)\n",
    "- Automatic resharding on load if worker count changes\n",
    "- Full optimizer state support for training resumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ac9d0",
   "metadata": {},
   "source": [
    "### Defining a Stateful object\n",
    "\n",
    "We take advantage of a Stateful object to handle calling distributed state dict methods on the model and optimizer.\n",
    "\n",
    "This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n",
    "with the Stateful protocol, PyTorch DCP will automatically call state_dict/load_stat_dict as needed in the\n",
    "dcp.save/load APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86135932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppState(Stateful):\n",
    "    def __init__(self, model, optimizer=None, epoch=0):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def state_dict(self):\n",
    "        # this line automatically manages FSDP2 FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n",
    "        model_state_dict, optimizer_state_dict = get_state_dict(\n",
    "            self.model, self.optimizer\n",
    "        )\n",
    "        return {\n",
    "            \"model\": model_state_dict,\n",
    "            \"optim\": optimizer_state_dict,\n",
    "            \"epoch\": self.epoch,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        # sets our state dicts on the model and optimizer, now that we've loaded\n",
    "        set_state_dict(\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "            model_state_dict=state_dict[\"model\"],\n",
    "            optim_state_dict=state_dict[\"optim\"],\n",
    "        )\n",
    "        # Load epoch state\n",
    "        self.epoch = state_dict[\"epoch\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b2aea",
   "metadata": {},
   "source": [
    "### Saving Distributed Checkpoints when training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361eb25",
   "metadata": {},
   "source": [
    "This function performs two critical operations:\n",
    "1. Saves the current model and optimizer state using distributed checkpointing\n",
    "2. Reports metrics to Ray Train for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4073af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics_and_save_fsdp_checkpoint(\n",
    "    model: FSDPModule, optimizer: torch.optim.Optimizer, metrics: dict, epoch: int\n",
    ") -> None:\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        # Perform a distributed checkpoint with DCP\n",
    "        state_dict = {\"app\": AppState(model, optimizer, epoch)}\n",
    "        dcp.save(state_dict=state_dict, checkpoint_id=temp_checkpoint_dir)\n",
    "\n",
    "        # Report each checkpoint shard from all workers\n",
    "        # This saves the checkpoint to shared cluster storage for persistence\n",
    "        checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "        ray.train.report(metrics, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d33c2",
   "metadata": {},
   "source": [
    "### Saving a final and full checkpoint for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9179e8",
   "metadata": {},
   "source": [
    "For inference, we want to save an unsharded copy of the model. This is an expensive operation given all parameters need to gather all model parameters from all ranks onto a single rank.\n",
    "\n",
    "This function consolidates the distributed model weights into a single checkpoint file that can be used for inference without FSDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a55a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_inference(model: FSDPModule, world_rank: int) -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        save_file = os.path.join(temp_checkpoint_dir, \"full-model.pt\")\n",
    "\n",
    "        # Step 1: All-gather the model state across all ranks\n",
    "        # This reconstructs the complete model from distributed shards\n",
    "        model_state_dict = get_model_state_dict(\n",
    "            model=model,\n",
    "            options=StateDictOptions(\n",
    "                full_state_dict=True,  # Reconstruct full model\n",
    "                cpu_offload=True,  # Move to CPU to save GPU memory\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        checkpoint = None\n",
    "\n",
    "        # Step 2: Save the complete model (rank 0 only)\n",
    "        if world_rank == 0:\n",
    "            torch.save(model_state_dict, save_file)\n",
    "\n",
    "            # Create checkpoint for shared storage\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        # Step 3: Report the final checkpoint to Ray Train\n",
    "        ray.train.report(metrics={}, checkpoint=checkpoint, checkpoint_dir_name=\"full_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020b48f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**NOTE:** In PyTorch, if both `cpu_offload` and `full_state_dict` are set to True, then only the rank0 will get the state_dict and all other ranks will get empty state_dict.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6c726",
   "metadata": {},
   "source": [
    "### Loading distributed checkpoints\n",
    "\n",
    "This function handles distributed checkpoint loading with automatic resharding support. It can restore checkpoints even when the number of workers differs from the original training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b0d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fsdp_checkpoint(\n",
    "    model: FSDPModule, optimizer: torch.optim.Optimizer, ckpt: ray.train.Checkpoint\n",
    ") -> int:\n",
    "    try:\n",
    "        with ckpt.as_directory() as checkpoint_dir:\n",
    "            # Create state wrapper for DCP loading\n",
    "            app_state = AppState(model, optimizer)\n",
    "            state_dict = {\"app\": app_state}\n",
    "\n",
    "            # Load the distributed checkpoint\n",
    "            dcp.load(state_dict=state_dict, checkpoint_id=checkpoint_dir)\n",
    "\n",
    "            # Return the update state's epoch\n",
    "            return app_state.epoch\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Checkpoint loading failed: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089b2e3",
   "metadata": {},
   "source": [
    "## Step 5: Final Training Configuration \n",
    "\n",
    "The training function runs on each worker:\n",
    "1. Initialize and shard model with FSDP2\n",
    "2. Run training loop with distributed data loading\n",
    "3. Save checkpoints using PyTorch DCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74850f6",
   "metadata": {},
   "source": [
    "Configure scaling and resource requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07f8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ray.train.ScalingConfig(\n",
    "    num_workers=2, use_gpu=True, resources_per_worker={\"accelerator_type:L4\": 0.0001}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b7903",
   "metadata": {},
   "source": [
    "Launch the distributed training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6767622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"epochs\": 1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"skip_model_shard\": True,\n",
    "    \"skip_cpu_offload\": True,\n",
    "    \"hidden_dim\": 3840,\n",
    "    \"use_float16\": False,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a817abe",
   "metadata": {},
   "source": [
    "## Step 6: Launch Distributed Training\n",
    "\n",
    "Ray Train's `TorchTrainer` handles worker spawning, process group initialization, and checkpoint coordination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4784041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 01:48:54,476\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.229.255:6379...\n",
      "2026-02-18 01:48:54,487\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-ffbqdd398vb4g8i97u3tsubr23.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-18 01:48:54,684\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_6462b622da9fc2f79ba5e96e1c91e3f0eea96575.zip' (82.47MiB) to Ray cluster...\n",
      "2026-02-18 01:48:55,011\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_6462b622da9fc2f79ba5e96e1c91e3f0eea96575.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[36m(TrainController pid=59639)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=59639)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'accelerator_type:L4': 0.0001, 'GPU': 1}] * 2\n",
      "\u001b[36m(RayTrainWorker pid=12661, ip=10.0.210.195)\u001b[0m INFO:2026-02-18 01:49:01 12661:12661 init.cpp:148] Registering daemon config loader, cpuOnly =  0\n",
      "\u001b[36m(RayTrainWorker pid=12661, ip=10.0.210.195)\u001b[0m INFO:2026-02-18 01:49:01 12661:12661 CuptiActivityProfiler.cpp:244] CUDA versions. CUPTI: 26; Runtime: 12080; Driver: 12090\n",
      "\u001b[36m(RayTrainWorker pid=12661, ip=10.0.210.195)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TrainController pid=59639)\u001b[0m Started training worker group of size 2: \n",
      "\u001b[36m(TrainController pid=59639)\u001b[0m - (ip=10.0.210.195, pid=12661) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TrainController pid=59639)\u001b[0m - (ip=10.0.197.3, pid=12905) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TrainController pid=59639)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
      "\u001b[36m(RayTrainWorker pid=12905, ip=10.0.197.3)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist/checkpoint_2026-02-18_01-50-30.327889)\n",
      "\u001b[36m(RayTrainWorker pid=12905, ip=10.0.197.3)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist/checkpoint_2026-02-18_01-50-30.327889), metrics={'loss': 2.506196004152298, 'epoch': 1}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=12905, ip=10.0.197.3)\u001b[0m INFO:2026-02-18 01:49:01 12905:12905 init.cpp:148] Registering daemon config loader, cpuOnly =  0\n",
      "\u001b[36m(RayTrainWorker pid=12905, ip=10.0.197.3)\u001b[0m INFO:2026-02-18 01:49:01 12905:12905 CuptiActivityProfiler.cpp:244] CUDA versions. CUPTI: 26; Runtime: 12080; Driver: 12090\n",
      "\u001b[36m(RayTrainWorker pid=12661, ip=10.0.210.195)\u001b[0m {'loss': 2.486290919780731, 'epoch': 1}\n",
      "\u001b[36m(RayTrainWorker pid=12905, ip=10.0.197.3)\u001b[0m Reporting training result 2: TrainingReport(checkpoint=None, metrics={}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=12661, ip=10.0.210.195)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist/checkpoint_2026-02-18_01-50-30.327889)\n",
      "\u001b[36m(RayTrainWorker pid=12661, ip=10.0.210.195)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist/checkpoint_2026-02-18_01-50-30.327889), metrics={'loss': 2.486290919780731, 'epoch': 1}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=12661, ip=10.0.210.195)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist/full_model)\n",
      "\u001b[36m(RayTrainWorker pid=12661, ip=10.0.210.195)\u001b[0m Reporting training result 2: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist/full_model), metrics={}, validation_spec=None)\n",
      "\u001b[36m(TrainController pid=59639)\u001b[0m [State Transition] RUNNING -> SHUTTING_DOWN.\n"
     ]
    }
   ],
   "source": [
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_loop_config,\n",
    "    run_config=ray.train.RunConfig(\n",
    "        storage_path=\"/mnt/cluster_storage/\",\n",
    "        name=\"fsdp_mnist\",\n",
    "        failure_config=ray.train.FailureConfig(max_failures=2),\n",
    "        worker_runtime_env={\n",
    "            \"env_vars\": {\"KINETO_USE_DAEMON\": \"1\", \"KINETO_DAEMON_INIT_DELAY_S\": \"5\"}\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526296b",
   "metadata": {},
   "source": [
    "## Step 7: Inspect Training Artifacts\n",
    "\n",
    "Training artifacts include:\n",
    "- `checkpoint_*/` - Epoch checkpoints with distributed shards\n",
    "- `full_model/` - Consolidated model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8fdcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=59639)\u001b[0m [State Transition] SHUTTING_DOWN -> FINISHED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts in /mnt/cluster_storage/fsdp_mnist/:\n",
      "total 24\n",
      "drwxr-xr-x 22 ray  1000 6144 Feb 18 01:48 ..\n",
      "-rw-r--r--  1 ray users    0 Feb 18 01:48 .validate_storage_marker\n",
      "drwxr-xr-x  2 ray users 6144 Feb 18 01:50 checkpoint_2026-02-18_01-50-30.327889\n",
      "drwxr-xr-x  4 ray users 6144 Feb 18 01:51 .\n",
      "drwxr-xr-x  2 ray users 6144 Feb 18 01:51 full_model\n",
      "-rw-r--r--  1 ray users  334 Feb 18 01:52 checkpoint_manager_snapshot.json\n"
     ]
    }
   ],
   "source": [
    "# List artifacts\n",
    "storage_path = f\"/mnt/cluster_storage/fsdp_mnist/\"\n",
    "print(f\"Artifacts in {storage_path}:\")\n",
    "!ls -ltra $storage_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ba48a",
   "metadata": {},
   "source": [
    "## Step 8: Load Model for Inference\n",
    "\n",
    "The consolidated model (`full-model.pt`) is a standard PyTorch checkpoint that works without FSDP2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e880332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +6m28s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(autoscaler +6m28s)\u001b[0m Memory cgroup out of memory: Killed process 20108 (python) total-vm:15808636kB, anon-rss:9593532kB, file-rss:312064kB, shmem-rss:0kB, UID:1000 pgtables:20884kB oom_score_adj:-998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = init_model(train_loop_config[\"hidden_dim\"])\n",
    "model_state_dict = torch.load(\"/mnt/cluster_storage/fsdp_mnist/full_model/full-model.pt\", map_location='cpu')\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede567a3",
   "metadata": {},
   "source": [
    "Load some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "320412a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: .\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "test_data = FashionMNIST(\n",
    "    root=\".\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9db011",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77437fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label=4 test_label=9\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(test_data.data[0].reshape(1, 1, 28, 28).float())\n",
    "    predicted_label = out.argmax().item()\n",
    "    test_label = test_data.targets[0].item()\n",
    "    print(f\"{predicted_label=} {test_label=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b29d2",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed22fa",
   "metadata": {},
   "source": [
    "\n",
    "This tutorial covered:\n",
    "1. **FSDP2 sharding** - Distributed model parameters across GPUs using `fully_shard()`\n",
    "2. **Ray Train integration** - Multi-GPU training with automatic process group management\n",
    "3. **PyTorch DCP** - Sharded checkpointing with automatic resharding on load\n",
    "4. **Inference** - Loading consolidated model for single-GPU inference\n",
    "\n",
    "**Next Steps:**\n",
    "- Add CPU offloading: `CPUOffloadPolicy()` for memory-constrained scenarios\n",
    "- Add mixed precision: `MixedPrecisionPolicy(param_dtype=torch.float16)`\n",
    "- Try [DeepSpeed tutorial](./DeepSpeed_RayTrain_Tutorial.ipynb) for comparison\n",
    "\n",
    "**Resources:**\n",
    "- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n",
    "- [Ray Train Documentation](https://docs.ray.io/en/latest/train/getting-started-pytorch.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
