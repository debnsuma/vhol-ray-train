{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Get Started with PyTorch FSDP2 and Ray Train\n",
    "\n",
    "This notebook demonstrates how to train large models using PyTorch's Fully Sharded Data Parallel (FSDP2) with Ray Train. FSDP2 enables model sharding across multiple GPUs, reducing memory footprint compared to standard DDP.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Configure FSDP2 sharding for distributed training\n",
    "2. Use PyTorch Distributed Checkpoint (DCP) for sharded model checkpointing\n",
    "3. Load trained models for inference\n",
    "\n",
    "## What is FSDP2?\n",
    "\n",
    "[FSDP2](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) is PyTorch's native solution for training large models:\n",
    "\n",
    "- Shards model parameters, gradients, and optimizer states across workers\n",
    "- All-gathers parameters during forward pass, then re-shards\n",
    "- Enables training models larger than single GPU memory\n",
    "\n",
    "**When to use FSDP2:**\n",
    "- Model exceeds single GPU memory\n",
    "- You want native PyTorch integration\n",
    "- Building custom training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Ray cluster with GPU workers (this example uses 2x GPUs)\n",
    "- PyTorch 2.0+ with CUDA support\n",
    "- Shared storage accessible from all workers (e.g., `/mnt/cluster_storage/`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Check Ray cluster status and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Autoscaler status: 2026-02-02 06:50:21.467745 ========\n",
      "Node status\n",
      "---------------------------------------------------------------\n",
      "Active:\n",
      " 1 head\n",
      " 1 1xL4:16CPU-64GB-2\n",
      "Idle:\n",
      " 1 1xL4:16CPU-64GB-1\n",
      "Pending:\n",
      " (no pending nodes)\n",
      "Recent failures:\n",
      " (no failures)\n",
      "\n",
      "Resources\n",
      "---------------------------------------------------------------\n",
      "Total Usage:\n",
      " 0.0/32.0 CPU\n",
      " 0.0/2.0 GPU\n",
      " 0.0/2.0 anyscale/accelerator_shape:1xL4\n",
      " 0.0/1.0 anyscale/cpu_only:true\n",
      " 0.0/1.0 anyscale/node-group:1xL4:16CPU-64GB-1\n",
      " 0.0/1.0 anyscale/node-group:1xL4:16CPU-64GB-2\n",
      " 0.0/1.0 anyscale/node-group:head\n",
      " 0.0/3.0 anyscale/provider:aws\n",
      " 0.0/3.0 anyscale/region:us-west-2\n",
      " 0B/160.00GiB memory\n",
      " 16.30KiB/44.64GiB object_store_memory\n",
      "\n",
      "From request_resources:\n",
      " (none)\n",
      "Pending Demands:\n",
      " (no resource demands)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Check Ray cluster status\n",
    "!ray status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m#################\n",
      "\n",
      "ANYSCALE WARNING:\n",
      "Local packages tensorboard are not supported across cluster, please check our documentations for workarounds: https://docs.anyscale.com/configuration/dependency-management/dependency-development\n",
      "\n",
      "#################\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully registered `torch, torchvision` packages to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_g54aiirwj1s8t9ktgzikqur41k/prj_f1j47h9srml4cyg962id75ms2e/workspaces/expwrk_p5rbudbzwfjvieqiireatn2pzp?workspace-tab=dependencies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install -q torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "Ray version: 2.53.0\n"
     ]
    }
   ],
   "source": [
    "# Verify installation and check versions\n",
    "import torch\n",
    "import ray\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Ray version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "import tempfile\n",
    "import uuid\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 2: Model Definition\n",
    "\n",
    "We use a Vision Transformer (ViT) with repeatable encoder blocks - ideal for demonstrating FSDP2's per-layer sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,006,090\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import VisionTransformer\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "def init_model():\n",
    "    \"\"\"Initialize Vision Transformer for FashionMNIST (28x28 grayscale, 10 classes).\"\"\"\n",
    "    model = VisionTransformer(\n",
    "        image_size=28, patch_size=7, num_layers=10, num_heads=2,\n",
    "        hidden_dim=128, mlp_dim=128, num_classes=10,\n",
    "    )\n",
    "    # Modify for grayscale input\n",
    "    model.conv_proj = torch.nn.Conv2d(1, 128, kernel_size=7, stride=7)\n",
    "    return model\n",
    "\n",
    "# Verify model\n",
    "test_model = init_model()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "del test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 3: FSDP2 Sharding Configuration\n",
    "\n",
    "FSDP2's `fully_shard` API shards model parameters across workers:\n",
    "\n",
    "- **Device Mesh**: Describes cluster topology for data parallelism\n",
    "- **Per-layer sharding**: Shard each encoder block individually for fine-grained memory control\n",
    "- **Resharding**: Option to free all-gathered weights after forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed.fsdp import fully_shard\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "import ray.train\n",
    "\n",
    "def shard_model(model):\n",
    "    \"\"\"Apply FSDP2 sharding to the model.\"\"\"\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    \n",
    "    # Create device mesh for data parallelism\n",
    "    mesh = init_device_mesh(\"cuda\", (world_size,), mesh_dim_names=(\"dp\",))\n",
    "    \n",
    "    # Shard each encoder block individually\n",
    "    for block in model.encoder.layers.children():\n",
    "        fully_shard(block, mesh=mesh, reshard_after_forward=True)\n",
    "    \n",
    "    # Shard the root model\n",
    "    fully_shard(model, mesh=mesh, reshard_after_forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Distributed Checkpointing\n",
    "\n",
    "PyTorch Distributed Checkpoint (DCP) provides efficient checkpointing for sharded models:\n",
    "- Each worker saves only its shard (parallel I/O)\n",
    "- Automatic resharding on load if worker count changes\n",
    "- Full optimizer state support for training resumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict, get_model_state_dict, StateDictOptions\n",
    "from torch.distributed.checkpoint.stateful import Stateful\n",
    "import torch.distributed.checkpoint as dcp\n",
    "\n",
    "class AppState(Stateful):\n",
    "    \"\"\"Wrapper for DCP checkpointing.\"\"\"\n",
    "    def __init__(self, model, optimizer=None, epoch=None):\n",
    "        self.model, self.optimizer, self.epoch = model, optimizer, epoch\n",
    "\n",
    "    def state_dict(self):\n",
    "        model_sd, optim_sd = get_state_dict(self.model, self.optimizer)\n",
    "        return {\"model\": model_sd, \"optim\": optim_sd, \"epoch\": self.epoch}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        set_state_dict(self.model, self.optimizer,\n",
    "                      model_state_dict=state_dict[\"model\"],\n",
    "                      optim_state_dict=state_dict[\"optim\"])\n",
    "        self.epoch = state_dict.get(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, ckpt):\n",
    "    \"\"\"Load FSDP checkpoint (handles resharding automatically).\"\"\"\n",
    "    with ckpt.as_directory() as ckpt_dir:\n",
    "        app_state = AppState(model, optimizer)\n",
    "        dcp.load(state_dict={\"app\": app_state}, checkpoint_id=ckpt_dir)\n",
    "    return app_state.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, metrics, epoch):\n",
    "    \"\"\"Save FSDP checkpoint and report metrics.\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        dcp.save(state_dict={\"app\": AppState(model, optimizer, epoch)}, checkpoint_id=tmp_dir)\n",
    "        ray.train.report(metrics, checkpoint=ray.train.Checkpoint.from_directory(tmp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_inference(model, world_rank):\n",
    "    \"\"\"Consolidate sharded model for inference (rank 0 saves full model).\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        model_sd = get_model_state_dict(model, options=StateDictOptions(full_state_dict=True, cpu_offload=True))\n",
    "        ckpt = None\n",
    "        if world_rank == 0:\n",
    "            torch.save(model_sd, os.path.join(tmp_dir, \"full-model.pt\"))\n",
    "            ckpt = ray.train.Checkpoint.from_directory(tmp_dir)\n",
    "        ray.train.report({}, checkpoint=ckpt, checkpoint_dir_name=\"full_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 5: Training Function\n",
    "\n",
    "The training function runs on each worker:\n",
    "1. Initialize and shard model with FSDP2\n",
    "2. Run training loop with distributed data loading\n",
    "3. Save checkpoints using PyTorch DCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train.torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(config):\n",
    "    \"\"\"FSDP2 training function.\"\"\"\n",
    "    # Model setup\n",
    "    model = init_model()\n",
    "    device = ray.train.torch.get_device()\n",
    "    torch.cuda.set_device(device)\n",
    "    model.to(device)\n",
    "    shard_model(model)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.get('lr', 0.001))\n",
    "    \n",
    "    # Resume from checkpoint if available\n",
    "    start_epoch = 0\n",
    "    if ray.train.get_checkpoint():\n",
    "        start_epoch = load_checkpoint(model, optimizer, ray.train.get_checkpoint()) + 1\n",
    "    \n",
    "    # Data loading\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    train_data = FashionMNIST(root=tempfile.gettempdir(), train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=config.get('batch_size', 64), shuffle=True)\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    \n",
    "    # Context\n",
    "    world_rank = ray.train.get_context().get_world_rank()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, config.get('epochs', 1)):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        \n",
    "        total_loss, num_batches = 0.0, 0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        save_checkpoint(model, optimizer, {\"loss\": avg_loss, \"epoch\": epoch}, epoch)\n",
    "        if world_rank == 0:\n",
    "            print(f\"Epoch {epoch}: loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # Save final model for inference\n",
    "    save_model_for_inference(model, world_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 6: Launch Distributed Training\n",
    "\n",
    "Ray Train's `TorchTrainer` handles worker spawning, process group initialization, and checkpoint coordination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: fsdp_b2f564ce\n"
     ]
    }
   ],
   "source": [
    "import ray.train.torch\n",
    "\n",
    "# Configuration\n",
    "experiment_name = f\"fsdp_{uuid.uuid4().hex[:8]}\"\n",
    "scaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True)\n",
    "run_config = ray.train.RunConfig(storage_path=\"/mnt/cluster_storage/\", name=experiment_name)\n",
    "train_config = {\"epochs\": 1, \"lr\": 0.001, \"batch_size\": 64}\n",
    "\n",
    "print(f\"Experiment: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 06:50:28,293\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.140.201:6379...\n",
      "2026-02-02 06:50:28,305\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-ffbqdd398vb4g8i97u3tsubr23.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-02 06:50:28,308\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_490ef89b3fa7416cf8cfa71609535e02c19cb158.zip' (0.58MiB) to Ray cluster...\n",
      "2026-02-02 06:50:28,311\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_490ef89b3fa7416cf8cfa71609535e02c19cb158.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [FailurePolicy] RETRY\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   Source: controller\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   Error count: 1 (max allowed: inf)\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m \n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/controller/controller.py\", line 338, in _start_worker_group\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     self._worker_group = self.worker_group_cls.create(\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 132, in create\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     worker_group._start()\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 207, in _start\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     raise e\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 200, in _start\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     self._start_impl(\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 291, in _start_impl\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     raise WorkerGroupStartupTimeoutError(\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m ray.train.ControllerError: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [FailurePolicy] RETRY\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   Source: controller\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   Error count: 2 (max allowed: inf)\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m \n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/controller/controller.py\", line 338, in _start_worker_group\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     self._worker_group = self.worker_group_cls.create(\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 132, in create\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     worker_group._start()\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 207, in _start\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     raise e\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 200, in _start\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     self._start_impl(\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 291, in _start_impl\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m     raise WorkerGroupStartupTimeoutError(\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m ray.train.ControllerError: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(RayTrainWorker pid=26663, ip=10.0.128.198)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m Started training worker group of size 2: \n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m - (ip=10.0.128.198, pid=26663) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m - (ip=10.0.170.190, pid=22505) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
      "\u001b[36m(RayTrainWorker pid=26663, ip=10.0.128.198)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_b2f564ce/checkpoint_2026-02-02_06-52-14.180406)\n",
      "\u001b[36m(RayTrainWorker pid=26663, ip=10.0.128.198)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_b2f564ce/checkpoint_2026-02-02_06-52-14.180406), metrics={'loss': 0.7410005530568836, 'epoch': 0}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=26663, ip=10.0.128.198)\u001b[0m Epoch 0: loss=0.7410\n",
      "\u001b[36m(RayTrainWorker pid=22505, ip=10.0.170.190)\u001b[0m Reporting training result 2: TrainingReport(checkpoint=None, metrics={}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=26663, ip=10.0.128.198)\u001b[0m Reporting training result 2: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_b2f564ce/full_model), metrics={}, validation_spec=None)\n",
      "\u001b[36m(TrainController pid=82530)\u001b[0m [State Transition] RUNNING -> SHUTTING_DOWN.\n",
      "\u001b[36m(RayTrainWorker pid=26663, ip=10.0.128.198)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_b2f564ce/full_model)\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22505, ip=10.0.170.190)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_b2f564ce/checkpoint_2026-02-02_06-52-14.180406), metrics={'loss': 0.7372158506531705, 'epoch': 0}, validation_spec=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Checkpoint: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_b2f564ce/full_model)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=82530)\u001b[0m [State Transition] SHUTTING_DOWN -> FINISHED.\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(f\"Training complete! Checkpoint: {result.checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Step 7: Inspect Training Artifacts\n",
    "\n",
    "Training artifacts include:\n",
    "- `checkpoint_*/` - Epoch checkpoints with distributed shards\n",
    "- `full_model/` - Consolidated model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts in /mnt/cluster_storage/fsdp_b2f564ce/:\n",
      "  .validate_storage_marker\n",
      "  checkpoint_2026-02-02_06-52-14.180406/\n",
      "  checkpoint_manager_snapshot.json\n",
      "  full_model/\n"
     ]
    }
   ],
   "source": [
    "# List artifacts\n",
    "storage_path = f\"/mnt/cluster_storage/{experiment_name}/\"\n",
    "print(f\"Artifacts in {storage_path}:\")\n",
    "for item in sorted(os.listdir(storage_path)):\n",
    "    print(f\"  {item}/\" if os.path.isdir(os.path.join(storage_path, item)) else f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 8: Load Model for Inference\n",
    "\n",
    "The consolidated model (`full-model.pt`) is a standard PyTorch checkpoint that works without FSDP2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: /mnt/cluster_storage/fsdp_b2f564ce/full_model/full-model.pt\n"
     ]
    }
   ],
   "source": [
    "# Load model for inference\n",
    "model_path = f\"/mnt/cluster_storage/{experiment_name}/full_model/full-model.pt\"\n",
    "print(f\"Loading from: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "inference_model = init_model()\n",
    "inference_model.load_state_dict(torch.load(model_path, map_location='cpu', weights_only=True))\n",
    "inference_model.eval()\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "test_data = FashionMNIST(root=\"/tmp\", train=False, download=True, transform=Compose([ToTensor(), Normalize((0.5,), (0.5,))]))\n",
    "with torch.no_grad():\n",
    "    sample = test_data.data[0].reshape(1, 1, 28, 28).float()\n",
    "    output = inference_model(sample)\n",
    "print(f\"Inference output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pbut9so1do",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "1. **FSDP2 sharding** - Distributed model parameters across GPUs using `fully_shard()`\n",
    "2. **Ray Train integration** - Multi-GPU training with automatic process group management\n",
    "3. **PyTorch DCP** - Sharded checkpointing with automatic resharding on load\n",
    "4. **Inference** - Loading consolidated model for single-GPU inference\n",
    "\n",
    "**Next Steps:**\n",
    "- Add CPU offloading: `CPUOffloadPolicy()` for memory-constrained scenarios\n",
    "- Add mixed precision: `MixedPrecisionPolicy(param_dtype=torch.float16)`\n",
    "- Try [DeepSpeed tutorial](./DeepSpeed_RayTrain_Tutorial.ipynb) for comparison\n",
    "\n",
    "**Resources:**\n",
    "- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n",
    "- [Ray Train Documentation](https://docs.ray.io/en/latest/train/getting-started-pytorch.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
