{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e495450c",
   "metadata": {},
   "source": [
    "# Get Started with PyTorch FSDP2 and Ray Train (LIVE)\n",
    "\n",
    "Streamlined notebook for ~1 hr hands-on workshops. Same ViT-on-FashionMNIST setup; compare with [DeepSpeed_RayTrain_Tutorial_LIVE.ipynb](./DeepSpeed_RayTrain_Tutorial_LIVE.ipynb) for the alternative backend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab037eed",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates how to train large models using PyTorch's Fully Sharded Data Parallel (FSDP2) with Ray Train. FSDP2 enables model sharding across multiple GPUs, reducing memory footprint compared to standard DDP.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Configure FSDP2 sharding for distributed training\n",
    "2. Use PyTorch Distributed Checkpoint (DCP) for sharded model checkpointing\n",
    "3. Load trained models for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf614847",
   "metadata": {},
   "source": [
    "This notebook will walk you through a high level overview of using FSDP with Ray Train.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Here is the roadmap for this notebook:\n",
    "\n",
    "<ol>\n",
    "  <li>What is FSDP?</li>\n",
    "  <li>FSDP vs DDP simplified</li>\n",
    "  <li>How to use FSDP (v2) with Ray Train?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a90ad",
   "metadata": {},
   "source": [
    "## `Step 0`: What is FSDP2?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc9f9c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[FSDP2](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) is PyTorch's native solution for training large models:\n",
    "\n",
    "- Shards model parameters, gradients, and optimizer states across workers\n",
    "- All-gathers parameters during forward pass, then re-shards\n",
    "- Enables training models larger than single GPU memory\n",
    "\n",
    "**When to use FSDP2:**\n",
    "- Model exceeds single GPU memory\n",
    "- You want native PyTorch integration\n",
    "- Building custom training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fc785",
   "metadata": {},
   "source": [
    "### FSDP Workflow\n",
    "\n",
    "Below is a diagram (taken and adapted from PyTorch) that shows the FSDP workflow\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/FSDP.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b402013",
   "metadata": {},
   "source": [
    "\n",
    "Here is a table explaining the different phases, their steps and what happens:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Phase</th>\n",
    "      <th>Step</th>\n",
    "      <th>What Happens</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Initialization</td>\n",
    "      <td>Parameter sharding</td>\n",
    "      <td>Each rank stores only its own shard of every parameter it owns.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Forward pass</td>\n",
    "      <td>\n",
    "        <ol>\n",
    "          <li>all_gather</li>\n",
    "          <li>Compute</li>\n",
    "          <li>Free full weights</li>\n",
    "        </ol>\n",
    "      </td>\n",
    "      <td>Ranks gather one another’s shards to reconstruct full weights(parameters), execute the forward computation, then immediately free the temporary shards.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Backward pass</td>\n",
    "      <td>\n",
    "        <ol>\n",
    "          <li>all_gather</li>\n",
    "          <li>Back-propagate</li>\n",
    "          <li>reduce_scatter</li>\n",
    "          <li>Free full weights</li>\n",
    "        </ol>\n",
    "      </td>\n",
    "      <td>Shards are re-gathered, gradients are computed, then reduced-and-scattered so each rank keeps only its own gradient shard; full replicas are discarded again.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180b274",
   "metadata": {},
   "source": [
    "### FSDP vs DDP simplified\n",
    "\n",
    "Let's go over a toy example  (inspired by [this guide on parallelism from huggingface](https://huggingface.co/docs/transformers/v4.13.0/en/parallelism)) comparing how FSDP and DDP operate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41726255",
   "metadata": {},
   "source": [
    "### 1  Toy model\n",
    "\n",
    "| **La** | **Lb** | **Lc** |\n",
    "|:------:|:------:|:------:|\n",
    " a₀ | b₀ | c₀ |\n",
    " a₁ | b₁ | c₁ |\n",
    " a₂ | b₂ | c₂ |\n",
    "\n",
    "Layer `La` contains the weights `[a₀, a₁, a₂]`\n",
    "\n",
    "Total parameters = **9 scalars** (3 per layer).\n",
    "\n",
    "---\n",
    "\n",
    "### 2  Parameter layout at rest  \n",
    "\n",
    "#### 2.1  DDP  (full replication)\n",
    "\n",
    "| **GPU** | **La**           | **Lb**           | **Lc**           |\n",
    "|---------|------------------|------------------|------------------|\n",
    "| 0       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "| 1       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "| 2       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "\n",
    "*Each worker stores **100 %** of the model (parameters + optimizer states + gradients).*\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2  FSDP  (parameter sharding)\n",
    "\n",
    "| **GPU** | **La** | **Lb** | **Lc** |\n",
    "|---------|:------:|:------:|:------:|\n",
    "| 0       | a₀     | b₀     | c₀     |\n",
    "| 1       | a₁     | b₁     | c₁     |\n",
    "| 2       | a₂     | b₂     | c₂     |\n",
    "\n",
    "*At rest each worker keeps only **1∕3** of every layer—so memory ≈ 33 % of DDP.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3  Execution flow (GPU 0’s perspective)\n",
    "\n",
    "#### 3.1  FSDP\n",
    "\n",
    "<details>\n",
    "<summary><strong>Forward pass (Layer La)</strong></summary>\n",
    "\n",
    "1. **Need:** a₀ a₁ a₂  \n",
    "2. **Has locally:** a₀  \n",
    "3. **Gather:** a₁ from GPU 1, a₂ from GPU 2  \n",
    "4. **Compute:** *y = La(x)* with full weights  \n",
    "5. **Free:** a₁, a₂ (optional halfway-release)\n",
    "\n",
    "Repeat for **Lb** then **Lc**.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Backward pass (Layer Lc)</strong></summary>\n",
    "\n",
    "1. **Need:** c₀ c₁ c₂  \n",
    "2. **Has locally:** c₀  \n",
    "3. **Gather:** c₁ from GPU 1, c₂ from GPU 2  \n",
    "4. **Compute:** ∂L/∂c₀, ∂L/∂c₁, ∂L/∂c₂  \n",
    "5. **Reduce-scatter:** average gradients; GPU k keeps only its own shard (cₖ)  \n",
    "6. **Free:** c₁, c₂ (optional halfway-release)\n",
    "\n",
    "Work upstream through **Lb → La** in reverse order.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Optimizer step</strong></summary>\n",
    "\n",
    "*Purely local.*  \n",
    "GPU 0 updates a₀, b₀, c₀ using the averaged gradient shards already resident in memory. No extra communication.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2  DDP  (for comparison)\n",
    "\n",
    "| Phase             | What GPU 0 already owns         | Communication |\n",
    "|-------------------|---------------------------------|---------------|\n",
    "| **Forward**       | Full La, Lb, Lc                 | None          |\n",
    "| **Backward**      | Full La, Lb, Lc + grads         | **All-reduce**|\n",
    "| **Optimizer step**| Full params & full grads        | None          |\n",
    "\n",
    "---\n",
    "\n",
    "### 4  Key take-aways\n",
    "\n",
    "|                         | **DDP**                           | **FSDP**                               |\n",
    "|-------------------------|-----------------------------------|----------------------------------------|\n",
    "| **Memory footprint**    | Replicated (× #GPUs)              | 1∕#GPUs at rest; slightly higher during gather |\n",
    "| **Communication**       | All-reduce once per layer (backward) | Gather + reduce-scatter per layer      |\n",
    "| **Optimizer states**    | Fully replicated                  | Sharded—1∕#GPUs memory|\n",
    "| **Implementation ease** | Very simple                       | More knobs (wrapping policy, offloading,prefetch) |\n",
    "| **When to prefer**      | Fits in memory; small models      | Large models that would otherwise OOM  |\n",
    "\n",
    "> **Rule of thumb:**  \n",
    "> *If you can replicate the model comfortably, DDP wins on simplicity and sometimes speed.  \n",
    "> If you’re out of memory or pushing model scale, FSDP (or ZeRO-style sharding) is the way forward.*\n",
    "\n",
    "---\n",
    "\n",
    "#### 5  Why we wrapped each layer separately\n",
    "\n",
    "For illustration we treated **“one layer = one FSDP unit.”**  \n",
    "In practice you can:\n",
    "\n",
    "* **Cluster layers** into larger FSDP units to reduce the number of gather calls.\n",
    "* **Wrap only the largest sub-modules** (e.g., big embeddings, attention blocks) and leave tiny layers unsharded.\n",
    "\n",
    "Experiment with *auto-wrap policies* to find the sweet spot between memory savings and communication overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a4689",
   "metadata": {},
   "source": [
    "### When to Consider FSDP\n",
    "\n",
    "- **Model no longer fits** on a single GPU even with mixed precision.  \n",
    "- **Batch size is GPU memory-bound** under classic DDP.  \n",
    "- **You have multiple GPUs** with sufficient interconnect bandwidth.  \n",
    "- **You already use DDP** but need to push to larger architectures (e.g., multi-billion-parameter transformers).  \n",
    "- **You want minimal code changes**—wrap layers with `torch.distributed.fsdp.FullyShardedDataParallel`.  \n",
    "\n",
    "FSDP lets you step beyond the memory limits of traditional data parallelism while keeping your training loop largely unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d868ce",
   "metadata": {},
   "source": [
    "### What is FSDP?\n",
    "\n",
    "Fully Sharded Data Parallel (FSDP) is a parallelism method that combines the advantages of data and model parallelism for distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ee6cd",
   "metadata": {},
   "source": [
    "### FSDP Workflow\n",
    "\n",
    "Below is a diagram (taken and adapted from PyTorch) that shows the FSDP workflow\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/FSDP.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16da40",
   "metadata": {},
   "source": [
    "\n",
    "Here is a table explaining the different phases, their steps and what happens:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Phase</th>\n",
    "      <th>Step</th>\n",
    "      <th>What Happens</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Initialization</td>\n",
    "      <td>Parameter sharding</td>\n",
    "      <td>Each rank stores only its own shard of every parameter it owns.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Forward pass</td>\n",
    "      <td>\n",
    "        <ol>\n",
    "          <li>all_gather</li>\n",
    "          <li>Compute</li>\n",
    "          <li>Free full weights</li>\n",
    "        </ol>\n",
    "      </td>\n",
    "      <td>Ranks gather one another’s shards to reconstruct full weights(parameters), execute the forward computation, then immediately free the temporary shards.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Backward pass</td>\n",
    "      <td>\n",
    "        <ol>\n",
    "          <li>all_gather</li>\n",
    "          <li>Back-propagate</li>\n",
    "          <li>reduce_scatter</li>\n",
    "          <li>Free full weights</li>\n",
    "        </ol>\n",
    "      </td>\n",
    "      <td>Shards are re-gathered, gradients are computed, then reduced-and-scattered so each rank keeps only its own gradient shard; full replicas are discarded again.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa7eea",
   "metadata": {},
   "source": [
    "### FSDP vs DDP simplified\n",
    "\n",
    "Let's go over a toy example  (inspired by [this guide on parallelism from huggingface](https://huggingface.co/docs/transformers/v4.13.0/en/parallelism)) comparing how FSDP and DDP operate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c20dce",
   "metadata": {},
   "source": [
    "### 1  Toy model\n",
    "\n",
    "| **La** | **Lb** | **Lc** |\n",
    "|:------:|:------:|:------:|\n",
    " a₀ | b₀ | c₀ |\n",
    " a₁ | b₁ | c₁ |\n",
    " a₂ | b₂ | c₂ |\n",
    "\n",
    "Layer `La` contains the weights `[a₀, a₁, a₂]`\n",
    "\n",
    "Total parameters = **9 scalars** (3 per layer).\n",
    "\n",
    "---\n",
    "\n",
    "### 2  Parameter layout at rest  \n",
    "\n",
    "#### 2.1  DDP  (full replication)\n",
    "\n",
    "| **GPU** | **La**           | **Lb**           | **Lc**           |\n",
    "|---------|------------------|------------------|------------------|\n",
    "| 0       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "| 1       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "| 2       | a₀ a₁ a₂         | b₀ b₁ b₂         | c₀ c₁ c₂         |\n",
    "\n",
    "*Each worker stores **100 %** of the model (parameters + optimizer states + gradients).*\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2  FSDP  (parameter sharding)\n",
    "\n",
    "| **GPU** | **La** | **Lb** | **Lc** |\n",
    "|---------|:------:|:------:|:------:|\n",
    "| 0       | a₀     | b₀     | c₀     |\n",
    "| 1       | a₁     | b₁     | c₁     |\n",
    "| 2       | a₂     | b₂     | c₂     |\n",
    "\n",
    "*At rest each worker keeps only **1∕3** of every layer—so memory ≈ 33 % of DDP.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3  Execution flow (GPU 0’s perspective)\n",
    "\n",
    "#### 3.1  FSDP\n",
    "\n",
    "<details>\n",
    "<summary><strong>Forward pass (Layer La)</strong></summary>\n",
    "\n",
    "1. **Need:** a₀ a₁ a₂  \n",
    "2. **Has locally:** a₀  \n",
    "3. **Gather:** a₁ from GPU 1, a₂ from GPU 2  \n",
    "4. **Compute:** *y = La(x)* with full weights  \n",
    "5. **Free:** a₁, a₂ (optional halfway-release)\n",
    "\n",
    "Repeat for **Lb** then **Lc**.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Backward pass (Layer Lc)</strong></summary>\n",
    "\n",
    "1. **Need:** c₀ c₁ c₂  \n",
    "2. **Has locally:** c₀  \n",
    "3. **Gather:** c₁ from GPU 1, c₂ from GPU 2  \n",
    "4. **Compute:** ∂L/∂c₀, ∂L/∂c₁, ∂L/∂c₂  \n",
    "5. **Reduce-scatter:** average gradients; GPU k keeps only its own shard (cₖ)  \n",
    "6. **Free:** c₁, c₂ (optional halfway-release)\n",
    "\n",
    "Work upstream through **Lb → La** in reverse order.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Optimizer step</strong></summary>\n",
    "\n",
    "*Purely local.*  \n",
    "GPU 0 updates a₀, b₀, c₀ using the averaged gradient shards already resident in memory. No extra communication.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2  DDP  (for comparison)\n",
    "\n",
    "| Phase             | What GPU 0 already owns         | Communication |\n",
    "|-------------------|---------------------------------|---------------|\n",
    "| **Forward**       | Full La, Lb, Lc                 | None          |\n",
    "| **Backward**      | Full La, Lb, Lc + grads         | **All-reduce**|\n",
    "| **Optimizer step**| Full params & full grads        | None          |\n",
    "\n",
    "---\n",
    "\n",
    "### 4  Key take-aways\n",
    "\n",
    "|                         | **DDP**                           | **FSDP**                               |\n",
    "|-------------------------|-----------------------------------|----------------------------------------|\n",
    "| **Memory footprint**    | Replicated (× #GPUs)              | 1∕#GPUs at rest; slightly higher during gather |\n",
    "| **Communication**       | All-reduce once per layer (backward) | Gather + reduce-scatter per layer      |\n",
    "| **Optimizer states**    | Fully replicated                  | Sharded—1∕#GPUs memory|\n",
    "| **Implementation ease** | Very simple                       | More knobs (wrapping policy, offloading,prefetch) |\n",
    "| **When to prefer**      | Fits in memory; small models      | Large models that would otherwise OOM  |\n",
    "\n",
    "> **Rule of thumb:**  \n",
    "> *If you can replicate the model comfortably, DDP wins on simplicity and sometimes speed.  \n",
    "> If you’re out of memory or pushing model scale, FSDP (or ZeRO-style sharding) is the way forward.*\n",
    "\n",
    "---\n",
    "\n",
    "#### 5  Why we wrapped each layer separately\n",
    "\n",
    "For illustration we treated **“one layer = one FSDP unit.”**  \n",
    "In practice you can:\n",
    "\n",
    "* **Cluster layers** into larger FSDP units to reduce the number of gather calls.\n",
    "* **Wrap only the largest sub-modules** (e.g., big embeddings, attention blocks) and leave tiny layers unsharded.\n",
    "\n",
    "Experiment with *auto-wrap policies* to find the sweet spot between memory savings and communication overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83f43a",
   "metadata": {},
   "source": [
    "### When to Consider FSDP\n",
    "\n",
    "- **Model no longer fits** on a single GPU even with mixed precision.  \n",
    "- **Batch size is GPU memory-bound** under classic DDP.  \n",
    "- **You have multiple GPUs** with sufficient interconnect bandwidth.  \n",
    "- **You already use DDP** but need to push to larger architectures (e.g., multi-billion-parameter transformers).  \n",
    "- **You want minimal code changes**—wrap layers with `torch.distributed.fsdp.FullyShardedDataParallel`.  \n",
    "\n",
    "FSDP lets you step beyond the memory limits of traditional data parallelism while keeping your training loop largely unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a4b3b",
   "metadata": {},
   "source": [
    "## `Step 1`: Environment Setup\n",
    "\n",
    "Check Ray cluster status and install dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb93eb1",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"./images/img_1.png\" alt=\"FSDP Diagram\" style=\"width: 80%;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea41fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Autoscaler status: 2026-02-18 23:07:05.898387 ========\n",
      "Node status\n",
      "---------------------------------------------------------------\n",
      "Active:\n",
      " (no active nodes)\n",
      "Idle:\n",
      " 1 head\n",
      " 1 1xL4:16CPU-64GB-1\n",
      " 1 1xL4:16CPU-64GB-2\n",
      "Pending:\n",
      " (no pending nodes)\n",
      "Recent failures:\n",
      " (no failures)\n",
      "\n",
      "Resources\n",
      "---------------------------------------------------------------\n",
      "Total Usage:\n",
      " 0.0/32.0 CPU\n",
      " 0.0/2.0 GPU\n",
      " 0.0/2.0 anyscale/accelerator_shape:1xL4\n",
      " 0.0/1.0 anyscale/cpu_only:true\n",
      " 0.0/1.0 anyscale/node-group:1xL4:16CPU-64GB-1\n",
      " 0.0/1.0 anyscale/node-group:1xL4:16CPU-64GB-2\n",
      " 0.0/1.0 anyscale/node-group:head\n",
      " 0.0/3.0 anyscale/provider:aws\n",
      " 0.0/3.0 anyscale/region:us-west-2\n",
      " 0B/160.00GiB memory\n",
      " 0B/44.60GiB object_store_memory\n",
      "\n",
      "From request_resources:\n",
      " (none)\n",
      "Pending Demands:\n",
      " (no resource demands)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Check Ray cluster status\n",
    "!ray status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fccaeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stdlib imports\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Ray Train imports\n",
    "import ray\n",
    "import ray.train\n",
    "import ray.train.torch\n",
    "\n",
    "# PyTorch core and FSDP2 imports\n",
    "import torch\n",
    "from torch.distributed.fsdp import (\n",
    "    fully_shard,\n",
    "    FSDPModule,\n",
    "    CPUOffloadPolicy,\n",
    "    MixedPrecisionPolicy,\n",
    ")\n",
    "\n",
    "# PyTorch Distributed Checkpoint (DCP) imports\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    get_state_dict,\n",
    "    set_state_dict,\n",
    "    get_model_state_dict,\n",
    "    StateDictOptions\n",
    ")\n",
    "from torch.distributed.device_mesh import init_device_mesh \n",
    "from torch.distributed.checkpoint.stateful import Stateful\n",
    "import torch.distributed.checkpoint as dcp\n",
    "\n",
    "# PyTorch training components\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Computer vision components\n",
    "from torchvision.models import VisionTransformer\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077a1b4",
   "metadata": {},
   "source": [
    "## `Step 2`: Model Definition\n",
    "\n",
    "We use a Vision Transformer (ViT) with repeatable encoder blocks - ideal for demonstrating FSDP2's per-layer sharding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459e9e2",
   "metadata": {},
   "source": [
    "Let's go over a simple example of how to use FSDP with Ray Train and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996dbf8",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"./images/img_2.png\" alt=\"Train Diagram\" style=\"width: 80%;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a6fc7",
   "metadata": {},
   "source": [
    "Below is a sample training function that we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd20bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    \"\"\"\n",
    "    Training function executed by EACH worker (GPU).\n",
    "    Ray Train spawns N copies of this function — one per GPU.\n",
    "    All workers run the same code but process different data batches.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # Step 1 : Initialize the model (on CPU, not sharded yet)\n",
    "    # ==========================================================\n",
    "    model = init_model(config[\"hidden_dim\"])\n",
    "\n",
    "    # ==========================================================\n",
    "    # Step 2 : Move the model to this worker's assigned GPU\n",
    "    # ==========================================================\n",
    "    device = ray.train.torch.get_device()\n",
    "    torch.cuda.set_device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # ==========================================================\n",
    "    # Step 3 : Apply FSDP2 sharding (now each worker owns only a slice of parameters)\n",
    "    #          Must come AFTER model is on GPU!\n",
    "    # ==========================================================\n",
    "    prepare_model(\n",
    "        model,\n",
    "        skip_model_shard=config[\"skip_model_shard\"],\n",
    "        skip_cpu_offload=config[\"skip_cpu_offload\"],\n",
    "        use_float16=config[\"use_float16\"],\n",
    "    )\n",
    "\n",
    "    # ==========================================================\n",
    "    # Step 4 : Define loss function and optimizer (sharded states under FSDP)\n",
    "    # ==========================================================\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.get(\"learning_rate\", 0.001))\n",
    "\n",
    "    # ==========================================================\n",
    "    # Step 5 : Resume from checkpoint if available (sharded, supports flexible recovery)\n",
    "    # ==========================================================\n",
    "    start_epoch = 0\n",
    "    loaded_checkpoint = ray.train.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        start_epoch = load_fsdp_checkpoint(model, optimizer, loaded_checkpoint)\n",
    "\n",
    "    # ==========================================================\n",
    "    # Step 6 : Prepare the distributed FashionMNIST data loader\n",
    "    #          Each worker gets a different shard automatically\n",
    "    # ==========================================================\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = FashionMNIST(\n",
    "        root=data_dir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=config.get(\"batch_size\", 128),\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "\n",
    "    world_rank = ray.train.get_context().get_world_rank()\n",
    "\n",
    "    # ==========================================================\n",
    "    # Step 7 : Main training loop (standard PyTorch, FSDP handles sharding transparently)\n",
    "    # ==========================================================\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # ==========================================================\n",
    "        # Ensure proper random shuffling by epoch for DistributedSampler\n",
    "        # ==========================================================\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "\n",
    "            # ==========================================================\n",
    "            # Forward pass: model(images) triggers all-gather and computation\n",
    "            # ==========================================================\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # ==========================================================\n",
    "            # Backward pass: loss.backward() handles sharded gradients\n",
    "            # Local optimizer step updates only this shard\n",
    "            # ==========================================================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # ==========================================================\n",
    "        # Step 8 : Save and report metrics and checkpoint after every epoch\n",
    "        #          Checkpointing is sharded—fast and memory-efficient\n",
    "        # ==========================================================\n",
    "        avg_loss = running_loss / num_batches\n",
    "        metrics = {\"loss\": avg_loss, \"epoch\": epoch + 1}\n",
    "        report_metrics_and_save_fsdp_checkpoint(model, optimizer, metrics, epoch + 1)\n",
    "\n",
    "        # ==========================================================\n",
    "        # Only rank 0 logs metrics to prevent duplicate outputs\n",
    "        # ==========================================================\n",
    "        if world_rank == 0:\n",
    "            print(metrics)\n",
    "\n",
    "    # ==========================================================\n",
    "    # Step 9 : Save the final model for inference (\"all_gather\" full state onto rank 0)\n",
    "    # ==========================================================\n",
    "    save_model_for_inference(model, world_rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3df2a2",
   "metadata": {},
   "source": [
    "### Initialize the model\n",
    "Initialize a Vision Transformer model for FashionMNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e494626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(hidden_dim) -> torch.nn.Module:\n",
    "    # Create a ViT model with architecture suitable for 28x28 images\n",
    "    model = VisionTransformer(\n",
    "        image_size=28,         # FashionMNIST image size\n",
    "        patch_size=7,          # Divide 28x28 into 4x4 patches of 7x7 pixels each\n",
    "        num_layers=12,         # Number of transformer encoder layers\n",
    "        num_heads=8,           # Number of attention heads per layer\n",
    "        hidden_dim=hidden_dim, # Hidden dimension size\n",
    "        mlp_dim=768,           # MLP dimension in transformer blocks\n",
    "        num_classes=10,        # FashionMNIST has 10 classes\n",
    "    )\n",
    "\n",
    "    # Modify the patch embedding layer for grayscale images (1 channel instead of 3)\n",
    "    model.conv_proj = torch.nn.Conv2d(\n",
    "        in_channels=1,            # FashionMNIST is grayscale (1 channel)\n",
    "        out_channels=hidden_dim,  # Must match the hidden_dim\n",
    "        kernel_size=7,            # Match patch_size\n",
    "        stride=7,                 # Non-overlapping patches\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd466bb1",
   "metadata": {},
   "source": [
    "## `Step 3`: Prepare the model and FSDP2 Sharding Configuration\n",
    "\n",
    "To prepare the model, we use the `fully_shard` (FSDP2) function from pytorch but we have to first:\n",
    "1. Create a device mesh\n",
    "2. Apply `fully_shard` to each block we want to shard\n",
    "3. Apply `fully_shard` to the model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c16411",
   "metadata": {},
   "source": [
    "> ```\n",
    "> ViT Model\n",
    ">   ├── Patch Embedding      ← wrapped by root fully_shard(model)\n",
    ">   ├── Encoder Block 1      ← fully_shard(block)\n",
    ">   ├── Encoder Block 2      ← fully_shard(block)\n",
    ">   ├── ...                  \n",
    ">   ├── Encoder Block 12     ← fully_shard(block)\n",
    ">   └── Classification Head  ← wrapped by root fully_shard(model)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138fa9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(\n",
    "    model: torch.nn.Module,\n",
    "    skip_model_shard: bool,\n",
    "    skip_cpu_offload: bool,\n",
    "    use_float16: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply FSDP2 sharding to the model.\n",
    "\n",
    "    This function performs the following:\n",
    "      1. Creates a device mesh (sets up GPU topology)\n",
    "      2. Shards each encoder block individually\n",
    "      3. Shards the root model (top-level)\n",
    "    \n",
    "    After running, each GPU keeps only a fraction (1/N) of the total parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # =================================================\n",
    "    # Step 1: Device Mesh Creation\n",
    "    # =================================================\n",
    "    #\n",
    "    # The device mesh defines how GPUs are organized for distributed training.\n",
    "    # Here, we set up a 1D mesh for pure data parallelism: all GPUs lined up along one dimension.\n",
    "    #\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    mesh = init_device_mesh(\n",
    "        device_type=\"cuda\",                # Use CUDA-enabled GPUs\n",
    "        mesh_shape=(world_size,),          # 1D mesh: a flat group of GPUs\n",
    "        mesh_dim_names=(\"data_parallel\",)  # Naming the dimension for clarity\n",
    "    )\n",
    "\n",
    "    # =================================================\n",
    "    # Step 2: CPU Offloading Configuration (Optional)\n",
    "    # =================================================\n",
    "    #\n",
    "    # CPU offloading will transfer parameter shards that aren't actively in use\n",
    "    # from GPU VRAM to CPU RAM, saving GPU memory at the expense of a bit more data transfer time.\n",
    "    # Useful if GPU memory is a tight resource.\n",
    "    #\n",
    "    offload_policy = CPUOffloadPolicy() if not skip_cpu_offload else None\n",
    "\n",
    "    # =================================================\n",
    "    # Step 3: Mixed Precision Configuration (Optional)\n",
    "    # =================================================\n",
    "    #\n",
    "    # Mixed precision stores model parameters in float16 (half precision) rather than float32,\n",
    "    # reducing memory requirements and using specialized GPU tensor cores for better performance.\n",
    "    # float16 works well on most GPUs. For production on A100/H100, use bfloat16.\n",
    "    #\n",
    "    mp_policy_float16 = MixedPrecisionPolicy(\n",
    "        param_dtype=torch.float16,    # Store model parameters as float16\n",
    "        reduce_dtype=torch.float16,   # Reduce the gradients in float16 as well\n",
    "    )\n",
    "    default_policy = MixedPrecisionPolicy(\n",
    "        param_dtype=None,             # Keep original precision (float32)\n",
    "        reduce_dtype=None,\n",
    "        output_dtype=None,\n",
    "        cast_forward_inputs=True      # Inputs will match parameter dtype\n",
    "    )\n",
    "    mp_policy = mp_policy_float16 if use_float16 else default_policy\n",
    "\n",
    "    # =================================================\n",
    "    # Step 4: Sharding Each Encoder Block Separately\n",
    "    # =================================================\n",
    "    #\n",
    "    # Each transformer encoder block becomes its own FSDP unit.\n",
    "    # This means that during the forward pass, only a single block's parameters are gathered onto GPU at a time,\n",
    "    # dramatically reducing peak memory usage.\n",
    "    #\n",
    "    # reshard_after_forward:\n",
    "    #   - True  → Free parameters after forward (memory savings, more comms)\n",
    "    #   - False → Keep parameters in memory for backward (faster, but uses more memory)\n",
    "    # skip_model_shard=True is equivalent to reshard_after_forward=False\n",
    "    #\n",
    "    for encoder_block in model.encoder.layers.children():\n",
    "        fully_shard(\n",
    "            encoder_block,\n",
    "            mesh=mesh,\n",
    "            reshard_after_forward=not skip_model_shard,\n",
    "            offload_policy=offload_policy,\n",
    "            mp_policy=mp_policy,\n",
    "        )\n",
    "\n",
    "    # =================================================\n",
    "    # Step 5: Sharding the Root Model\n",
    "    # =================================================\n",
    "    #\n",
    "    # This wraps the remaining parts of the model—such as patch embedding,\n",
    "    # positional encoding, and the classification head—with FSDP as well.\n",
    "    # Top-level sharding is essential for FSDP2's coordination and memory efficiency.\n",
    "    #\n",
    "    fully_shard(\n",
    "        model,\n",
    "        mesh=mesh,\n",
    "        reshard_after_forward=not skip_model_shard,\n",
    "        offload_policy=offload_policy,\n",
    "        mp_policy=mp_policy,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce5091",
   "metadata": {},
   "source": [
    "Here is a table of the main keyword arguments for FSDP:\n",
    "\n",
    "| Parameter | What it controls | Typical values & when to use |\n",
    "|-----------|-----------------|------------------------------|\n",
    "| `reshard_after_forward` | Whether to free (reshard) parameters immediately after forward pass to save memory | **`True`** – free parameters after forward pass for maximum memory savings; increases communication overhead.<br>**`False`** (default) – keep parameters in memory through backward pass; faster but uses more memory. |\n",
    "| `offload_policy` | Whether inactive parameter shards are moved to CPU RAM | **`None`** (default) – fastest, keeps all data on GPU.<br>**`CPUOffloadPolicy()`** – offloads parameters to CPU RAM; frees GPU memory at the cost of extra PCIe traffic; enables larger models on small-RAM GPUs. |\n",
    "| `mp_policy` | Controls mixed precision settings for parameters and gradients | **`None`** (default) – use model's native precision (typically float32).<br>**`MixedPrecisionPolicy(param_dtype=torch.float16)`** – store parameters in half precision for memory savings and to leverage tensor cores.<br>**`MixedPrecisionPolicy(param_dtype=torch.bfloat16)`** – use bfloat16 for newer architectures (A100, H100) with better numerical stability and tensor core acceleration.<br>**`MixedPrecisionPolicy(param_dtype=torch.bfloat16, reduce_dtype=torch.float32)`** – commonly used configuration; bfloat16 parameters with float32 gradient reduction for better numerical stability during reduce-scatter operations.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df808d",
   "metadata": {},
   "source": [
    "## `Step 4`: Distributed Checkpointing with PyTorch\n",
    "\n",
    "`torch.distributed.checkpoint()` enables saving and loading models from multiple ranks in parallel. You can use this module to save on any number of ranks in parallel, and then re-shard across differing cluster topologies at load time.\n",
    "\n",
    "PyTorch Distributed Checkpoint (DCP) provides efficient checkpointing for sharded models:\n",
    "- Each worker saves only its shard (parallel I/O)\n",
    "- Automatic resharding on load if worker count changes\n",
    "- Full optimizer state support for training resumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ac9d0",
   "metadata": {},
   "source": [
    "### Defining a Stateful object\n",
    "\n",
    "We take advantage of a Stateful object to handle calling distributed state dict methods on the model and optimizer.\n",
    "\n",
    "This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n",
    "with the Stateful protocol, PyTorch DCP will automatically call state_dict/load_stat_dict as needed in the\n",
    "dcp.save/load APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86135932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AppState(Stateful):\n",
    "    \"\"\"\n",
    "    AppState is a checkpointable wrapper for model, optimizer, and epoch state.\n",
    "\n",
    "    This class implements the `Stateful` protocol and provides:\n",
    "      - `state_dict()`: Gathers the full checkpoint state using FSDP2-aware utilities.\n",
    "      - `load_state_dict()`: Restores both model and optimizer state, handling possible resharding and distributed setups.\n",
    "\n",
    "    Purpose:\n",
    "      - Makes distributed checkpointing simple and robust.\n",
    "      - Handles sharded model state via FSDP2 Fully Qualified Names (FQNs).\n",
    "      - epoch is also checkpointed for resume support.\n",
    "\n",
    "    FSDP2 NOTE:\n",
    "        - Use `get_state_dict` and `set_state_dict` from FSDP2 utilities.\n",
    "        - Never call model.state_dict() directly with FSDP2!\n",
    "        - State dict keys include fully sharded paths such as 'encoder.layers.0.self_attention.weight'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer=None, epoch=0):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.epoch = epoch  # Track training progress for resume\n",
    "\n",
    "    def state_dict(self):\n",
    "        # ============================================================\n",
    "        # Step 1: Gather and return complete checkpoint state\n",
    "        # ============================================================\n",
    "        #\n",
    "        # FSDP2: Use `get_state_dict()` instead of model.state_dict():\n",
    "        #    - Collects sharded state dict with global param names (FQNs)\n",
    "        #    - Returns both model and optimizer state\n",
    "        #\n",
    "        # CAUTION: Only use `get_state_dict()` with FSDP2!\n",
    "        #\n",
    "        model_state_dict, optimizer_state_dict = get_state_dict(\n",
    "            self.model, self.optimizer\n",
    "        )\n",
    "        return {\n",
    "            \"model\": model_state_dict,\n",
    "            \"optim\": optimizer_state_dict,\n",
    "            \"epoch\": self.epoch,  # Allows checkpointing epoch for resume\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        # ============================================================\n",
    "        # Step 2: Restore state from checkpoint dictionary\n",
    "        # ============================================================\n",
    "        #\n",
    "        # FSDP2: Use `set_state_dict()` for distributed, sharded recovery:\n",
    "        #    - Handles model, optimizer shards, and resharding if world_size changes\n",
    "        #    - Automatically maps FQNs\n",
    "        #\n",
    "        set_state_dict(\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "            model_state_dict=state_dict[\"model\"],\n",
    "            optim_state_dict=state_dict[\"optim\"],\n",
    "        )\n",
    "        self.epoch = state_dict[\"epoch\"]  # Ensure resume from checkpoint is correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b2aea",
   "metadata": {},
   "source": [
    "### Saving Distributed Checkpoints when training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361eb25",
   "metadata": {},
   "source": [
    "This function performs two critical operations:\n",
    "1. Saves the current model and optimizer state using distributed checkpointing\n",
    "2. Reports metrics to Ray Train for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics_and_save_fsdp_checkpoint(\n",
    "    model: FSDPModule,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    metrics: dict,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save distributed FSDP model & optimizer state, and report metrics to Ray Train.\n",
    "\n",
    "    This function:\n",
    "      - Saves a *sharded* model & optimizer checkpoint using DCP (across all workers)\n",
    "      - Reports metrics and checkpoint to Ray Train for experiment tracking\n",
    "    \"\"\"\n",
    "    # ========================================================================\n",
    "    # Step 1: Create a temporary directory for the distributed checkpoint\n",
    "    # ========================================================================\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 2: Compose app state for FSDP checkpointing\n",
    "        # (Combines model, optimizer, and epoch into a single checkpointed object)\n",
    "        # =====================================================================\n",
    "        state_dict = {\"app\": AppState(model, optimizer, epoch)}\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 3: Save the distributed sharded checkpoint using DCP\n",
    "        # (Ensures model & optimizer shards are persisted on all workers)\n",
    "        # =====================================================================\n",
    "        dcp.save(\n",
    "            state_dict=state_dict,\n",
    "            checkpoint_id=temp_checkpoint_dir,\n",
    "        )\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 4: Inform Ray Train about this checkpoint and current metrics\n",
    "        #         - checkpoint: gathers all shards into a Ray-readable checkpoint\n",
    "        #         - metrics: dictionary with current validation/train metrics\n",
    "        # =====================================================================\n",
    "        checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "        ray.train.report(\n",
    "            metrics,              # experiment metrics to track\n",
    "            checkpoint=checkpoint # distributed checkpoint to save\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d33c2",
   "metadata": {},
   "source": [
    "### Saving a final and full checkpoint for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9179e8",
   "metadata": {},
   "source": [
    "For inference, we want to save an unsharded copy of the model. This is an expensive operation given all parameters need to gather all model parameters from all ranks onto a single rank.\n",
    "\n",
    "This function consolidates the distributed model weights into a single checkpoint file that can be used for inference without FSDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_inference(\n",
    "    model: FSDPModule,\n",
    "    world_rank: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Consolidate distributed FSDP model weights into a single checkpoint\n",
    "    file for inference, gathering all parameters onto rank 0.\n",
    "\n",
    "    Args:\n",
    "        model (FSDPModule): The FSDP-wrapped model.\n",
    "        world_rank (int): Current process rank.\n",
    "    \"\"\"\n",
    "    # ========================================================================\n",
    "    # Step 1: Create a temporary directory to store the unsharded full model\n",
    "    # ========================================================================\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        save_file = os.path.join(temp_checkpoint_dir, \"full-model.pt\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 2: Gather full model weights (from all shards) onto rank 0\n",
    "        #         - full_state_dict=True gathers all weights to rank 0\n",
    "        #         - cpu_offload=True moves tensors to CPU to reduce GPU memory usage\n",
    "        # =====================================================================\n",
    "        model_state_dict = get_model_state_dict(\n",
    "            model=model,\n",
    "            options=StateDictOptions(\n",
    "                full_state_dict==True,   # Gather all model weights\n",
    "                cpu_offload==True        # Offload to CPU for serialization\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        checkpoint = None\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 3: On rank 0, save the full model, and register a Ray checkpoint\n",
    "        # =====================================================================\n",
    "        if world_rank == 0:\n",
    "            torch.save(model_state_dict, save_file)\n",
    "            # Prepare checkpoint directory for Ray Train\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 4: Report the inference checkpoint to Ray Train on all ranks\n",
    "        #         - Only rank 0 will have a non-None checkpoint object\n",
    "        # =====================================================================\n",
    "        ray.train.report(\n",
    "            metrics={},\n",
    "            checkpoint=checkpoint,\n",
    "            checkpoint_dir_name=\"full_model\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020b48f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**NOTE:** In PyTorch, if both `cpu_offload` and `full_state_dict` are set to True, then only the rank0 will get the state_dict and all other ranks will get empty state_dict.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6c726",
   "metadata": {},
   "source": [
    "### Loading distributed checkpoints\n",
    "\n",
    "This function handles distributed checkpoint loading with automatic resharding support. It can restore checkpoints even when the number of workers differs from the original training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# load_fsdp_checkpoint: Load a Distributed Checkpoint w/ Resharding\n",
    "# ============================================================\n",
    "# Loads a distributed checkpoint using torch DCP, restoring model and optimizer state.\n",
    "# Also handles resharding if loading occurs with a different world size.\n",
    "def load_fsdp_checkpoint(\n",
    "    model: FSDPModule,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    ckpt: ray.train.Checkpoint,\n",
    ") -> int:\n",
    "    try:\n",
    "        with ckpt.as_directory() as checkpoint_dir:\n",
    "            # Wrap the model and optimizer for DCP loading\n",
    "            app_state = AppState(model, optimizer)\n",
    "            state_dict = {\"app\": app_state}\n",
    "\n",
    "            # Load the distributed checkpoint (reshard if necessary)\n",
    "            dcp.load(\n",
    "                state_dict=state_dict,\n",
    "                checkpoint_id=checkpoint_dir,\n",
    "            )\n",
    "\n",
    "            # Return epoch from updated app_state\n",
    "            return app_state.epoch\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Checkpoint loading failed: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089b2e3",
   "metadata": {},
   "source": [
    "## `Step 5`: Final Training Configuration \n",
    "\n",
    "The training function runs on each worker:\n",
    "1. Initialize and shard model with FSDP2\n",
    "2. Run training loop with distributed data loading\n",
    "3. Save checkpoints using PyTorch DCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74850f6",
   "metadata": {},
   "source": [
    "Configure scaling and resource requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f07f8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ScalingConfig: HOW to run (infrastructure)\n",
    "# ============================================================\n",
    "# This tells Ray Train:\n",
    "#   - Spawn 2 workers (one per GPU)\n",
    "#   - Each worker needs one GPU\n",
    "# Changing num_workers changes how many GPUs participate in FSDP.\n",
    "# More workers = each GPU holds less of the model = lower memory per GPU.\n",
    "\n",
    "scaling_config = ray.train.ScalingConfig(\n",
    "    num_workers=2,           # Number of GPU workers (= number of FSDP shards)\n",
    "    use_gpu=True,            # Each worker gets one GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b7903",
   "metadata": {},
   "source": [
    "Launch the distributed training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6767622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# train_loop_config: WHAT to run (hyperparameters)\n",
    "# ============================================================\n",
    "# Walk through each parameter:\n",
    "#   - epochs=1: Just 1 epoch for demo speed. Try 3+ for better accuracy.\n",
    "#   - hidden_dim=3840: Intentionally large to need FSDP. The model will\n",
    "#     be ~180M parameters, which is tight for a single L4 (24GB).\n",
    "#\n",
    "# FSDP KNOBS (the interesting part!):\n",
    "#   - skip_model_shard=True  → reshard_after_forward=False (keep params)\n",
    "#   - skip_cpu_offload=True  → No CPU offloading\n",
    "#   - use_float16=False      → Full fp32 precision\n",
    "#\n",
    "# EXERCISE: After the first run, try changing these:\n",
    "#   1. skip_model_shard=False → enables resharding (lower memory)\n",
    "#   2. skip_cpu_offload=False → enables CPU offload (even lower memory)\n",
    "#   3. use_float16=True       → half precision (half the memory, faster)\n",
    "#   4. hidden_dim=7680        → double the model size (will it OOM?)\n",
    "\n",
    "train_loop_config = {\n",
    "    \"epochs\": 1,                  # Number of training epochs\n",
    "    \"learning_rate\": 0.001,       # Adam learning rate\n",
    "    \"batch_size\": 128,            # Batch size per worker (not global!)\n",
    "    \"skip_model_shard\": True,     # True = keep params after forward (faster)\n",
    "    \"skip_cpu_offload\": True,     # True = no CPU offloading (faster)\n",
    "    \"hidden_dim\": 3840,           # ViT hidden dim (controls model size)\n",
    "    \"use_float16\": False,         # True = use float16 mixed precision\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a817abe",
   "metadata": {},
   "source": [
    "## `Step 6`: Launch Distributed Training\n",
    "\n",
    "Ray Train's `TorchTrainer` handles worker spawning, process group initialization, and checkpoint coordination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4784041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 23:31:54,136\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.23.119:6379...\n",
      "2026-02-18 23:31:54,147\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-ffbqdd398vb4g8i97u3tsubr23.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-18 23:31:54,152\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_338ba8a2c55854004cd37d310db5092fe44dd789.zip' (1.25MiB) to Ray cluster...\n",
      "2026-02-18 23:31:54,158\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_338ba8a2c55854004cd37d310db5092fe44dd789.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [FailurePolicy] RETRY\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   Source: controller\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   Error count: 1 (max allowed: inf)\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m \n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/controller/controller.py\", line 338, in _start_worker_group\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     self._worker_group = self.worker_group_cls.create(\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 132, in create\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     worker_group._start()\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 207, in _start\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     raise e\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 200, in _start\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     self._start_impl(\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 291, in _start_impl\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     raise WorkerGroupStartupTimeoutError(\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m ray.train.ControllerError: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [FailurePolicy] RETRY\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   Source: controller\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   Error count: 2 (max allowed: inf)\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m \n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/controller/controller.py\", line 338, in _start_worker_group\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     self._worker_group = self.worker_group_cls.create(\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 132, in create\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     worker_group._start()\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 207, in _start\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     raise e\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 200, in _start\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     self._start_impl(\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/train/v2/_internal/execution/worker_group/worker_group.py\", line 291, in _start_impl\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m     raise WorkerGroupStartupTimeoutError(\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m ray.train.ControllerError: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(RayTrainWorker pid=11303, ip=10.0.33.107)\u001b[0m INFO:2026-02-18 23:33:11 11303:11303 init.cpp:148] Registering daemon config loader, cpuOnly =  0\n",
      "\u001b[36m(RayTrainWorker pid=11303, ip=10.0.33.107)\u001b[0m INFO:2026-02-18 23:33:11 11303:11303 CuptiActivityProfiler.cpp:244] CUDA versions. CUPTI: 26; Runtime: 12080; Driver: 12090\n",
      "\u001b[36m(RayTrainWorker pid=10950, ip=10.0.40.249)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m Started training worker group of size 2: \n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m - (ip=10.0.40.249, pid=10950) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m - (ip=10.0.33.107, pid=11303) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
      "  0%|          | 0.00/26.4M [00:00<?, ?B/s]107)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=10950, ip=10.0.40.249)\u001b[0m INFO:2026-02-18 23:33:11 10950:10950 init.cpp:148] Registering daemon config loader, cpuOnly =  0\n",
      "\u001b[36m(RayTrainWorker pid=10950, ip=10.0.40.249)\u001b[0m INFO:2026-02-18 23:33:11 10950:10950 CuptiActivityProfiler.cpp:244] CUDA versions. CUPTI: 26; Runtime: 12080; Driver: 12090\n",
      "  0%|          | 32.8k/26.4M [00:00<01:51, 237kB/s] \n",
      "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.0MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 207kB/s] \n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 213kB/s] \n",
      "  0%|          | 0.00/4.42M [00:00<?, ?B/s]\u001b[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      " 28%|██▊       | 1.25M/4.42M [00:01<00:01, 2.06MB/s]\u001b[32m [repeated 46x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=11303, ip=10.0.33.107)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_bd7ac8fb/checkpoint_2026-02-18_23-34-44.513953)\n",
      "\u001b[36m(RayTrainWorker pid=11303, ip=10.0.33.107)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_bd7ac8fb/checkpoint_2026-02-18_23-34-44.513953), metrics={'loss': 2.378581392765045, 'epoch': 1}, validation_spec=None)\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 52.7MB/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.50MB/s]\n",
      "\u001b[36m(RayTrainWorker pid=10950, ip=10.0.40.249)\u001b[0m {'loss': 2.3675916135311126, 'epoch': 1}\n",
      "\u001b[36m(RayTrainWorker pid=11303, ip=10.0.33.107)\u001b[0m Reporting training result 2: TrainingReport(checkpoint=None, metrics={}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=10950, ip=10.0.40.249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_bd7ac8fb/checkpoint_2026-02-18_23-34-44.513953)\n",
      "\u001b[36m(RayTrainWorker pid=10950, ip=10.0.40.249)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_bd7ac8fb/checkpoint_2026-02-18_23-34-44.513953), metrics={'loss': 2.3675916135311126, 'epoch': 1}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=10950, ip=10.0.40.249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_bd7ac8fb/full_model)\n",
      "\u001b[36m(RayTrainWorker pid=10950, ip=10.0.40.249)\u001b[0m Reporting training result 2: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_bd7ac8fb/full_model), metrics={}, validation_spec=None)\n",
      "\u001b[36m(TrainController pid=45328)\u001b[0m [State Transition] RUNNING -> SHUTTING_DOWN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=45328)\u001b[0m [State Transition] SHUTTING_DOWN -> FINISHED.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#                Launch distributed training!\n",
    "# ============================================================\n",
    "\n",
    "# TorchTrainer is the main entry point for Ray Train.\n",
    "# It takes your training function, config, and handles all the distributed plumbing:\n",
    "#     - Worker spawning and GPU assignment\n",
    "#     - Process group initialization (NCCL backend)\n",
    "#     - Checkpoint coordination between workers\n",
    "#     - Fault tolerance (auto-restart on failure)\n",
    "\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Generate a unique experiment name (folder) for this run\n",
    "# ------------------------------------------------------------\n",
    "training_name = \"fsdp_mnist_\" + str(uuid.uuid4())[:8]\n",
    "\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_func,                               # The function each worker runs\n",
    "    scaling_config=scaling_config,            # How many workers, what GPUs\n",
    "    train_loop_config=train_loop_config,      # Hyperparameters\n",
    "    run_config=ray.train.RunConfig(\n",
    "        storage_path=\"/mnt/cluster_storage/\", # Shared storage for checkpoints\n",
    "        name=training_name,                   # Experiment name (unique for each run)\n",
    "        failure_config=ray.train.FailureConfig(\n",
    "            max_failures=2                    # Auto-retry up to 2 times on worker failure\n",
    "        ),\n",
    "        worker_runtime_env={\n",
    "            # These env vars configure the Kineto profiler to avoid log warnings.\n",
    "            \"env_vars\": {\n",
    "                \"KINETO_USE_DAEMON\": \"1\",\n",
    "                \"KINETO_DAEMON_INIT_DELAY_S\": \"5\"\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "# trainer.fit() blocks until training completes.\n",
    "#     INITIALIZING -> SCHEDULING -> RUNNING -> SHUTTING_DOWN -> FINISHED\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526296b",
   "metadata": {},
   "source": [
    "## `Step 7`: Inspect Training Artifacts\n",
    "\n",
    "Training artifacts include:\n",
    "- `checkpoint_*/` - Epoch checkpoints with distributed shards\n",
    "- `full_model/` - Consolidated model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d8fdcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts in /mnt/cluster_storage/fsdp_mnist_bd7ac8fb/:\n",
      "total 24\n",
      "drwxr-xr-x 23 ray  1000 6144 Feb 18 23:31 ..\n",
      "-rw-r--r--  1 ray users    0 Feb 18 23:31 .validate_storage_marker\n",
      "drwxr-xr-x  2 ray users 6144 Feb 18 23:34 checkpoint_2026-02-18_23-34-44.513953\n",
      "drwxr-xr-x  4 ray users 6144 Feb 18 23:35 .\n",
      "drwxr-xr-x  2 ray users 6144 Feb 18 23:35 full_model\n",
      "-rw-r--r--  1 ray users  335 Feb 18 23:36 checkpoint_manager_snapshot.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# List training artifacts on shared storage\n",
    "# ============================================================\n",
    "#   - checkpoint_* = sharded (for resuming training)\n",
    "#   - full_model/ = consolidated (for inference)\n",
    "storage_path = f\"/mnt/cluster_storage/{training_name}/\"\n",
    "print(f\"Artifacts in {storage_path}:\")\n",
    "!ls -ltra $storage_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ba48a",
   "metadata": {},
   "source": [
    "## `Step 8`: Load Model for Inference\n",
    "\n",
    "The consolidated model (`full-model.pt`) is a standard PyTorch checkpoint that works without FSDP2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e880332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load the consolidated model for inference\n",
    "# ============================================================\n",
    "\n",
    "# 1. Create the same architecture as used during training\n",
    "model = init_model(train_loop_config[\"hidden_dim\"])\n",
    "\n",
    "# 2. Load the full model checkpoint (standard PyTorch, no FSDP required)\n",
    "model_state_dict = torch.load(\n",
    "    f\"/mnt/cluster_storage/{training_name}/full_model/full-model.pt\",\n",
    "    map_location='cpu'  # Loads model on CPU, works even without GPU\n",
    ")\n",
    "\n",
    "# 3. Load the checkpointed weights into the model\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede567a3",
   "metadata": {},
   "source": [
    "Load some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "320412a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: .\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Prepare test data for inference\n",
    "# ============================================================\n",
    "# Use the same preprocessing as during training to ensure compatibility.\n",
    "# The FashionMNIST test set contains 10,000 images across 10 classes.\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "test_data = FashionMNIST(\n",
    "    root=\".\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9db011",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77437fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 4 (Coat), Actual: 9 (Ankle Boot)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Run a single inference prediction on the test dataset\n",
    "# ============================================================\n",
    "\n",
    "# Set model to evaluation mode (disables dropout, uses running stats for batch norm)\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation for inference (saves memory and is faster)\n",
    "with torch.no_grad():\n",
    "    # Prepare the first test image:\n",
    "    # - reshape to [batch_size, channels, height, width]\n",
    "    # - convert to float tensor\n",
    "    first_image = test_data.data[0].reshape(1, 1, 28, 28).float()\n",
    "    output = model(first_image)\n",
    "    \n",
    "    # Get the predicted class (index with highest score)\n",
    "    predicted_label = output.argmax().item()\n",
    "    # Get the actual label\n",
    "    actual_label = test_data.targets[0].item()\n",
    "    \n",
    "    # Map label indices to human-readable class names\n",
    "    class_names = [\n",
    "        \"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "        \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Predicted: {predicted_label} ({class_names[predicted_label]}), \"\n",
    "          f\"Actual: {actual_label} ({class_names[actual_label]})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b29d2",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed22fa",
   "metadata": {},
   "source": [
    "\n",
    "This tutorial covered:\n",
    "1. **FSDP2 sharding** - Distributed model parameters across GPUs using `fully_shard()`\n",
    "2. **Ray Train integration** - Multi-GPU training with automatic process group management\n",
    "3. **PyTorch DCP** - Sharded checkpointing with automatic resharding on load\n",
    "4. **Inference** - Loading consolidated model for single-GPU inference\n",
    "\n",
    "**Next Steps:**\n",
    "- Add CPU offloading: `CPUOffloadPolicy()` for memory-constrained scenarios\n",
    "- Add mixed precision: `MixedPrecisionPolicy(param_dtype=torch.float16)`\n",
    "- Try [DeepSpeed tutorial](./DeepSpeed_RayTrain_Tutorial.ipynb) for comparison\n",
    "\n",
    "**Resources:**\n",
    "- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n",
    "- [Ray Train Documentation](https://docs.ray.io/en/latest/train/getting-started-pytorch.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
