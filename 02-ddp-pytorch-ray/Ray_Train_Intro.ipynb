{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Distributed Training with PyTorch and Ray Train\n",
    "\n",
    "This notebook will show you how to distribute your PyTorch training code with Ray Train and Ray Data, getting performance and usability improvements along the way.\n",
    "\n",
    "In this tutorial, you:\n",
    "1. Start with a basic single machine PyTorch example.\n",
    "2. Distribute it to multiple GPUs on multiple machines with [Ray Train](https://docs.ray.io/en/latest/train/train.html) and, if you are using an Anyscale Workspace, inspect results with the Ray Train dashboard.\n",
    "3. Scale data ingest separately from training with [Ray Data](https://docs.ray.io/en/latest/data/data.html) and, if you are using an Anyscale Workspace, inspect results with the Ray Data dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Step 1`: Start with a basic single machine PyTorch example\n",
    "\n",
    "In this step you train a PyTorch VisionTransformer model to recognize objects using the open CIFAR-10 dataset. It's a minimal example that trains on a single machine. Note that the code has multiple functions to highlight the changes needed to run things with Ray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/single_gpu_pytorch_v3.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|An overview of the single GPU training process. At a high level, here is how training loop in PyTorch looks like. The key stages include loading the dataset; run the training on mini-batches on a single GPU; saving the model checkpoint to the persistent storage.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from filelock import FileLock\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Normalize, ToTensor\n",
    "from torchvision.models import VisionTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function that returns PyTorch DataLoaders for train and test data. Note how the code also preprocesses the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size):\n",
    "    # Transform to normalize the input images.\n",
    "    transform = transforms.Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        # Download training data from open datasets.\n",
    "        training_data = datasets.CIFAR10(\n",
    "            root=\"~/data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "        # Download test data from open datasets.\n",
    "        testing_data = datasets.CIFAR10(\n",
    "            root=\"~/data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(testing_data, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a function that runs the core training loop. Note how the code synchronously alternates between training the model for one epoch and then evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "    lr = 1e-3\n",
    "    epochs = 1\n",
    "    batch_size = 512\n",
    "\n",
    "    # Get data loaders inside the worker training function.\n",
    "    train_dataloader, valid_dataloader = get_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        image_size=32,   # CIFAR-10 image size is 32x32\n",
    "        patch_size=4,    # Patch size is 4x4\n",
    "        num_layers=12,   # Number of transformer layers\n",
    "        num_heads=8,     # Number of attention heads\n",
    "        hidden_dim=384,  # Hidden size (can be adjusted)\n",
    "        mlp_dim=768,     # MLP dimension (can be adjusted)\n",
    "        num_classes=10   # CIFAR-10 has 10 classes\n",
    "    )\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "    # Model training loop.\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X, y in tqdm(train_dataloader, desc=f\"Train Epoch {epoch}\"):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss, num_correct, num_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(valid_dataloader, desc=f\"Valid Epoch {epoch}\"):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                num_total += y.shape[0]\n",
    "                num_correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        valid_loss /= len(train_dataloader)\n",
    "        accuracy = num_correct / num_total\n",
    "\n",
    "        print({\"epoch_num\": epoch, \"loss\": valid_loss, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training should take about 2 minutes and 10 seconds with an accuracy of about 0.35.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Step 2`: Distribute training to multiple machines with Ray Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, modify this example to run with Ray Train on multiple machines with distributed data parallel (DDP) training. In DDP training, each process trains a copy of the model on a subset of the data and synchronizes gradients across all processes after each backward pass to keep models consistent. Essentially, Ray Train allows you to wrap PyTorch training code in a function and run the function on each worker in your Ray Cluster. With a few modifications, you get the fault tolerance and auto-scaling of a [Ray Cluster](https://docs.ray.io/en/latest/cluster/getting-started.html), as well as the observability and ease-of-use of [Ray Train](https://docs.ray.io/en/latest/train/train.html).\n",
    "\n",
    "First, set some environment variables and import some modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture overview\n",
    "\n",
    "Ray Train's architecture is based on the following components:\n",
    "1. A Ray Train Controller/Driver that schedules the training workers, handles errors and manages checkpoints\n",
    "2. Ray Train Worker executing the training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key API concepts\n",
    "\n",
    "Below are the key API concepts of Ray Train:\n",
    "\n",
    "1. `train_loop_per_worker`: This is the main function that contains your model training logic.\n",
    "2. `ScalingConfig`: This is used to specify the number of workers and compute resources (CPUs or GPUs, TPUs).\n",
    "3. `Trainer`: This is used to manage the training process.\n",
    "4. `Trainer.fit()`: This is used to start the training process.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the diagram showing how the key components usually work together.\n",
    "\n",
    "- The Train Controller/Driver is constantly performing health checks on the Train workers.\n",
    "- The Train workers are running the training loop and at a particular frequency checkpointing the model to a persistent storage.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/ray_train_detailed_architecture.png\" width=\"800\" loading=\"lazy\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case where we have a very large dataset of images that would take a long time to train on a single GPU. We would now like to scale this training job to run on multiple GPUs.\n",
    "\n",
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Schematic overview of DistributedDataParallel (DDP) training: (1) the model is replicated from the <code>GPU rank 0</code> to all other workers; (2) each worker receives a shard of the dataset and processes a mini-batch; (3) during the backward pass, gradients are averaged across GPUs; (4) checkpoint and metrics from rank 0 GPU are saved to the persistent storage.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train\n",
    "from ray.train import RunConfig, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "import tempfile\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify the training function you wrote earlier. Every difference from the previous script is highlighted and explained with a numbered comment; for example, “[1].”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_per_worker(config: Dict):\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size_per_worker\"]\n",
    "\n",
    "    # Get data loaders inside the worker training function.\n",
    "    train_dataloader, valid_dataloader = get_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    # [1] Prepare data loader for distributed training.\n",
    "    # The prepare_data_loader method assigns unique rows of data to each worker so that\n",
    "    # the model sees each row once per epoch.\n",
    "    # NOTE: This approach only works for map-style datasets. For a general distributed\n",
    "    # preprocessing and sharding solution, see the next part using Ray Data for data \n",
    "    # ingestion.\n",
    "    # =================================================================================\n",
    "    train_dataloader = ray.train.torch.prepare_data_loader(train_dataloader)\n",
    "    valid_dataloader = ray.train.torch.prepare_data_loader(valid_dataloader)\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        image_size=32,   # CIFAR-10 image size is 32x32\n",
    "        patch_size=4,    # Patch size is 4x4\n",
    "        num_layers=12,   # Number of transformer layers\n",
    "        num_heads=8,     # Number of attention heads\n",
    "        hidden_dim=384,  # Hidden size (can be adjusted)\n",
    "        mlp_dim=768,     # MLP dimension (can be adjusted)\n",
    "        num_classes=10   # CIFAR-10 has 10 classes\n",
    "    )\n",
    "\n",
    "    # [2] Prepare and wrap your model with DistributedDataParallel.\n",
    "    # The prepare_model method moves the model to the correct GPU/CPU device.\n",
    "    # =======================================================================\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "    # Model training loop.\n",
    "    for epoch in range(epochs):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            # Required for the distributed sampler to shuffle properly across epochs.\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        for X, y in tqdm(train_dataloader, desc=f\"Train Epoch {epoch}\"):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss, num_correct, num_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(valid_dataloader, desc=f\"Valid Epoch {epoch}\"):\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                num_total += y.shape[0]\n",
    "                num_correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        valid_loss /= len(train_dataloader)\n",
    "        accuracy = num_correct / num_total\n",
    "\n",
    "        # [3] (Optional) Report checkpoints and attached metrics to Ray Train.\n",
    "        # ====================================================================\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics={\"loss\": valid_loss, \"accuracy\": accuracy},\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "            if ray.train.get_context().get_world_rank() == 0:\n",
    "                print({\"epoch_num\": epoch, \"loss\": valid_loss, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the training function on the Ray Cluster with a TorchTrainer using GPU workers.\n",
    "\n",
    "The `TorchTrainer` takes the following arguments:\n",
    "* `train_loop_per_worker`: the training function you defined earlier. Each Ray Train worker runs this function. Note that you can call special Ray Train functions from within this function.\n",
    "* `train_loop_config`: a hyperparameter dictionary. Each Ray Train worker calls its `train_loop_per_worker` function with this dictionary.\n",
    "* `scaling_config`: a configuration object that specifies the number of workers and compute resources, for example, CPUs or GPUs, that your training run needs.\n",
    "* `run_config`: a configuration object that specifies various fields used at runtime, including the path to save checkpoints.\n",
    "\n",
    "`trainer.fit` spawns a controller process to orchestrate the training run and worker processes to actually execute the PyTorch training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar_10(num_workers, use_gpu):\n",
    "    global_batch_size = 512\n",
    "\n",
    "    train_config = {\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size_per_worker\": global_batch_size // num_workers,\n",
    "    }\n",
    "\n",
    "    # [1] Start distributed training.\n",
    "    # Define computation resources for workers.\n",
    "    # Run `train_func_per_worker` on those workers.\n",
    "    # =============================================\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers, use_gpu=use_gpu)\n",
    "    run_config = RunConfig(\n",
    "        # /mnt/cluster_storage is an Anyscale-specific storage path.\n",
    "        # OSS users should set up this path themselves.\n",
    "        storage_path=\"/mnt/cluster_storage\", \n",
    "        name=f\"train_run-{uuid.uuid4().hex}\",\n",
    "    )\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        train_loop_config=train_config,\n",
    "        scaling_config=scaling_config,\n",
    "        run_config=run_config,\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "    print(f\"Training result: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_cifar_10(num_workers=8, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you're running training in a data parallel fashion this time, it should take under 1 minute while maintaining similar accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Step 3`: Scale data ingest separately from training with Ray Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Modify this example to load data with Ray Data instead of the native PyTorch DataLoader. With a few modifications, you can scale data preprocessing and training separately. For example, you can do the former with a pool of CPU workers and the latter with a pool of GPU workers. See [How does Ray Data compare to other solutions for offline inference?](https://docs.ray.io/en/latest/data/comparisons.html#how-does-ray-data-compare-to-other-solutions-for-ml-training-ingest) for a comparison between Ray Data and PyTorch data loading.\n",
    "\n",
    "First, create [Ray Data Datasets](https://docs.ray.io/en/latest/data/key-concepts.html#datasets-and-blocks) from S3 data and inspect their schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram showing the Ray Data and Ray Train integration\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/ray_train_v2_architecture.png\" width=\"1200\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "STORAGE_PATH = \"s3://ray-example-data/cifar10-parquet\"\n",
    "train_dataset = ray.data.read_parquet(f'{STORAGE_PATH}/train')\n",
    "test_dataset = ray.data.read_parquet(f'{STORAGE_PATH}/test')\n",
    "train_dataset.schema()\n",
    "test_dataset.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use Ray Data to transform the data. Note that both loading and transformation happen lazily, which means that only the training workers materialize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cifar(row: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    # Define the torchvision transform.\n",
    "    transform = transforms.Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    row[\"image\"] = transform(row[\"image\"])\n",
    "    return row\n",
    "\n",
    "train_dataset = train_dataset.map(transform_cifar)\n",
    "test_dataset = test_dataset.map(transform_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify the training function you wrote earlier. Every difference from the previous script is highlighted and explained with a numbered comment; for example, “[1].”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_per_worker(config: Dict):\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size_per_worker\"]\n",
    "\n",
    "\n",
    "    # [1] Prepare `Dataloader` for distributed training.\n",
    "    # The get_dataset_shard method gets the associated dataset shard to pass to the \n",
    "    # TorchTrainer constructor in the next code block.\n",
    "    # The iter_torch_batches method lazily shards the dataset among workers.\n",
    "    # =============================================================================\n",
    "    train_data_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    valid_data_shard = ray.train.get_dataset_shard(\"valid\")\n",
    "    train_dataloader = train_data_shard.iter_torch_batches(batch_size=batch_size)\n",
    "    valid_dataloader = valid_data_shard.iter_torch_batches(batch_size=batch_size)\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        image_size=32,   # CIFAR-10 image size is 32x32\n",
    "        patch_size=4,    # Patch size is 4x4\n",
    "        num_layers=12,   # Number of transformer layers\n",
    "        num_heads=8,     # Number of attention heads\n",
    "        hidden_dim=384,  # Hidden size (can be adjusted)\n",
    "        mlp_dim=768,     # MLP dimension (can be adjusted)\n",
    "        num_classes=10   # CIFAR-10 has 10 classes\n",
    "    )\n",
    "\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "    # Model training loop.\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Train Epoch {epoch}\"):\n",
    "            X, y = batch['image'], batch['label']\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss, num_correct, num_total, num_batches = 0, 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_dataloader, desc=f\"Valid Epoch {epoch}\"):\n",
    "                # [2] Each Ray Data batch is a dict so you must access the\n",
    "                # underlying data using the appropriate keys.\n",
    "                # =======================================================\n",
    "                X, y = batch['image'], batch['label']\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                num_total += y.shape[0]\n",
    "                num_batches += 1\n",
    "                num_correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        valid_loss /= num_batches\n",
    "        accuracy = num_correct / num_total\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics={\"loss\": valid_loss, \"accuracy\": accuracy},\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "            if ray.train.get_context().get_world_rank() == 0:\n",
    "                print({\"epoch_num\": epoch, \"loss\": valid_loss, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the training function with the Ray Data Dataset on the Ray Cluster with 8 GPU workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar_10(num_workers, use_gpu):\n",
    "    global_batch_size = 512\n",
    "\n",
    "    train_config = {\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size_per_worker\": global_batch_size // num_workers,\n",
    "    }\n",
    "\n",
    "    # Configure computation resources.\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers, use_gpu=use_gpu)\n",
    "    run_config = RunConfig(\n",
    "        storage_path=\"/mnt/cluster_storage\", \n",
    "        name=f\"train_data_run-{uuid.uuid4().hex}\",\n",
    "    )\n",
    "\n",
    "    # Initialize a Ray TorchTrainer.\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        # [1] With Ray Data you pass the Dataset directly to the Trainer.\n",
    "        # ==============================================================\n",
    "        datasets={\"train\": train_dataset, \"valid\": test_dataset},\n",
    "        train_loop_config=train_config,\n",
    "        scaling_config=scaling_config,\n",
    "        run_config=run_config,\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(f\"Training result: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_cifar_10(num_workers=8, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again your training run should take around 1 minute with similar accuracy. There aren't big performance wins with Ray Data on this example due to the small size of the dataset; for more interesting benchmarking information see [this blog post](https://www.anyscale.com/blog/fast-flexible-scalable-data-loading-for-ml-training-with-ray-data). The main advantage of Ray Data is that it allows you to stream data across heterogeneous compute, maximizing GPU utilization while minimizing RAM usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you are using an Anyscale Workspace, in addition to the Ray Train and Metrics dashboards you saw in the previous section, you can also view the Ray Data dashboard by clicking **Ray Workloads** and then **Data** where you can view the throughput and status of each [Ray Data operator](https://docs.ray.io/en/latest/data/key-concepts.html#operators-and-plans).\n",
    "\n",
    "![Data Dashboard](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/train/examples/pytorch/distributing-pytorch/images/data_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train in Production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here are some use-cases of using Ray Train in production:\n",
    "1. Canva uses Ray Train + Ray Data to cut down Stable Diffusion training costs by 3.7x. Read this [Anyscale blog post here](https://www.anyscale.com/blog/scalable-and-cost-efficient-stable-diffusion-pre-training-with-ray) and the [Canva  case study here](https://www.anyscale.com/resources/case-study/how-canva-built-a-modern-ai-platform-using-anyscale)\n",
    "2. Anyscale uses Ray Train + Deepspeed to finetune language models. Read more [here](https://github.com/ray-project/ray/tree/master/doc/source/templates/04_finetuning_llms_with_deepspeed).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this notebook, you:\n",
    "* Trained a PyTorch VisionTransformer model on a Ray Cluster with multiple GPU workers with Ray Train and Ray Data\n",
    "* Verified that speed improved without affecting accuracy\n",
    "* Gained insight into your distributed training and data preprocessing workloads with various dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
