{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Distributed Training with PyTorch and Ray Train\n",
    "\n",
    "This notebook will show you how to distribute your PyTorch training code with Ray Train and Ray Data, getting performance and usability improvements along the way.\n",
    "\n",
    "In this tutorial, you:\n",
    "1. Start with a basic single machine PyTorch example.\n",
    "2. Distribute it to multiple GPUs on multiple machines with [Ray Train](https://docs.ray.io/en/latest/train/train.html) and, if you are using an Anyscale Workspace, inspect results with the Ray Train dashboard.\n",
    "3. Scale data ingest separately from training with [Ray Data](https://docs.ray.io/en/latest/data/data.html) and, if you are using an Anyscale Workspace, inspect results with the Ray Data dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Step 1`: Start with a basic single machine PyTorch example\n",
    "\n",
    "In this step you train a PyTorch VisionTransformer model to recognize objects using the open CIFAR-10 dataset. It's a minimal example that trains on a single machine. Note that the code has multiple functions to highlight the changes needed to run things with Ray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/single_gpu_pytorch_v3.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|An overview of the single GPU training process. At a high level, here is how training loop in PyTorch looks like. The key stages include loading the dataset; run the training on mini-batches on a single GPU; saving the model checkpoint to the persistent storage.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from filelock import FileLock\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Normalize, ToTensor\n",
    "from torchvision.models import VisionTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function that returns PyTorch DataLoaders for train and test data. Note how the code also preprocesses the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size):\n",
    "    # Transform to normalize the input images.\n",
    "    transform = transforms.Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        # Download training data from open datasets.\n",
    "        training_data = datasets.CIFAR10(\n",
    "            root=\"~/data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "        # Download test data from open datasets.\n",
    "        testing_data = datasets.CIFAR10(\n",
    "            root=\"~/data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(testing_data, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a function that runs the core training loop. Note how the code synchronously alternates between training the model for one epoch and then evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "    lr = 1e-3\n",
    "    epochs = 1\n",
    "    batch_size = 512\n",
    "\n",
    "    # Get data loaders inside the worker training function.\n",
    "    train_dataloader, valid_dataloader = get_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        image_size=32,   # CIFAR-10 image size is 32x32\n",
    "        patch_size=4,    # Patch size is 4x4\n",
    "        num_layers=12,   # Number of transformer layers\n",
    "        num_heads=8,     # Number of attention heads\n",
    "        hidden_dim=384,  # Hidden size (can be adjusted)\n",
    "        mlp_dim=768,     # MLP dimension (can be adjusted)\n",
    "        num_classes=10   # CIFAR-10 has 10 classes\n",
    "    )\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "    # Model training loop.\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X, y in tqdm(train_dataloader, desc=f\"Train Epoch {epoch}\"):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss, num_correct, num_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(valid_dataloader, desc=f\"Valid Epoch {epoch}\"):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                num_total += y.shape[0]\n",
    "                num_correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        valid_loss /= len(train_dataloader)\n",
    "        accuracy = num_correct / num_total\n",
    "\n",
    "        print({\"epoch_num\": epoch, \"loss\": valid_loss, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:04<00:00, 38.8MB/s] \n",
      "Train Epoch 0: 100%|██████████| 98/98 [27:22<00:00, 16.76s/it]\n",
      "Valid Epoch 0: 100%|██████████| 20/20 [00:56<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch_num': 0, 'loss': 0.3542855759056247, 'accuracy': 0.3493}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Step 2`: Distribute training to multiple machines with Ray Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, modify this example to run with Ray Train on multiple machines with distributed data parallel (DDP) training. In DDP training, each process trains a copy of the model on a subset of the data and synchronizes gradients across all processes after each backward pass to keep models consistent. Essentially, Ray Train allows you to wrap PyTorch training code in a function and run the function on each worker in your Ray Cluster. With a few modifications, you get the fault tolerance and auto-scaling of a [Ray Cluster](https://docs.ray.io/en/latest/cluster/getting-started.html), as well as the observability and ease-of-use of [Ray Train](https://docs.ray.io/en/latest/train/train.html).\n",
    "\n",
    "First, set some environment variables and import some modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture overview\n",
    "\n",
    "Ray Train's architecture is based on the following components:\n",
    "1. A `Ray Train Controller/Driver` that schedules the training workers, handles errors and manages checkpoints\n",
    "2. `Ray Train Worker` executing the training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/train_architecture.jpg\" loading=\"lazy\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key API concepts\n",
    "\n",
    "Below are the key API concepts of Ray Train:\n",
    "\n",
    "1. `train_loop_per_worker`: This is the main function that contains your model training logic.\n",
    "2. `ScalingConfig`: This is used to specify the number of workers and compute resources (CPUs or GPUs, TPUs).\n",
    "3. `Trainer`: This is used to manage the training process.\n",
    "4. `Trainer.fit()`: This is used to start the training process.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the diagram showing how the key components usually work together.\n",
    "\n",
    "- The Train Controller/Driver is constantly performing health checks on the Train workers.\n",
    "- The Train workers are running the training loop and at a particular frequency checkpointing the model to a persistent storage.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/ray_train_detailed_architecture.png\" width=\"800\" loading=\"lazy\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case where we have a very large dataset of images that would take a long time to train on a single GPU. We would now like to scale this training job to run on multiple GPUs.\n",
    "\n",
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Schematic overview of DistributedDataParallel (DDP) training: (1) the model is replicated from the <code>GPU rank 0</code> to all other workers; (2) each worker receives a shard of the dataset and processes a mini-batch; (3) during the backward pass, gradients are averaged across GPUs; (4) checkpoint and metrics from rank 0 GPU are saved to the persistent storage.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train\n",
    "from ray.train import RunConfig, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "import tempfile\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify the training function you wrote earlier. Every difference from the previous script is highlighted and explained with a numbered comment; for example, “[1].”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_per_worker(config: Dict):\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size_per_worker\"]\n",
    "\n",
    "    # Get data loaders inside the worker training function.\n",
    "    train_dataloader, valid_dataloader = get_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    # [1] Prepare data loader for distributed training.\n",
    "    # The prepare_data_loader method assigns unique rows of data to each worker so that\n",
    "    # the model sees each row once per epoch.\n",
    "    # NOTE: This approach only works for map-style datasets. For a general distributed\n",
    "    # preprocessing and sharding solution, see the next part using Ray Data for data \n",
    "    # ingestion.\n",
    "    # =================================================================================\n",
    "    train_dataloader = ray.train.torch.prepare_data_loader(train_dataloader)\n",
    "    valid_dataloader = ray.train.torch.prepare_data_loader(valid_dataloader)\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        image_size=32,   # CIFAR-10 image size is 32x32\n",
    "        patch_size=4,    # Patch size is 4x4\n",
    "        num_layers=12,   # Number of transformer layers\n",
    "        num_heads=8,     # Number of attention heads\n",
    "        hidden_dim=384,  # Hidden size (can be adjusted)\n",
    "        mlp_dim=768,     # MLP dimension (can be adjusted)\n",
    "        num_classes=10   # CIFAR-10 has 10 classes\n",
    "    )\n",
    "\n",
    "    # [2] Prepare and wrap your model with DistributedDataParallel.\n",
    "    # The prepare_model method moves the model to the correct GPU/CPU device.\n",
    "    # =======================================================================\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "    # Model training loop.\n",
    "    for epoch in range(epochs):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            # Required for the distributed sampler to shuffle properly across epochs.\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        for X, y in tqdm(train_dataloader, desc=f\"Train Epoch {epoch}\"):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss, num_correct, num_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(valid_dataloader, desc=f\"Valid Epoch {epoch}\"):\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                num_total += y.shape[0]\n",
    "                num_correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        valid_loss /= len(train_dataloader)\n",
    "        accuracy = num_correct / num_total\n",
    "\n",
    "        # [3] (Optional) Report checkpoints and attached metrics to Ray Train.\n",
    "        # ====================================================================\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics={\"loss\": valid_loss, \"accuracy\": accuracy},\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "            if ray.train.get_context().get_world_rank() == 0:\n",
    "                print({\"epoch_num\": epoch, \"loss\": valid_loss, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the training function on the Ray Cluster with a TorchTrainer using GPU workers.\n",
    "\n",
    "The `TorchTrainer` takes the following arguments:\n",
    "* `train_loop_per_worker`: the training function you defined earlier. Each Ray Train worker runs this function. Note that you can call special Ray Train functions from within this function.\n",
    "* `train_loop_config`: a hyperparameter dictionary. Each Ray Train worker calls its `train_loop_per_worker` function with this dictionary.\n",
    "* `scaling_config`: a configuration object that specifies the number of workers and compute resources, for example, CPUs or GPUs, that your training run needs.\n",
    "* `run_config`: a configuration object that specifies various fields used at runtime, including the path to save checkpoints.\n",
    "\n",
    "`trainer.fit` spawns a controller process to orchestrate the training run and worker processes to actually execute the PyTorch training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=64403)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=64403)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(RayTrainWorker pid=17211, ip=10.0.170.190)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TrainController pid=64403)\u001b[0m Started training worker group of size 2: \n",
      "\u001b[36m(TrainController pid=64403)\u001b[0m - (ip=10.0.170.190, pid=17211) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TrainController pid=64403)\u001b[0m - (ip=10.0.128.198, pid=19224) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TrainController pid=64403)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
      "  0%|          | 0.00/170M [00:00<?, ?B/s]0.190)\u001b[0m \n",
      "  0%|          | 65.5k/170M [00:00<04:41, 605kB/s]0m \n",
      " 93%|█████████▎| 159M/170M [00:03<00:00, 45.0MB/s]0m \n",
      " 96%|█████████▌| 164M/170M [00:03<00:00, 39.1MB/s]0m \n",
      " 98%|█████████▊| 168M/170M [00:03<00:00, 35.7MB/s]0m \n",
      "100%|██████████| 170M/170M [00:04<00:00, 42.6MB/s]0m \n",
      " 93%|█████████▎| 159M/170M [00:04<00:00, 36.7MB/s]0m \n",
      " 96%|█████████▌| 164M/170M [00:04<00:00, 39.1MB/s]0m \n",
      "100%|██████████| 170M/170M [00:04<00:00, 39.4MB/s]0m \n",
      "\u001b[36m(RayTrainWorker pid=17211, ip=10.0.170.190)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=17211, ip=10.0.170.190)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "  0%|          | 0.00/170M [00:00<?, ?B/s]8.198)\u001b[0m \n",
      " 91%|█████████ | 155M/170M [00:03<00:00, 33.6MB/s]\u001b[32m [repeated 68x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Train Epoch 0:   0%|          | 0/98 [00:00<?, ?it/s]\n",
      "Train Epoch 0:   1%|          | 1/98 [00:00<00:59,  1.62it/s]\n",
      "\u001b[36m(RayTrainWorker pid=19224, ip=10.0.128.198)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=19224, ip=10.0.128.198)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "Train Epoch 0:   0%|          | 0/98 [00:00<?, ?it/s]\n",
      "Train Epoch 0:  17%|█▋        | 17/98 [00:05<00:27,  2.91it/s]\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "Train Epoch 0:  33%|███▎      | 32/98 [00:11<00:22,  2.90it/s]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Train Epoch 0:  48%|████▊     | 47/98 [00:16<00:17,  2.88it/s]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Train Epoch 0:  63%|██████▎   | 62/98 [00:21<00:12,  2.87it/s]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Train Epoch 0:  79%|███████▊  | 77/98 [00:26<00:07,  2.87it/s]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Train Epoch 0:  92%|█████████▏| 90/98 [00:31<00:02,  2.87it/s]\n",
      "Train Epoch 0:  91%|█████████ | 89/98 [00:30<00:03,  2.86it/s]\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Train Epoch 0: 100%|██████████| 98/98 [00:34<00:00,  2.88it/s]\n",
      "Valid Epoch 0:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Valid Epoch 0:   5%|▌         | 1/20 [00:00<00:05,  3.71it/s]\n",
      "Valid Epoch 0: 100%|██████████| 20/20 [00:02<00:00,  8.74it/s]\n",
      "Train Epoch 0:  99%|█████████▉| 97/98 [00:33<00:00,  2.86it/s]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=17211, ip=10.0.170.190)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/train_run-bdff82ee7a794ba8aca67fea413e23ca/checkpoint_2026-02-02_06-17-28.511894)\n",
      "\u001b[36m(RayTrainWorker pid=17211, ip=10.0.170.190)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/train_run-bdff82ee7a794ba8aca67fea413e23ca/checkpoint_2026-02-02_06-17-28.511894), metrics={'loss': 0.3551751241392019, 'accuracy': 0.3476}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=17211, ip=10.0.170.190)\u001b[0m {'epoch_num': 0, 'loss': 0.3551751241392019, 'accuracy': 0.3476}\n",
      "\u001b[36m(TrainController pid=64403)\u001b[0m [State Transition] RUNNING -> SHUTTING_DOWN.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result: Result(metrics={'loss': 0.3551751241392019, 'accuracy': 0.3476}, checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/train_run-bdff82ee7a794ba8aca67fea413e23ca/checkpoint_2026-02-02_06-17-28.511894), error=None, path='/mnt/cluster_storage/train_run-bdff82ee7a794ba8aca67fea413e23ca', metrics_dataframe=       loss  accuracy\n",
      "0  0.355175    0.3476, best_checkpoints=[(Checkpoint(filesystem=local, path=/mnt/cluster_storage/train_run-bdff82ee7a794ba8aca67fea413e23ca/checkpoint_2026-02-02_06-17-28.511894), {'loss': 0.3551751241392019, 'accuracy': 0.3476})], _storage_filesystem=<pyarrow._fs.LocalFileSystem object at 0x7683b441ec70>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=64403)\u001b[0m [State Transition] SHUTTING_DOWN -> FINISHED.\n"
     ]
    }
   ],
   "source": [
    "def train_cifar_10(num_workers, use_gpu):\n",
    "    global_batch_size = 512\n",
    "\n",
    "    train_config = {\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size_per_worker\": global_batch_size // num_workers,\n",
    "    }\n",
    "\n",
    "    # [1] Start distributed training.\n",
    "    # Define computation resources for workers.\n",
    "    # Run `train_func_per_worker` on those workers.\n",
    "    # =============================================\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers, use_gpu=use_gpu)\n",
    "    run_config = RunConfig(\n",
    "        # /mnt/cluster_storage is an Anyscale-specific storage path.\n",
    "        # OSS users should set up this path themselves.\n",
    "        storage_path=\"/mnt/cluster_storage\", \n",
    "        name=f\"train_run-{uuid.uuid4().hex}\",\n",
    "    )\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        train_loop_config=train_config,\n",
    "        scaling_config=scaling_config,\n",
    "        run_config=run_config,\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "    print(f\"Training result: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_cifar_10(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you're running training in a data parallel fashion this time, it should take under 1 minute while maintaining similar accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Step 3`: Scale data ingest separately from training with Ray Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Modify this example to load data with Ray Data instead of the native PyTorch DataLoader. With a few modifications, you can scale data preprocessing and training separately. For example, you can do the former with a pool of CPU workers and the latter with a pool of GPU workers. See [How does Ray Data compare to other solutions for offline inference?](https://docs.ray.io/en/latest/data/comparisons.html#how-does-ray-data-compare-to-other-solutions-for-ml-training-ingest) for a comparison between Ray Data and PyTorch data loading.\n",
    "\n",
    "First, create [Ray Data Datasets](https://docs.ray.io/en/latest/data/key-concepts.html#datasets-and-blocks) from S3 data and inspect their schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare differences on a distributed setup\n",
    "\n",
    "Let's now look at how PyTorch DataLoader vs Ray Data work in a distributed setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a diagram showing how Pytorch DataLoader works in a distributed setup.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/pytorch_dataloader_multi_worker.png\" width=\"800\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a diagram showing how Ray Data works in a distributed setup.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/ray_data_ray_train_multi_worker.png\" width=\"1000\" loading=\"lazy\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Integrate Ray Train with Ray Data\n",
    "Use both Ray Train and Ray Data when you face one of the following challenges:\n",
    "| Challenge | Detail | Solution |\n",
    "| --- | --- | --- |\n",
    "| Need a **consistent interface for loading and sharding** data | The training process may need to load data from various sources, such as Parquet, CSV, or lakehouses. | Ray Data provides a standardize approach for loading, shuffling, sharding, and streaming batch data to your distributed training run. |\n",
    "| Need to perform online or **just-in-time data processing** on **CPU and GPU nodes** | You are in a scenario where it makes more sense to perform data preprocessing on the fly instead of preprocess your data in batch in a separate job. This usually is the case if preprocessing your data takes an extremely long time (on the order of weeks) and you don't want this to block your training run. | Ray Train's integration with Ray Data makes it easy to implement just-in-time data processing. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Architecture\n",
    "\n",
    "Here is a diagram showing the a sample Ray Data and Ray Train integration for a simple homogenous cluster setup\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/homogeneous_training_ray_train_ray_data.png\" width=\"900\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column  Type\n",
       "------  ----\n",
       "image   ArrowTensorType(shape=(32, 32, 3), dtype=uint8)\n",
       "label   int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray.data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "STORAGE_PATH = \"s3://ray-example-data/cifar10-parquet\"\n",
    "\n",
    "train_dataset = ray.data.read_parquet(f'{STORAGE_PATH}/train')\n",
    "test_dataset = ray.data.read_parquet(f'{STORAGE_PATH}/test')\n",
    "\n",
    "train_dataset.schema()\n",
    "test_dataset.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use Ray Data to transform the data. Note that both loading and transformation happen lazily, which means that only the training workers materialize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cifar(row: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    # Define the torchvision transform.\n",
    "    transform = transforms.Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    row[\"image\"] = transform(row[\"image\"])\n",
    "    return row\n",
    "\n",
    "train_dataset = train_dataset.map(transform_cifar)\n",
    "test_dataset = test_dataset.map(transform_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify the training function you wrote earlier. Every difference from the previous script is highlighted and explained with a numbered comment; for example, “[1].”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_per_worker(config: Dict):\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size_per_worker\"]\n",
    "\n",
    "\n",
    "    # [1] Prepare `Dataloader` for distributed training.\n",
    "    # The get_dataset_shard method gets the associated dataset shard to pass to the \n",
    "    # TorchTrainer constructor in the next code block.\n",
    "    # The iter_torch_batches method lazily shards the dataset among workers.\n",
    "    # =============================================================================\n",
    "    train_data_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    valid_data_shard = ray.train.get_dataset_shard(\"valid\")\n",
    "    train_dataloader = train_data_shard.iter_torch_batches(batch_size=batch_size)\n",
    "    valid_dataloader = valid_data_shard.iter_torch_batches(batch_size=batch_size)\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        image_size=32,   # CIFAR-10 image size is 32x32\n",
    "        patch_size=4,    # Patch size is 4x4\n",
    "        num_layers=12,   # Number of transformer layers\n",
    "        num_heads=8,     # Number of attention heads\n",
    "        hidden_dim=384,  # Hidden size (can be adjusted)\n",
    "        mlp_dim=768,     # MLP dimension (can be adjusted)\n",
    "        num_classes=10   # CIFAR-10 has 10 classes\n",
    "    )\n",
    "\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "    # Model training loop.\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Train Epoch {epoch}\"):\n",
    "            X, y = batch['image'], batch['label']\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss, num_correct, num_total, num_batches = 0, 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_dataloader, desc=f\"Valid Epoch {epoch}\"):\n",
    "                # [2] Each Ray Data batch is a dict so you must access the\n",
    "                # underlying data using the appropriate keys.\n",
    "                # =======================================================\n",
    "                X, y = batch['image'], batch['label']\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                num_total += y.shape[0]\n",
    "                num_batches += 1\n",
    "                num_correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        valid_loss /= num_batches\n",
    "        accuracy = num_correct / num_total\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics={\"loss\": valid_loss, \"accuracy\": accuracy},\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "            if ray.train.get_context().get_world_rank() == 0:\n",
    "                print({\"epoch_num\": epoch, \"loss\": valid_loss, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the training function with the Ray Data Dataset on the Ray Cluster with 8 GPU workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=65322)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=65322)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(RayTrainWorker pid=19510, ip=10.0.128.198)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TrainController pid=65322)\u001b[0m Started training worker group of size 2: \n",
      "\u001b[36m(TrainController pid=65322)\u001b[0m - (ip=10.0.128.198, pid=19510) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TrainController pid=65322)\u001b[0m - (ip=10.0.170.190, pid=17613) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TrainController pid=65322)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
      "\u001b[36m(RayTrainWorker pid=17613, ip=10.0.170.190)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=17613, ip=10.0.170.190)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "Train Epoch 0: 0it [00:00, ?it/s]p=10.0.170.190)\u001b[0m \n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Registered dataset logger for dataset train_8_0\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Starting execution of Dataset train_8_0. Full logs are in /tmp/ray/session_2026-02-02_03-39-03_978542_2343/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Execution plan of Dataset train_8_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[Map(transform_cifar)] -> OutputSplitter[split(2, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Progress bar disabled because stdout is a non-interactive terminal.\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m ⚠️  Ray's object store is configured to use only 27.9% of available memory (44.6GiB out of 160.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m === Ray Data Progress {ListFiles} ===\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m === Ray Data Progress {ReadFiles} ===\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m === Ray Data Progress {Map(transform_cifar)} ===\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Map(transform_cifar): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m === Ray Data Progress {split(2, equal=True)} ===\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m split(2, equal=True): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m === Ray Data Progress {Running Dataset} ===\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Running Dataset: train_8_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 88.0B object store: Progress Completed 1 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m ReadFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Map(transform_cifar): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m split(2, equal=True): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Running Dataset: train_8_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "\u001b[36m(RayTrainWorker pid=19510, ip=10.0.128.198)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=19510, ip=10.0.128.198)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "Train Epoch 0: 0it [00:00, ?it/s]p=10.0.128.198)\u001b[0m \n",
      "\u001b[36m(Map(transform_cifar) pid=17498, ip=10.0.170.190)\u001b[0m /tmp/ray/session_2026-02-02_03-39-03_978542_2343/runtime_resources/pip/396d934a75052c48a4befae9107ea6c41348d639/virtualenv/lib/python3.12/site-packages/torchvision/transforms/functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n",
      "\u001b[36m(Map(transform_cifar) pid=17498, ip=10.0.170.190)\u001b[0m   img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 129.0MiB object store: Progress Completed 50000 / 50000\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Map(transform_cifar): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 221.3MiB object store: Progress Completed 16657 / 12250\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m split(2, equal=True): Tasks: 0; Actors: 0; Queued blocks: 1 (123.6MiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 6125 / 12250\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m Running Dataset: train_8_0. Active & requested resources: 1/32 CPU, 350.4MiB/26.9GiB object store: Progress Completed 6125 / 12250\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m ✔️  Dataset train_8_0 execution finished in 12.69 seconds\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "\u001b[36m(SplitCoordinator pid=65576)\u001b[0m INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n",
      "\u001b[36m(Map(transform_cifar) pid=19223, ip=10.0.128.198)\u001b[0m /tmp/ray/session_2026-02-02_03-39-03_978542_2343/runtime_resources/pip/396d934a75052c48a4befae9107ea6c41348d639/virtualenv/lib/python3.12/site-packages/torchvision/transforms/functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n",
      "\u001b[36m(Map(transform_cifar) pid=19223, ip=10.0.128.198)\u001b[0m   img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
      "Train Epoch 0: 1it [00:17, 17.73s/it].0.128.198)\u001b[0m \n",
      "Train Epoch 0: 17it [00:23,  2.68it/s]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Train Epoch 0: 32it [00:28,  2.84it/s]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Train Epoch 0: 47it [00:33,  2.82it/s]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Train Epoch 0: 61it [00:38,  2.82it/s]\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "Train Epoch 0: 75it [00:43,  2.81it/s]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Train Epoch 0: 89it [00:48,  2.78it/s]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Train Epoch 0: 98it [00:51,  1.89it/s]0.128.198)\u001b[0m \n",
      "Valid Epoch 0: 0it [00:00, ?it/s]p=10.0.128.198)\u001b[0m \n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m Registered dataset logger for dataset valid_10_0\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m Starting execution of Dataset valid_10_0. Full logs are in /tmp/ray/session_2026-02-02_03-39-03_978542_2343/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m Execution plan of Dataset valid_10_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[Map(transform_cifar)] -> OutputSplitter[split(2, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m Progress bar disabled because stdout is a non-interactive terminal.\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m ⚠️  Ray's object store is configured to use only 27.9% of available memory (44.6GiB out of 160.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m === Ray Data Progress {ListFiles} ===\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m === Ray Data Progress {ReadFiles} ===\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m === Ray Data Progress {Map(transform_cifar)} ===\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m Map(transform_cifar): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m === Ray Data Progress {split(2, equal=True)} ===\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m split(2, equal=True): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m === Ray Data Progress {Running Dataset} ===\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m Running Dataset: valid_10_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m ✔️  Dataset valid_10_0 execution finished in 2.36 seconds\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "Train Epoch 0: 97it [00:51,  2.78it/s]\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(SplitCoordinator pid=65631)\u001b[0m INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n",
      "Valid Epoch 0: 1it [00:02,  2.74s/it].0.170.190)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=19510, ip=10.0.128.198)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/train_data_run-2f79d728fff148adaf600a74676f402f/checkpoint_2026-02-02_06-19-08.369116)\n",
      "\u001b[36m(RayTrainWorker pid=19510, ip=10.0.128.198)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/train_data_run-2f79d728fff148adaf600a74676f402f/checkpoint_2026-02-02_06-19-08.369116), metrics={'loss': 1.7432047188282014, 'accuracy': 0.3514}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=19510, ip=10.0.128.198)\u001b[0m {'epoch_num': 0, 'loss': 1.7432047188282014, 'accuracy': 0.3514}\n",
      "\u001b[36m(TrainController pid=65322)\u001b[0m [State Transition] RUNNING -> SHUTTING_DOWN.\n",
      "Train Epoch 0: 98it [00:51,  1.89it/s]0.170.190)\u001b[0m \n",
      "Valid Epoch 0: 0it [00:00, ?it/s]p=10.0.170.190)\u001b[0m \n",
      "Valid Epoch 0: 20it [00:04,  4.50it/s]\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result: Result(metrics={'loss': 1.7432047188282014, 'accuracy': 0.3514}, checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/train_data_run-2f79d728fff148adaf600a74676f402f/checkpoint_2026-02-02_06-19-08.369116), error=None, path='/mnt/cluster_storage/train_data_run-2f79d728fff148adaf600a74676f402f', metrics_dataframe=       loss  accuracy\n",
      "0  1.743205    0.3514, best_checkpoints=[(Checkpoint(filesystem=local, path=/mnt/cluster_storage/train_data_run-2f79d728fff148adaf600a74676f402f/checkpoint_2026-02-02_06-19-08.369116), {'loss': 1.7432047188282014, 'accuracy': 0.3514})], _storage_filesystem=<pyarrow._fs.LocalFileSystem object at 0x7683b4011870>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=65322)\u001b[0m [State Transition] SHUTTING_DOWN -> FINISHED.\n"
     ]
    }
   ],
   "source": [
    "def train_cifar_10(num_workers, use_gpu):\n",
    "    global_batch_size = 512\n",
    "\n",
    "    train_config = {\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size_per_worker\": global_batch_size // num_workers,\n",
    "    }\n",
    "\n",
    "    # Configure computation resources.\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers, use_gpu=use_gpu)\n",
    "    run_config = RunConfig(\n",
    "        storage_path=\"/mnt/cluster_storage\", \n",
    "        name=f\"train_data_run-{uuid.uuid4().hex}\",\n",
    "    )\n",
    "\n",
    "    # Initialize a Ray TorchTrainer.\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        # [1] With Ray Data you pass the Dataset directly to the Trainer.\n",
    "        # ==============================================================\n",
    "        datasets={\"train\": train_dataset, \"valid\": test_dataset},\n",
    "        train_loop_config=train_config,\n",
    "        scaling_config=scaling_config,\n",
    "        run_config=run_config,\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(f\"Training result: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_cifar_10(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again your training run should take around 1 minute with similar accuracy. There aren't big performance wins with Ray Data on this example due to the small size of the dataset; for more interesting benchmarking information see [this blog post](https://www.anyscale.com/blog/fast-flexible-scalable-data-loading-for-ml-training-with-ray-data). The main advantage of Ray Data is that it allows you to stream data across heterogeneous compute, maximizing GPU utilization while minimizing RAM usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you are using an Anyscale Workspace, in addition to the Ray Train and Metrics dashboards you saw in the previous section, you can also view the Ray Data dashboard by clicking **Ray Workloads** and then **Data** where you can view the throughput and status of each [Ray Data operator](https://docs.ray.io/en/latest/data/key-concepts.html#operators-and-plans).\n",
    "\n",
    "![Data Dashboard](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/train/examples/pytorch/distributing-pytorch/images/data_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train in Production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here are some use-cases of using Ray Train in production:\n",
    "1. Canva uses Ray Train + Ray Data to cut down Stable Diffusion training costs by 3.7x. Read this [Anyscale blog post here](https://www.anyscale.com/blog/scalable-and-cost-efficient-stable-diffusion-pre-training-with-ray) and the [Canva  case study here](https://www.anyscale.com/resources/case-study/how-canva-built-a-modern-ai-platform-using-anyscale)\n",
    "2. Anyscale uses Ray Train + Deepspeed to finetune language models. Read more [here](https://github.com/ray-project/ray/tree/master/doc/source/templates/04_finetuning_llms_with_deepspeed).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cleanup\n",
    "\n",
    "Always clean up GPU and CPU memory after training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cleanup complete.\n",
      "Ray shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import ray\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Clean up GPU and CPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"Local cleanup complete.\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup()\n",
    "\n",
    "# Optional: Shutdown Ray (uncomment if ending session)\n",
    "ray.shutdown()\n",
    "print(\"Ray shutdown complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
